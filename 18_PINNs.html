<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="es" xml:lang="es"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Redes Neuronales Informadas por la Física (PINNs) – Aprendizaje Automático para Físicos</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-e13f217ceb0135cb77e1494052e28e8a.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Sin resultados",
    "search-matching-documents-text": "documentos encontrados",
    "search-copy-link-title": "Copiar el enlace en la búsqueda",
    "search-hide-matches-text": "Ocultar resultados adicionales",
    "search-more-match-text": "resultado adicional en este documento",
    "search-more-matches-text": "resultados adicionales en este documento",
    "search-clear-button-title": "Borrar",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancelar",
    "search-submit-button-title": "Enviar",
    "search-label": "Buscar"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Aprendizaje Automático para Físicos</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Buscar"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Navegación de palanca" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-clases" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Clases</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-clases">    
        <li class="dropdown-header">Probabilidad y Estadística</li>
        <li>
    <a class="dropdown-item" href="./1_probabilidad.html">
 <span class="dropdown-text">1. Probabilidad</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./2_distribuciones_y_estimacion.html">
 <span class="dropdown-text">2. Distribuciones y Estimación</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./3_verosimilitud.html">
 <span class="dropdown-text">3. Verosimilitud</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./4_minimos_cuadrados_intervalos.html">
 <span class="dropdown-text">4. Mínimos Cuadrados e Intervalos</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./5_testeo_de_hipotesis_entropia.html">
 <span class="dropdown-text">5. Testeo de Hipótesis y Entropía</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">Aprendizaje Automático</li>
        <li>
    <a class="dropdown-item" href="./6_aprendizaje_automatico_regresion_lineal.html">
 <span class="dropdown-text">6. Aprendizaje Automático y Regresión Lineal</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./7_clasificacion.html">
 <span class="dropdown-text">7. Clasificación</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./8_seleccion_de_modelos.html">
 <span class="dropdown-text">8. Selección de Modelos</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./9_arboles_bosques_boosting.html">
 <span class="dropdown-text">9. Árboles, Bosques y Boosting</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./10_svm_y_no_supervisado.html">
 <span class="dropdown-text">10. SVM y Aprendizaje No Supervisado</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">Redes Neuronales</li>
        <li>
    <a class="dropdown-item" href="./11_perceptron_y_redes_conexas.html">
 <span class="dropdown-text">11. Perceptrón y Redes Conexas y Pérdidas</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./12_gradient_descent_and_autodiff.html">
 <span class="dropdown-text">12. Descenso de Gradiente y Auto-diferenciación</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./13_evaluacion_y_regularizacion_de_nn.html">
 <span class="dropdown-text">13. Evaluación y Regularización de redes neuronales</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./14_redes_convolucionales.html">
 <span class="dropdown-text">14. Redes Convolucionales</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./15_redes_residuales.html">
 <span class="dropdown-text">15. Redes Residuales</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./16_transformadores.html">
 <span class="dropdown-text">16. Transformadores y el mecanismo de atención</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">Algunas aplicaciones</li>
        <li>
    <a class="dropdown-item" href="./17_flujos_normalizantes.html">
 <span class="dropdown-text">17. Flujos Normalizantes</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./18_PINNs.html">
 <span class="dropdown-text">18. Redes Neuronales Informadas por la Física (PINNs)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./19_Operador_Neural.html">
 <span class="dropdown-text">19. Operadores Neuronales</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="./about.html"> 
<span class="menu-text">Acerca de</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Alternar barra lateral" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./17_flujos_normalizantes.html">Algunas aplicaciones</a></li><li class="breadcrumb-item"><a href="./18_PINNs.html">18. Redes Neuronales Informadas por la Física (PINNs)</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Alternar barra lateral" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Probabilidad y Estadística</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Alternar sección">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1_probabilidad.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. Probabilidad</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2_distribuciones_y_estimacion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. Distribuciones y Estimación</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3_verosimilitud.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3. Verosimilitud</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./4_minimos_cuadrados_intervalos.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4. Mínimos Cuadrados e Intervalos</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./5_testeo_de_hipotesis_entropia.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5. Testeo de Hipótesis y Entropía</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Aprendizaje Automático</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Alternar sección">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./6_aprendizaje_automatico_regresion_lineal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6. Aprendizaje Automático y Regresión Lineal</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./7_clasificacion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7. Clasificación</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./8_seleccion_de_modelos.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8. Selección de Modelos</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./9_arboles_bosques_boosting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9. Árboles, Bosques y Boosting</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10_svm_y_no_supervisado.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">10. SVM y Aprendizaje No Supervisado</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Redes Neuronales</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Alternar sección">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11_perceptron_y_redes_conexas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">11. Perceptrón, Redes Conexas y Pérdidas</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12_gradient_descent_and_autodiff.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">12. Descenso de Gradiente y Auto-diferenciación</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13_evaluacion_y_regularizacion_de_nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">13. Evaluación y Regularización de redes neuronales</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14_redes_convolucionales.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">14. Redes Convolucionales</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15_redes_residuales.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">15. Redes Residuales</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16_transformadores.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">16. Transformadores y el mecanismo de atención</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Algunas aplicaciones</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Alternar sección">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17_flujos_normalizantes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">17. Flujos Normalizantes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18_PINNs.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">18. Redes Neuronales Informadas por la Física (PINNs)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19_Operador_Neural.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">19. Operadores Neuronales</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">En esta página</h2>
   
  <ul>
  <li><a href="#redes-neuronales-informadas-por-la-física-para-ecuaciones-diferenciales-ordinarias" id="toc-redes-neuronales-informadas-por-la-física-para-ecuaciones-diferenciales-ordinarias" class="nav-link active" data-scroll-target="#redes-neuronales-informadas-por-la-física-para-ecuaciones-diferenciales-ordinarias">Redes Neuronales Informadas por la Física para ecuaciones diferenciales ordinarias</a>
  <ul class="collapse">
  <li><a href="#idea-básica-para-una-edo" id="toc-idea-básica-para-una-edo" class="nav-link" data-scroll-target="#idea-básica-para-una-edo">Idea básica (para una EDO)</a></li>
  <li><a href="#procedimiento-de-entrenamiento" id="toc-procedimiento-de-entrenamiento" class="nav-link" data-scroll-target="#procedimiento-de-entrenamiento">Procedimiento de entrenamiento</a></li>
  </ul></li>
  <li><a href="#extensión-de-pinns-a-edps" id="toc-extensión-de-pinns-a-edps" class="nav-link" data-scroll-target="#extensión-de-pinns-a-edps">Extensión de PINNs a EDPs</a>
  <ul class="collapse">
  <li><a href="#formulación-general" id="toc-formulación-general" class="nav-link" data-scroll-target="#formulación-general">Formulación general:</a></li>
  <li><a href="#diferenciación-automática-para-edps" id="toc-diferenciación-automática-para-edps" class="nav-link" data-scroll-target="#diferenciación-automática-para-edps">Diferenciación automática para EDPs</a></li>
  <li><a href="#ejemplo-ecuación-de-burgers" id="toc-ejemplo-ecuación-de-burgers" class="nav-link" data-scroll-target="#ejemplo-ecuación-de-burgers">Ejemplo (ecuación de Burgers)</a></li>
  <li><a href="#alternativas-para-la-condición-de-contorno" id="toc-alternativas-para-la-condición-de-contorno" class="nav-link" data-scroll-target="#alternativas-para-la-condición-de-contorno">Alternativas para la condición de contorno</a></li>
  </ul></li>
  <li><a href="#comparación-con-métodos-numéricos-tradicionales" id="toc-comparación-con-métodos-numéricos-tradicionales" class="nav-link" data-scroll-target="#comparación-con-métodos-numéricos-tradicionales">Comparación con Métodos Numéricos Tradicionales</a>
  <ul class="collapse">
  <li><a href="#precisión-y-convergencia" id="toc-precisión-y-convergencia" class="nav-link" data-scroll-target="#precisión-y-convergencia">Precisión y Convergencia</a></li>
  </ul></li>
  <li><a href="#ventajas-de-las-pinns-en-escenarios-con-escasez-de-datos-y-condiciones-complejas" id="toc-ventajas-de-las-pinns-en-escenarios-con-escasez-de-datos-y-condiciones-complejas" class="nav-link" data-scroll-target="#ventajas-de-las-pinns-en-escenarios-con-escasez-de-datos-y-condiciones-complejas">Ventajas de las PINNs en Escenarios con Escasez de Datos y Condiciones Complejas</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./17_flujos_normalizantes.html">Algunas aplicaciones</a></li><li class="breadcrumb-item"><a href="./18_PINNs.html">18. Redes Neuronales Informadas por la Física (PINNs)</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Redes Neuronales Informadas por la Física (PINNs)</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="redes-neuronales-informadas-por-la-física-para-ecuaciones-diferenciales-ordinarias" class="level1">
<h1>Redes Neuronales Informadas por la Física para ecuaciones diferenciales ordinarias</h1>
<p>Una red neuronal informada por la física (PINN, por sus siglas en inglés) es un aproximador de funciones basado en redes neuronales que se entrena no solo con datos, sino también para satisfacer las leyes físicas (normalmente expresadas como ecuaciones diferenciales) que gobiernan el sistema. En otras palabras, una PINN incorpora las ecuaciones gobernantes (por ejemplo, una ecuación diferencial ordinaria [EDO] o una ecuación diferencial parcial [EDP]) en su función de pérdida como restricción o regularizador. Este enfoque fue introducido por Raissi, Perdikaris y Karniadakis en 2017–2019 (<a href="https://arxiv.org/abs/1711.10561">Raissi et al.&nbsp;(2017)</a>, <a href="https://arxiv.org/abs/1711.10566">Raissi et al.&nbsp;(2019)</a>). La idea es ayudarle a la red a generalizar usando la información de la ecuación diferencial.</p>
<p>Para una revisión reciente, véase <a href="https://arxiv.org/abs/2408.16806v1">Raissi et al.&nbsp;(2024)</a>. Este documento se basó en esa revisión, además de material de <a href="https://book.sciml.ai/">este libro</a>.</p>
<p>Este es uno de los métodos más conocidos del llamado “aprendizaje automático científico” (Scientific Machine Learning, SML). Ha recibido críticas recientemente por la fragilidad del método. Es decir, cuando se aplica a ecuaciones diferenciales nuevas, puede que el método no funcione bien. También se ha notado que si se usa para resolver ecuaciones diferenciales numéricamente puede ser más lento que los métodos tradicionales en muchos casos.</p>
<section id="idea-básica-para-una-edo" class="level2">
<h2 class="anchored" data-anchor-id="idea-básica-para-una-edo">Idea básica (para una EDO)</h2>
<p>Considere una ecuación diferencial ordinaria simple, por ejemplo:</p>
<p><span class="math display">\[
y'(t) + y(t) = 0, \qquad y(0) = 1,
\]</span></p>
<p>cuya solución analítica es <span class="math inline">\(y(t)=e^{-t}\)</span>. Para resolver esto usando una PINN, aproximamos la solución <span class="math inline">\(y(t)\)</span> con una red neuronal <span class="math inline">\(y_\theta(t)\)</span> (con parámetros <span class="math inline">\(\theta\)</span>). La red se entrena para satisfacer tanto la EDO como cualquier dato o condición inicial/de contorno dada. Logramos esto definiendo una función de pérdida que incluye la EDO, llamada una “función de pérdida informada por la física”. Para la EDO anterior, definimos el residuo <span class="math display">\[
r(t) = y_\theta'(t) + y_\theta(t),
\]</span></p>
<p>que debería ser cero para todo <span class="math inline">\(t\)</span> si <span class="math inline">\(y_\theta(t)\)</span> satisface exactamente la EDO. La pérdida de la PINN puede entonces construirse para penalizar el residuo y cualquier desviación de las condiciones iniciales:</p>
<p><span class="math display">\[
\mathcal{J}(\theta) \;=\; \underbrace{\frac{1}{N_f}\sum_{i=1}
^{N_f} |r(t_i)|^2}_{\text{pérdida del residuo de la EDO}} \;+\; \underbrace{|\,y_\theta(0) - 1\,|^2}_{\text{pérdida de la condición inicial}},
\]</span></p>
<p>donde <span class="math inline">\({t_i}\)</span> son un conjunto de puntos donde se evalúa la EDO, llamados puntos de colocación, en el dominio de interés (p.&nbsp;ej. <span class="math inline">\(t\in[0,T]\)</span>). El término <span class="math inline">\(y_\theta'(t)\)</span> se obtiene mediante diferenciación automática de la red neuronal con respecto a su entrada <span class="math inline">\(t\)</span>. Durante el entrenamiento, el optimizador ajusta <span class="math inline">\(\theta\)</span> para minimizar <span class="math inline">\(\mathcal{J}(\theta)\)</span>, empujando así a <span class="math inline">\(y_\theta(t)\)</span> a satisfacer tanto la EDO (haciendo <span class="math inline">\(r(t)\approx 0\)</span>) como la condición inicial. En efecto, la ley física actúa como un regularizador que guía la red hacia soluciones físicamente consistentes.</p>
</section>
<section id="procedimiento-de-entrenamiento" class="level2">
<h2 class="anchored" data-anchor-id="procedimiento-de-entrenamiento">Procedimiento de entrenamiento</h2>
<p>Resolver una EDO con una PINN típicamente implica los siguientes pasos:</p>
<ul>
<li><p>Definir la Red Neuronal: Elegir una arquitectura de red neuronal para <span class="math inline">\(y_\theta(t)\)</span>. Esta red tomará <span class="math inline">\(t\)</span> como entrada y producirá una aproximación para <span class="math inline">\(y(t)\)</span>.</p></li>
<li><p>Configurar la Función de Pérdida: Formular la pérdida como la suma de un término basado en la física y términos de datos. Para una EDO, el término físico es el MSE (error cuadrático medio) del residuo de la EDO <span class="math inline">\(r(t)=0\)</span> sobre un conjunto de puntos de colocación <span class="math inline">\({t_i}\)</span>, y los términos de datos imponen condiciones iniciales o de contorno (y cualquier punto de datos observado adicional si está disponible).</p></li>
<li><p>Diferenciación Automática: Habilitar la autodiferenciación para <span class="math inline">\(t\)</span> de modo que la salida de la red pueda diferenciarse con respecto a <span class="math inline">\(t\)</span>. Esto permite calcular <span class="math inline">\(y_\theta'(t)\)</span> exactamente (con precisión de máquina) mediante retropropagación, en lugar de usar diferencias finitas.</p></li>
<li><p>Optimizar: Inicializar los parámetros de la red y usar un optimizador (p.&nbsp;ej. descenso de gradiente o Adam) para minimizar la pérdida. Esto requerirá evaluar el residuo y sus gradientes repetidamente para ajustar <span class="math inline">\(\theta\)</span>.</p></li>
<li><p>Resultado: Después del entrenamiento, la red neuronal <span class="math inline">\(y_\theta(t)\)</span> sirve como una solución sustituta de la EDO. Se puede evaluar <span class="math inline">\(y_\theta(t)\)</span> en cualquier <span class="math inline">\(t\)</span> deseado para obtener la solución (la red neuronal es una interpolación de la solución).</p></li>
</ul>
<p>Para ilustrar, a continuación se muestra un ejemplo simple usando PyTorch para la EDO <span class="math inline">\(y' + y = 0,; y(0)=1\)</span>. Definimos una red pequeña y la entrenamos con la pérdida informada por la física:</p>
<div id="6c08ddf4" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Definir una red neuronal simple para y(t)</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RedEDO(nn.Module):</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(RedEDO, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.red <span class="op">=</span> nn.Sequential(</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">1</span>, <span class="dv">20</span>), nn.Tanh(),</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">20</span>, <span class="dv">20</span>), nn.Tanh(),</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">20</span>, <span class="dv">1</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, t):</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.red(t)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Instanciar la red y el optimizador</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>modelo <span class="op">=</span> RedEDO()</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>optimizador <span class="op">=</span> torch.optim.Adam(modelo.parameters(), lr<span class="op">=</span><span class="fl">1e-3</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>T_max <span class="op">=</span> <span class="fl">5.0</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Bucle de entrenamiento (entrenamiento informado por la física para EDO)</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoca <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10000</span>):</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Muestrear puntos de colocación en el dominio [0, T]</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    t_coloc <span class="op">=</span> torch.rand(<span class="dv">32</span>, <span class="dv">1</span>) <span class="op">*</span> T_max</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    t_coloc.requires_grad_(<span class="va">True</span>)  <span class="co"># habilitar gradiente respecto a t</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calcular la salida de la red y su derivada temporal mediante autograd</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> modelo(t_coloc)  <span class="co"># y_θ(t)</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    dy_dt <span class="op">=</span> torch.autograd.grad(y_pred, t_coloc,</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>                                grad_outputs<span class="op">=</span>torch.ones_like(y_pred),</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>                                create_graph<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>]  <span class="co"># y'_θ(t)</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Residuo físico para la EDO: f(t) = y'_θ + y_θ</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    f <span class="op">=</span> dy_dt <span class="op">+</span> y_pred</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    perdida_fisica <span class="op">=</span> torch.mean(f<span class="op">**</span><span class="dv">2</span>)  <span class="co"># residuo de la EDO</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Pérdida por condición inicial en t=0: (y_θ(0) - 1)^2</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    y0_pred <span class="op">=</span> modelo(torch.zeros(<span class="dv">1</span>, <span class="dv">1</span>))  <span class="co"># salida en t=0</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    perdida_ci <span class="op">=</span> (y0_pred <span class="op">-</span> <span class="fl">1.0</span>) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Pérdida total y paso de optimización</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>    perdida <span class="op">=</span> perdida_fisica <span class="op">+</span> perdida_ci</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    optimizador.zero_grad()</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>    perdida.backward()</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>    optimizador.step()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>En este código, el término <code>torch.autograd.grad(..., create_graph=True)</code> se utiliza para calcular <span class="math inline">\(y_\theta'(t)\)</span> mediante diferenciación automática. La pérdida física impulsa a la red a satisfacer <span class="math inline">\(y' + y = 0\)</span>, mientras que la pérdida de la condición inicial hace tender <span class="math inline">\(y_\theta(0)\)</span> a <span class="math inline">\(1\)</span>. Después del entrenamiento, la salida de la red <span class="math inline">\(y_\theta(t)\)</span> se aproximará mucho a la solución verdadera <span class="math inline">\(e^{-t}\)</span>, ver figura <a href="#fig-pinn" class="quarto-xref">Figura&nbsp;1</a>. Al entrenar en puntos de colocación muestreados del dominio, la PINN efectivamente “aprende” la solución continua sin pasar explícitamente por el tiempo. Este enfoque puede manejar problemas directos (encontrar <span class="math inline">\(y(t)\)</span> dada la ecuación diferencial y la condición inicial) así como problemas inversos (por ejemplo inferir parámetros en la EDO a partir de datos) dentro del mismo marco.</p>
<div id="cell-fig-pinn" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>t_vals <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">5</span>, <span class="dv">200</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>y_exact <span class="op">=</span> np.exp(<span class="op">-</span>t_vals)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluar la red neuronal entrenada (modelo) en los valores de t</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>t_tensor <span class="op">=</span> torch.from_numpy(t_vals.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)).<span class="bu">float</span>()</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    y_modelo <span class="op">=</span> modelo(t_tensor).cpu().numpy().flatten()</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>residuos <span class="op">=</span> np.<span class="bu">abs</span>(y_modelo <span class="op">-</span> y_exact)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">5</span>), sharex<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>                       gridspec_kw<span class="op">=</span>{<span class="st">"height_ratios"</span>: [<span class="dv">2</span>, <span class="dv">1</span>]})</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Panel superior: Soluciones</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].plot(t_vals, y_exact, label<span class="op">=</span><span class="st">"$e^{-t}$"</span>, color<span class="op">=</span><span class="st">"C1"</span>)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].plot(t_vals, y_modelo, label<span class="op">=</span><span class="st">"PINN $y_</span><span class="ch">\\</span><span class="st">theta(t)$"</span>, color<span class="op">=</span><span class="st">"C0"</span>, linestyle<span class="op">=</span><span class="st">"--"</span>)</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].set_ylabel(<span class="st">"$y(t)$"</span>)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].set_title(<span class="st">"Solución exacta y PINN para $y(t) = e^{-t}$"</span>)</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].legend()</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Panel inferior: Residuos</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].plot(t_vals, residuos, color<span class="op">=</span><span class="st">"C2"</span>)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].axhline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">"gray"</span>, linestyle<span class="op">=</span><span class="st">"--"</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].set_xlabel(<span class="st">"$t$"</span>)</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].set_ylabel(<span class="st">"Error absoluto"</span>)</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].set_title(<span class="st">"Residuo PINN - Exacto"</span>)</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-pinn" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pinn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="18_PINNs_files/figure-html/fig-pinn-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pinn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;1: Solución exacta y PINN para <span class="math inline">\(y(t) = e^{-t}\)</span>
</figcaption>
</figure>
</div>
</div>
</div>
<p>Vale la pena señalar que este concepto de usar redes neuronales para resolver ecuaciones diferenciales tiene sus raíces en trabajos anteriores (p.&nbsp;ej. Lagaris et al., 1998). La novedad es la integración perfecta de herramientas modernas de autodiferenciación, entrenamiento eficiente en puntos de colocación y capacidad para incorporar datos adicionales o parámetros desconocidos.</p>
</section>
</section>
<section id="extensión-de-pinns-a-edps" class="level1">
<h1>Extensión de PINNs a EDPs</h1>
<p>Extender las PINNs de EDOs a ecuaciones diferenciales parciales (EDPs) es directo. En una EDP, la solución <span class="math inline">\(u\)</span> depende de múltiples entradas (p.&nbsp;ej. tiempo <span class="math inline">\(t\)</span> y coordenadas espaciales <span class="math inline">\(x\)</span>) y la ecuación involucra derivadas parciales. Las PINNs manejan esto usando redes neuronales con múltiples entradas y usando diferenciación automática para calcular derivadas parciales con respecto a cada entrada. La red neuronal <span class="math inline">\(u_\theta(t,x)\)</span> se entrena para satisfacer la EDP en todo el dominio, así como cualquier condición inicial y de contorno.</p>
<section id="formulación-general" class="level2">
<h2 class="anchored" data-anchor-id="formulación-general">Formulación general:</h2>
<p>Consideremos la siguiente clase de EDPs (extenderlo a otras es directo):</p>
<p><span class="math display">\[
\partial_t u(t,x) + \mathcal{N}[u(t,x);\lambda] = 0, \qquad x \in \Omega,\;\; t \in [0,T],
\]</span></p>
<p>donde <span class="math inline">\(u(t,x)\)</span> es la solución desconocida, <span class="math inline">\(\mathcal{N}[\cdot;\lambda]\)</span> es un operador diferencial (no lineal) (que representa la parte espacial de la EDP, con parámetros <span class="math inline">\(\lambda\)</span> como constantes físicas), y <span class="math inline">\(\Omega\)</span> es el conjunto de puntos en el espacio donde queremos resolver el problema. Esta forma general puede representar una amplia gama de fenómenos físicos (difusión, advección, leyes de conservación, etc.). Uno de los primeros problemas resueltos con PINNs es la ecuación de Burgers 1D <span class="math inline">\(\partial_t u + u\partial_x u - \nu \partial_x^2 u=0\)</span>. Esta ecuación se ajusta a esta forma con <span class="math inline">\(\mathcal{N}[u] = -u\partial_x u + \nu \partial_x^2 u\)</span>.</p>
<p>Para resolver tal EDP con una PINN, procedemos de manera similar al caso de las EDO:</p>
<ul>
<li><p>Definimos una red neuronal <span class="math inline">\(u_\theta(t,x)\)</span> que toma <span class="math inline">\((t,x)\)</span> como entrada y produce un escalar <span class="math inline">\(u\)</span> (la solución predicha). La red es nuestra función de prueba para la solución de la EDP.</p></li>
<li><p>Luego definimos el residuo de la EDP <span class="math inline">\(r(t,x)\)</span> sustituyendo la red en la EDP. Para la forma general anterior, sea</p></li>
</ul>
<p><span class="math display">\[
r(t,x) \equiv \partial_t u_\theta(t,x) + \mathcal{N}[\,u_\theta(t,x)\,;\lambda],
\]</span></p>
<p>que por construcción debería ser cero en todas partes si <span class="math inline">\(u_\theta\)</span> satisface exactamente la EDP.</p>
<ul>
<li>La función de pérdida se construye para minimizar el residuo cuadrático medio de la EDP y los errores al cumplir las condiciones iniciales/de contorno (y cualquier otro dato). Típicamente escribimos</li>
</ul>
<p><span class="math display">\[
\mathcal{J}(\theta) \;=\; \mathcal{J}_{\text{EDP}}(\theta) + \mathcal{J}_{\text{CI}}(\theta) + \mathcal{J}_{\text{CC}}(\theta) + \mathcal{J}_{\text{datos}}(\theta),
\]</span></p>
<p>donde</p>
<ul>
<li><p><span class="math inline">\(\mathcal{J}_{\text{EDP}} = \frac{1}{N_f}\sum_{i=1}^{N_f} |r(t_i, x_i)|^2\)</span> es la pérdida física evaluada en un conjunto de <span class="math inline">\(N_f\)</span> puntos de colocación <span class="math inline">\((t_i,x_i)\)</span> en el dominio (a veces elegidos aleatoriamente o en una cuadrícula);</p></li>
<li><p><span class="math inline">\(\mathcal{J}_{\text{CI}}\)</span> es un término que impone la condición inicial (por ejemplo <span class="math inline">\(\frac{1}{N_{ci}}\sum |u_\theta(0,x_j) - u_0(x_j)|^2\)</span> para el perfil inicial dado <span class="math inline">\(u_0(x)\)</span>);</p></li>
<li><p><span class="math inline">\(\mathcal{J}_{\text{CC}}\)</span> impone condiciones de contorno (recordemos que ciertos tipos de EDPs requieren ambas condiciones iniciales así como condiciones en el borde del dominio <span class="math inline">\(\Omega\)</span>);</p></li>
<li><p><span class="math inline">\(\mathcal{J}_{\text{datos}}\)</span> (opcional) puede imponer cualquier observación/punto de datos interior adicional que la solución deba igualar.</p></li>
</ul>
<p>El entrenamiento de la PINN busca entonces minimizar <span class="math inline">\(\mathcal{J}(\theta)\)</span>, encontrando así una <span class="math inline">\(u_\theta(t,x)\)</span> que simultáneamente (1) se ajuste a cualquier restricción de datos/inicial/de contorno y (2) haga que el residuo de la EDP <span class="math inline">\(r(t,x)\)</span> sea pequeño en todas partes. Ajustando los pesos relativos de estos términos, se pueden manejar casos donde ciertas restricciones son más importantes. Se ha notado que en muchos casos conviene multiplicar la pérdida de condiciones iniciales o de borde por un número grande para obligar a la red a cumplirlas. Se pueden también introducir factores de ponderación para equilibrar la pérdida del residuo frente a la pérdida de datos si es necesario.</p>
</section>
<section id="diferenciación-automática-para-edps" class="level2">
<h2 class="anchored" data-anchor-id="diferenciación-automática-para-edps">Diferenciación automática para EDPs</h2>
<p>Una ventaja clave de las PINNs es la facilidad para calcular derivadas parciales usando diferenciación automática (DA) incorporada en los paquetes modernos como Pytorch. La red neuronal <span class="math inline">\(u_\theta(t,x)\)</span> es una función compuesta (de la entrada a través de las capas de la red); por lo tanto, podemos obtener <span class="math inline">\(\partial_t u\)</span>, <span class="math inline">\(\partial_x u\)</span>, <span class="math inline">\(\partial_{xx} u\)</span>, etc., aplicando autodiferenciación (retropropagación) tal como calcularíamos gradientes para el entrenamiento de la red. Esto significa que no necesitamos derivar fórmulas de diferencias finitas o derivadas simbólicas de la EDP. El paquete calculará directamente las derivadas exactas de la salida de la red con respecto a sus entradas. Este enfoque produce alta precisión para el residuo y evita los errores de truncamiento asociados con la diferenciación numérica. En esencia, la PINN trata el operador diferencial <span class="math inline">\(\mathcal{N}\)</span> como parte del grafo computacional de la red, usando la regla de la cadena para evaluarlo.</p>
</section>
<section id="ejemplo-ecuación-de-burgers" class="level2">
<h2 class="anchored" data-anchor-id="ejemplo-ecuación-de-burgers">Ejemplo (ecuación de Burgers)</h2>
<p>Para concretar, supongamos que queremos resolver la ecuación de Burgers 1D <span class="math inline">\(\partial_t u + u \partial_x u - \nu \partial_{xx} u = 0\)</span> con una PINN. Configuramos una red neuronal <span class="math inline">\(u_\theta(t,x)\)</span> e incluimos la EDP de Burgers en la pérdida. Un fragmento del código de entrenamiento podría verse así:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Muestrea puntos de colocación en el dominio (t en [0,T], x en [a,b])</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>t_coloc <span class="op">=</span> torch.rand(N, <span class="dv">1</span>) <span class="op">*</span> T_max</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>x_coloc <span class="op">=</span> torch.rand(N, <span class="dv">1</span>) <span class="op">*</span> (b <span class="op">-</span> a) <span class="op">+</span> a</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>t_coloc.requires_grad_(), x_coloc.requires_grad_()  <span class="co"># habilita derivadas parciales</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Salida de la PINN para estos puntos</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>u_predicho <span class="op">=</span> modelo(torch.hstack([t_coloc, x_coloc]))  <span class="co"># u_θ(t,x)</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Calcula derivadas parciales usando autograd</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>u_t   <span class="op">=</span> torch.autograd.grad(u_predicho, t_coloc, torch.ones_like(u_predicho), create_graph<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>]</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>u_x   <span class="op">=</span> torch.autograd.grad(u_predicho, x_coloc, torch.ones_like(u_predicho), create_graph<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>]</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>u_xx  <span class="op">=</span> torch.autograd.grad(u_x,       x_coloc, torch.ones_like(u_x),        create_graph<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>]</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Residuo de la EDP r(t,x) = u_t + u * u_x - ν * u_xx</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> u_t <span class="op">+</span> u_predicho <span class="op">*</span> u_x <span class="op">-</span> nu <span class="op">*</span> u_xx</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>perdida_fisica <span class="op">=</span> torch.mean(r<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Pérdida de la condición inicial (por ejemplo, u(0,x) = u0(x))</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>u_inicial <span class="op">=</span> modelo(torch.hstack([torch.zeros(N0,<span class="dv">1</span>), x0_muestras]))</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>perdida_ci <span class="op">=</span> torch.mean((u_inicial <span class="op">-</span> u0(x0_muestras))<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Pérdida de la condición de frontera (por ejemplo, Dirichlet u(t,a)=g1(t), u(t,b)=g2(t))</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>u_izq  <span class="op">=</span> modelo(torch.hstack([t_coloc, a<span class="op">*</span>torch.ones_like(t_coloc)]))</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>u_der  <span class="op">=</span> modelo(torch.hstack([t_coloc, b<span class="op">*</span>torch.ones_like(t_coloc)]))</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>perdida_cf <span class="op">=</span> torch.mean((u_izq <span class="op">-</span> g1(t_coloc))<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> (u_der <span class="op">-</span> g2(t_coloc))<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Pérdida total: física + inicial + frontera</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>perdida_total <span class="op">=</span> perdida_fisica <span class="op">+</span> perdida_ci <span class="op">+</span> perdida_cf</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Aquí vemos cómo se utilizan los puntos de colocación <span class="math inline">\((t_{\text{colloc}},x_{\text{colloc}})\)</span> para evaluar el residuo de la EDP en todo el dominio, y cómo la diferenciación automática produce <span class="math inline">\(\partial_t u\)</span>, <span class="math inline">\(\partial_x u\)</span>, <span class="math inline">\(\partial_{xx} u\)</span> para calcular el residuo <span class="math inline">\(f(t,x)\)</span>. La función de pérdida penaliza el residuo (haciendo que la solución satisfaga la EDP) e impone las condiciones iniciales y de contorno. Al minimizar esta pérdida, la red aprende una solución <span class="math inline">\(u_\theta(t,x)\)</span> que satisface aproximadamente la EDP en todas partes y cumple aproximadamente las restricciones.</p>
</section>
<section id="alternativas-para-la-condición-de-contorno" class="level2">
<h2 class="anchored" data-anchor-id="alternativas-para-la-condición-de-contorno">Alternativas para la condición de contorno</h2>
<p>En el ejemplo anterior, impusimos condiciones de contorno en la pérdida (a menudo llamado cumplimiento “suave”). En muchas implementaciones de PINN, esto funciona bien ponderando fuertemente la pérdida de CC para asegurar la precisión en los contornos.</p>
<p>Una alternativa es el cumplimiento “duro”, donde incorporamos la condición de contorno en el ansatz de la red neuronal para que se satisfaga exactamente por construcción (por ejemplo añadiendo una función de penalización o usando una arquitectura especial).</p>
<p>Para la EDO considerada en la sección anterior, podemos definir <span class="math inline">\(y_\theta(t) = 1 + t u_\theta(t)\)</span> donde <span class="math inline">\(u_\theta(t)\)</span> es la red neuronal. Entonces <span class="math inline">\(y_\theta(t)\)</span> satisface la condición de contorno <span class="math inline">\(y_\theta(0) = 1\)</span> por construcción y no necesitamos agregar un término correspondiente a la pérdida.</p>
<p>Para una EDP, se puede construir <span class="math inline">\(u_\theta(t,x) = g(x,t) + H(x,t)\tilde{u}\theta(t,x)\)</span> donde <span class="math inline">\(g\)</span> es una función simple que satisface las condiciones de contorno y <span class="math inline">\(H\)</span> es una función que se anula en el contorno, asegurando que <span class="math inline">\(u\theta\)</span> coincida con <span class="math inline">\(g\)</span> en el contorno para cualquier <span class="math inline">\(\tilde{u}_\theta\)</span>.</p>
</section>
</section>
<section id="comparación-con-métodos-numéricos-tradicionales" class="level1">
<h1>Comparación con Métodos Numéricos Tradicionales</h1>
<ul>
<li><p>Los métodos tradicionales aproximan la solución en una malla o cuadrícula discreta (p.&nbsp;ej. valores en puntos de la cuadrícula o elementos), lo que requiere un mallado cuidadoso del dominio. En contraste, una PINN representa la solución como una función continua (la red neuronal) definida sobre el dominio. Esto hace que las PINNs sean efectivamente libres de malla: No hay una malla fija, solo puntos de colocación que pueden muestrearse flexiblemente. La PINN entrenada incorpora automáticamente la interpolación de la solución.</p></li>
<li><p>Ambos enfoques finalmente imponen la misma física, pero el mecanismo difiere. Los métodos de diferencias/elementos finitos imponen la ecuación diferencial localmente en la malla mediante ecuaciones algebraicas discretas (p.&nbsp;ej. aproximando derivadas por restas entre puntos). Las PINNs imponen las ecuaciones “débilmente” a través de la función de pérdida. En este sentido, en una PINN no se garantiza que la solución satisfaga la EDP exactamente en cada paso, pero el proceso de entrenamiento intenta hacer que el residuo sea pequeño en todas partes. Con suficiente entrenamiento y capacidad, una PINN puede lograr un residuo bajo comparable al error de discretización en los métodos tradicionales. Un beneficio distintivo de las PINNs es que incorporan fácilmente física o restricciones adicionales – solo se añaden términos a la pérdida. Por ejemplo, si se tiene una ley de conservación más una restricción adicional (como la conservación de energía o una EDO suplementaria para un parámetro), se pueden incluir ambas en la pérdida sin cambiar fundamentalmente la estructura del solucionador. Los solucionadores tradicionales requerirían escribir nuevo código o acoplar ecuaciones explícitamente.</p></li>
<li><p>Los métodos clásicos imponen condiciones iniciales y de contorno exactamente por construcción (p.&nbsp;ej. estableciendo valores de nodos de contorno o celdas fantasma en diferencias finitas, o aplicando condiciones de contorno esenciales en elementos finitos). Las PINNs típicamente imponen estas condiciones a través de términos de penalización en la pérdida (restricciones suaves). Esto significa que durante el entrenamiento, se alienta a la red a satisfacer las CC pero podría no satisfacerlas exactamente hasta la convergencia. En la práctica, se puede imponer alta precisión en los contornos dando a los términos de CC un peso mayor o codificando la red para satisfacerlas como se discutió anteriormente. La flexibilidad de las PINNs permite manejar condiciones de contorno complejas o irregulares de forma natural muestreando puntos en esos contornos, incluso si los datos del contorno solo se dan en puntos discretos o ruidosos. Por ejemplo, si una temperatura de contorno se proporciona como datos experimentales (estocásticos con ruido), una PINN puede ingerir esos puntos de datos directamente en la pérdida, mientras que un solucionador clásico requeriría una interpolación de esas mediciones ruidosas en los nodos de la cuadrícula de contorno y podría tener dificultades con la incertidumbre.</p></li>
<li><p>Generalidad y Problemas Inversos: Un área donde las PINNs brillan es en su generalidad. Un único marco de código PINN puede, con cambios mínimos, aplicarse a una amplia variedad de EDPs diferentes – solo se necesita especificar el residuo de la EDP y proporcionar datos/condiciones. En contraste, los métodos tradicionales a menudo requieren derivar y codificar un nuevo solucionador o al menos rederivar formas débiles, jacobianos, etc., para cada nueva ecuación. Además, las PINNs manejan naturalmente problemas inversos (estimación de parámetros, identificación de sistemas) tratando los parámetros desconocidos como variables aprendibles y añadiendo términos de pérdida apropiados. Los métodos clásicos también pueden abordar problemas inversos, pero típicamente a través de bucles externos iterativos (p.&nbsp;ej. resolviendo el problema directo repetidamente mientras se ajustan los parámetros, o usando ecuaciones adjuntas). Las PINNs integran esto en un flujo de trabajo unificado, entrenando los parámetros junto con la solución. Esto puede simplificar el código y aprovechar el poder de los optimizadores de aprendizaje profundo para problemas inversos complejos que podrían ser difíciles de resolver para los métodos tradicionales. Adicionalmente, las PINNs pueden incorporar datos experimentales directamente (incluso si son dispersos), mezclando datos con ecuaciones gobernantes de manera fluida – algo que los solucionadores tradicionales no hacen de forma nativa.</p></li>
</ul>
<section id="precisión-y-convergencia" class="level2">
<h2 class="anchored" data-anchor-id="precisión-y-convergencia">Precisión y Convergencia</h2>
<p>Los métodos numéricos tradicionales tienen un comportamiento de convergencia bien entendido. Por ejemplo las diferencias finitas tienen un cierto orden de precisión dependiendo de la resolución de la malla y el esquema (refinar la malla produce sistemáticamente menos error, en el curso de métodos numéricos diríamos que el error de un cierto método escala como <span class="math inline">\(h^n\)</span>).</p>
<p>Las PINNs, al estar basadas en optimización, no tienen teoremas asociados que garantizan el mismo tipo de convergencia con el tamaño de la red o las iteraciones de entrenamiento de una manera simple. En la práctica, lograr una precisión de alto orden puede ser difícil para las PINNs. Un método clásico (de elementos finitos o espectral) de alto orden bien ajustado podría alcanzar un error muy bajo con un esfuerzo computacional moderado, mientras que una PINN podría requerir una red muy grande y un entrenamiento prolongado para igualar esa precisión.</p>
<p>La literatura informa que las PINNs a menudo tienen menor precisión que los esquemas de alto orden para problemas directos, especialmente en casos donde la solución tiene gradientes agudos o características multiescala. Existen intentos de mejorar la precisión de las PINN mediante muestreo adaptativo, mejores arquitecturas y formulaciones variacionales (p.&nbsp;ej. vpPINNs).</p>
</section>
</section>
<section id="ventajas-de-las-pinns-en-escenarios-con-escasez-de-datos-y-condiciones-complejas" class="level1">
<h1>Ventajas de las PINNs en Escenarios con Escasez de Datos y Condiciones Complejas</h1>
<p>En muchos problemas de física (especialmente en sistemas de ingeniería y biológicos), obtener grandes conjuntos de datos de alta calidad es difícil. Las redes neuronales tradicionales normalmente tendrían dificultades o se sobreajustarían con muy pocos datos. Las PINNs, sin embargo, aprovechan las leyes físicas como conocimiento previo, lo que reduce drásticamente la cantidad de datos necesarios para entrenar un modelo. Las ecuaciones gobernantes actúan como una regularización estricta que reduce el espacio de posibles soluciones, por lo que la red no necesita aprender la física solo a partir de los datos; ya la conoce.</p>
<p>Las PINNs sobresalen en escenarios donde se tiene un conocimiento parcial del sistema. Por ejemplo, se podría conocer la forma de la EDP (leyes de conservación) pero no una propiedad material específica o un término fuente; o se tienen algunos datos pero se sospecha que el modelo es incompleto. Las PINNs permiten una combinación fluida de modelado basado en datos con modelado basado en la física. Se pueden incluir parámetros desconocidos en <span class="math inline">\(\lambda\)</span> como parte de <span class="math inline">\(\theta\)</span> para ser aprendidos, o incluso representar un término fuente desconocido con una red adicional, todo restringido por la EDP principal. De esta manera, las PINNs pueden descubrir física faltante o calibrar modelos a los datos.</p>
<p>Finalmente, son aplicables a problemas de alta dimensión. Aunque las EDPs de alta dimensionalidad plantean desafíos para las PINNs (como lo hacen para cualquier método), las PINNs son uno de los enfoques prometedores para eludir la maldición de la dimensionalidad, especialmente cuando se combinan con técnicas como DeepONets u operadores neuronales. Las PINNs pueden ser más adecuadas para problemas de alta dimensionalidad que los métodos basados en cuadrículas, porque muestrear puntos en un espacio de alta dimensionalidad podría ser más barato que construir una cuadrícula de alta dimensionalidad (que crece exponencialmente). Además, las PINNs pueden integrar datos de multifidelidad (algunas simulaciones gruesas + algunas mediciones finas) incluyendo todas las fuentes de información en la pérdida.</p>
<!--
# Referencias y Lecturas Adicionales

La metodología PINN fue detallada por primera vez por Raissi et al. (2019), con desarrollos anteriores en Raissi et al. (2017) para resolver y descubrir EDPs a partir de datos. Ese trabajo demostró las PINNs en ecuaciones clásicas (Burgers, Schrödinger, Navier–Stokes) tanto para soluciones directas como para estimación inversa de parámetros. Desde entonces, han aparecido numerosas extensiones y estudios. Para una visión general completa, véase la revisión de Karniadakis et al. (2021/2023) que discute las PINNs y diversas mejoras. Investigaciones recientes han abordado fallos en el entrenamiento de PINN (p. ej. Wang et al., 2022, usando la teoría del Kernel Tangente Neural para entender la optimización de PINN) e introducido extensiones como XPINNs (PINNs descompuestas por dominio para entrenamiento paralelo en subdominios) y vpPINNs (PINNs variacionales) para mejorar la precisión. También ha habido aplicaciones exitosas en el descubrimiento de física oculta (p. ej. Chen et al., 2021, quienes combinaron PINNs con regresión dispersa para encontrar ecuaciones gobernantes a partir de datos escasos). Para orientación práctica, existen bibliotecas de código abierto como DeepXDE (Lu et al., 2021) y PyDEns que proporcionan implementaciones de PINN. Como este es un campo activo, se anima a los estudiantes a seguir la literatura más reciente – el rápido progreso sugiere que muchas limitaciones actuales de las PINNs continuarán abordándose, solidificando aún más su papel en la física computacional y la ingeniería.
-->


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiado");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiado");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>