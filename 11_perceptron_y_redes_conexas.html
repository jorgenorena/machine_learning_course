<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="es" xml:lang="es"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>El perceptrón, redes completamente conexas y pérdidas – Aprendizaje Automático para Físicos</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-e13f217ceb0135cb77e1494052e28e8a.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Sin resultados",
    "search-matching-documents-text": "documentos encontrados",
    "search-copy-link-title": "Copiar el enlace en la búsqueda",
    "search-hide-matches-text": "Ocultar resultados adicionales",
    "search-more-match-text": "resultado adicional en este documento",
    "search-more-matches-text": "resultados adicionales en este documento",
    "search-clear-button-title": "Borrar",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancelar",
    "search-submit-button-title": "Enviar",
    "search-label": "Buscar"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Aprendizaje Automático para Físicos</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Buscar"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Navegación de palanca" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-clases" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Clases</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-clases">    
        <li class="dropdown-header">Probabilidad y Estadística</li>
        <li>
    <a class="dropdown-item" href="./1_probabilidad.html">
 <span class="dropdown-text">1. Probabilidad</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./2_distribuciones_y_estimacion.html">
 <span class="dropdown-text">2. Distribuciones y Estimación</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./3_verosimilitud.html">
 <span class="dropdown-text">3. Verosimilitud</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./4_minimos_cuadrados_intervalos.html">
 <span class="dropdown-text">4. Mínimos Cuadrados e Intervalos</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./5_testeo_de_hipotesis_entropia.html">
 <span class="dropdown-text">5. Testeo de Hipótesis y Entropía</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">Aprendizaje Automático</li>
        <li>
    <a class="dropdown-item" href="./6_aprendizaje_automatico_regresion_lineal.html">
 <span class="dropdown-text">6. Aprendizaje Automático y Regresión Lineal</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./7_clasificacion.html">
 <span class="dropdown-text">7. Clasificación</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./8_seleccion_de_modelos.html">
 <span class="dropdown-text">8. Selección de Modelos</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./9_arboles_bosques_boosting.html">
 <span class="dropdown-text">9. Árboles, Bosques y Boosting</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./10_svm_y_no_supervisado.html">
 <span class="dropdown-text">10. SVM y Aprendizaje No Supervisado</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">Redes Neuronales</li>
        <li>
    <a class="dropdown-item" href="./11_perceptron_y_redes_conexas.html">
 <span class="dropdown-text">11. Perceptrón y Redes Conexas y Pérdidas</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./12_gradient_descent_and_autodiff.html">
 <span class="dropdown-text">12. Descenso de Gradiente y Auto-diferenciación</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./13_evaluacion_y_regularizacion_de_nn.html">
 <span class="dropdown-text">13. Evaluación y Regularización de redes neuronales</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./14_redes_convolucionales.html">
 <span class="dropdown-text">14. Redes Convolucionales</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./15_redes_residuales.html">
 <span class="dropdown-text">15. Redes Residuales</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./16_transformadores.html">
 <span class="dropdown-text">16. Transformadores y el mecanismo de atención</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">Algunas aplicaciones</li>
        <li>
    <a class="dropdown-item" href="./17_flujos_normalizantes.html">
 <span class="dropdown-text">17. Flujos Normalizantes</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./18_PINNs.html">
 <span class="dropdown-text">18. Redes Neuronales Informadas por la Física (PINNs)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./19_Operador_Neural.html">
 <span class="dropdown-text">19. Operadores Neuronales</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="./about.html"> 
<span class="menu-text">Acerca de</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Alternar barra lateral" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./11_perceptron_y_redes_conexas.html">Redes Neuronales</a></li><li class="breadcrumb-item"><a href="./11_perceptron_y_redes_conexas.html">11. Perceptrón, Redes Conexas y Pérdidas</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Alternar barra lateral" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Probabilidad y Estadística</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Alternar sección">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1_probabilidad.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. Probabilidad</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2_distribuciones_y_estimacion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. Distribuciones y Estimación</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3_verosimilitud.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3. Verosimilitud</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./4_minimos_cuadrados_intervalos.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4. Mínimos Cuadrados e Intervalos</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./5_testeo_de_hipotesis_entropia.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5. Testeo de Hipótesis y Entropía</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Aprendizaje Automático</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Alternar sección">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./6_aprendizaje_automatico_regresion_lineal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6. Aprendizaje Automático y Regresión Lineal</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./7_clasificacion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7. Clasificación</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./8_seleccion_de_modelos.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8. Selección de Modelos</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./9_arboles_bosques_boosting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9. Árboles, Bosques y Boosting</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10_svm_y_no_supervisado.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">10. SVM y Aprendizaje No Supervisado</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Redes Neuronales</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Alternar sección">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11_perceptron_y_redes_conexas.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">11. Perceptrón, Redes Conexas y Pérdidas</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12_gradient_descent_and_autodiff.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">12. Descenso de Gradiente y Auto-diferenciación</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13_evaluacion_y_regularizacion_de_nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">13. Evaluación y Regularización de redes neuronales</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14_redes_convolucionales.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">14. Redes Convolucionales</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15_redes_residuales.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">15. Redes Residuales</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16_transformadores.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">16. Transformadores y el mecanismo de atención</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Algunas aplicaciones</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Alternar sección">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17_flujos_normalizantes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">17. Flujos Normalizantes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18_PINNs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">18. Redes Neuronales Informadas por la Física (PINNs)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19_Operador_Neural.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">19. Operadores Neuronales</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">En esta página</h2>
   
  <ul>
  <li><a href="#introducción" id="toc-introducción" class="nav-link active" data-scroll-target="#introducción">Introducción</a>
  <ul class="collapse">
  <li><a href="#arquitecturas-importantes" id="toc-arquitecturas-importantes" class="nav-link" data-scroll-target="#arquitecturas-importantes">Arquitecturas importantes</a></li>
  <li><a href="#aplicaciones-a-la-física" id="toc-aplicaciones-a-la-física" class="nav-link" data-scroll-target="#aplicaciones-a-la-física">Aplicaciones a la física</a></li>
  </ul></li>
  <li><a href="#el-perceptrón-y-redes-neuronales-superficiales" id="toc-el-perceptrón-y-redes-neuronales-superficiales" class="nav-link" data-scroll-target="#el-perceptrón-y-redes-neuronales-superficiales">El Perceptrón y Redes Neuronales Superficiales</a>
  <ul class="collapse">
  <li><a href="#el-perceptrón-definición-y-clasificación-lineal" id="toc-el-perceptrón-definición-y-clasificación-lineal" class="nav-link" data-scroll-target="#el-perceptrón-definición-y-clasificación-lineal">El Perceptrón: Definición y Clasificación Lineal</a></li>
  <li><a href="#redes-con-una-capa-oculta" id="toc-redes-con-una-capa-oculta" class="nav-link" data-scroll-target="#redes-con-una-capa-oculta">Redes con Una Capa Oculta</a></li>
  <li><a href="#teorema-de-aproximación-universal-para-redes-superficiales" id="toc-teorema-de-aproximación-universal-para-redes-superficiales" class="nav-link" data-scroll-target="#teorema-de-aproximación-universal-para-redes-superficiales">Teorema de Aproximación Universal para Redes Superficiales</a></li>
  <li><a href="#redes-superficiales-con-múltiples-salidas-mlp-de-una-capa-oculta" id="toc-redes-superficiales-con-múltiples-salidas-mlp-de-una-capa-oculta" class="nav-link" data-scroll-target="#redes-superficiales-con-múltiples-salidas-mlp-de-una-capa-oculta">Redes Superficiales con Múltiples Salidas (MLP de una Capa Oculta)</a></li>
  </ul></li>
  <li><a href="#perceptrones-multicapa-mlps-profundos" id="toc-perceptrones-multicapa-mlps-profundos" class="nav-link" data-scroll-target="#perceptrones-multicapa-mlps-profundos">Perceptrones Multicapa (MLPs) Profundos</a>
  <ul class="collapse">
  <li><a href="#arquitectura-multicapa" id="toc-arquitectura-multicapa" class="nav-link" data-scroll-target="#arquitectura-multicapa">Arquitectura Multicapa</a></li>
  <li><a href="#mlps-con-relu-como-funciones-afines-por-tramos-piecewise-affine" id="toc-mlps-con-relu-como-funciones-afines-por-tramos-piecewise-affine" class="nav-link" data-scroll-target="#mlps-con-relu-como-funciones-afines-por-tramos-piecewise-affine">MLPs con ReLU como Funciones Afines por Tramos (Piecewise Affine)</a></li>
  <li><a href="#capacidad-de-representación-parámetros-vs.-regiones-lineales" id="toc-capacidad-de-representación-parámetros-vs.-regiones-lineales" class="nav-link" data-scroll-target="#capacidad-de-representación-parámetros-vs.-regiones-lineales">Capacidad de Representación: Parámetros vs.&nbsp;Regiones Lineales</a></li>
  </ul></li>
  <li><a href="#funciones-de-pérdida" id="toc-funciones-de-pérdida" class="nav-link" data-scroll-target="#funciones-de-pérdida">Funciones de Pérdida</a>
  <ul class="collapse">
  <li><a href="#derivación-general-desde-máxima-verosimilitud" id="toc-derivación-general-desde-máxima-verosimilitud" class="nav-link" data-scroll-target="#derivación-general-desde-máxima-verosimilitud">Derivación General desde Máxima Verosimilitud</a></li>
  <li><a href="#entropía-cruzada-para-clasificación-cross-entropy-loss" id="toc-entropía-cruzada-para-clasificación-cross-entropy-loss" class="nav-link" data-scroll-target="#entropía-cruzada-para-clasificación-cross-entropy-loss">Entropía Cruzada para Clasificación (Cross-Entropy Loss)</a>
  <ul class="collapse">
  <li><a href="#clasificación-binaria" id="toc-clasificación-binaria" class="nav-link" data-scroll-target="#clasificación-binaria">Clasificación Binaria:</a></li>
  </ul></li>
  <li><a href="#error-cuadrático-medio-mse-para-regresión" id="toc-error-cuadrático-medio-mse-para-regresión" class="nav-link" data-scroll-target="#error-cuadrático-medio-mse-para-regresión">Error Cuadrático Medio (MSE) para Regresión</a></li>
  <li><a href="#receta-general-para-construir-funciones-de-pérdida-usando-la-estimación-de-máxima-verosimilitud" id="toc-receta-general-para-construir-funciones-de-pérdida-usando-la-estimación-de-máxima-verosimilitud" class="nav-link" data-scroll-target="#receta-general-para-construir-funciones-de-pérdida-usando-la-estimación-de-máxima-verosimilitud">Receta General para Construir Funciones de Pérdida usando la estimación de máxima verosimilitud</a></li>
  </ul></li>
  <li><a href="#ejercicios-sugeridos" id="toc-ejercicios-sugeridos" class="nav-link" data-scroll-target="#ejercicios-sugeridos">Ejercicios sugeridos</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./11_perceptron_y_redes_conexas.html">Redes Neuronales</a></li><li class="breadcrumb-item"><a href="./11_perceptron_y_redes_conexas.html">11. Perceptrón, Redes Conexas y Pérdidas</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">El perceptrón, redes completamente conexas y pérdidas</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introducción" class="level1">
<h1>Introducción</h1>
<section id="arquitecturas-importantes" class="level2">
<h2 class="anchored" data-anchor-id="arquitecturas-importantes">Arquitecturas importantes</h2>
<p>Cada una de estas arquitecturas responde a una estructura de datos o problema específico, y se han vuelto herramientas clave en física computacional:</p>
<ul>
<li><strong>Redes completamente conectadas (MLPs)</strong>: consisten en capas de neuronas donde cada una está conectada a todas las de la capa anterior. Son adecuadas para datos tabulares o de baja dimensión. Se utilizan para aproximación de funciones, regresión y clasificación simple. Son la base de muchas otras arquitecturas más complejas.</li>
<li><strong>Redes convolucionales</strong>: se basan en aplicar filtros (kernels) que exploran estructuras locales en los datos. Son especializadas en datos espaciales como imágenes. Son comunes en reconocimiento de patrones y análisis de mapas de temperatura o distribución de materia en cosmología.</li>
<li><strong>Redes recurrentes y variantes</strong>: incorporan ciclos en su arquitectura para mantener una memoria de estados anteriores. Se usan para procesamiento secuencial y temporal. Útiles para series temporales físicas, como evolución temporal de sistemas o señales en experimentos.</li>
<li><strong>Redes residuales y profundas</strong>: introducen conexiones identitarias (skip connections) que permiten pasar información sin alteración a capas posteriores. Facilitan el entrenamiento de redes muy profundas al evitar el problema del desvanecimiento del gradiente. Son utilizadas en física para modelos predictivos muy complejos con datos jerárquicos o multiescalares.</li>
<li><strong>Transformadores</strong>: utilizan mecanismos de atención que asignan pesos a diferentes partes de la entrada según su relevancia contextual. Son la arquitectura dominante para datos secuenciales, procesamiento de lenguaje y más allá. Recientemente aplicados en física para modelar correlaciones de largo alcance y aprendizaje de representaciones de sistemas físicos.</li>
</ul>
</section>
<section id="aplicaciones-a-la-física" class="level2">
<h2 class="anchored" data-anchor-id="aplicaciones-a-la-física">Aplicaciones a la física</h2>
<p>Cada una de las siguientes áreas ha sido impulsada recientemente por el uso de redes neuronales, en muchos casos superando métodos tradicionales:</p>
<ul>
<li><strong>Predicción de parámetros cosmológicos a partir de mapas del cielo</strong>: redes profundas permiten inferir parámetros fundamentales del modelo cosmológico a partir de datos como el fondo cósmico de microondas o mapas de lentes gravitacionales.</li>
<li><strong>Reconstrucción de dinámica de fluidos mediante operadores neuronales</strong>: los operadores neuronales permiten modelar sistemas complejos como turbulencia sin resolver explícitamente todas las escalas del sistema.</li>
<li><strong>Reducción de ruido en datos experimentales (e.g., espectros)</strong>: redes autoencoder y convolucionales permiten eliminar ruido instrumental y mejorar la calidad de señales físicas.</li>
<li><strong>Resolución de EDPs en geometrías complejas</strong>: métodos como PINNs (Physics-Informed Neural Networks) resuelven ecuaciones diferenciales directamente en dominios arbitrarios, incluyendo condiciones de frontera no triviales.</li>
</ul>
</section>
</section>
<section id="el-perceptrón-y-redes-neuronales-superficiales" class="level1">
<h1>El Perceptrón y Redes Neuronales Superficiales</h1>
<p>Ahora introducimos el perceptrón como bloque fundamental y extendemos la idea a redes neuronales superficiales (con una capa oculta).</p>
<section id="el-perceptrón-definición-y-clasificación-lineal" class="level2">
<h2 class="anchored" data-anchor-id="el-perceptrón-definición-y-clasificación-lineal">El Perceptrón: Definición y Clasificación Lineal</h2>
<p>El perceptrón, es un algoritmo para clasificación binaria supervisada. Calcula una <em>salida</em> <span class="math inline">\(z\)</span> como una combinación lineal de las entradas <span class="math inline">\(\mathbf{x} \in \mathbb{R}^d\)</span>, más un sesgo <span class="math inline">\(b\)</span>:</p>
<p><span class="math display">\[
z = \mathbf{w}^T \mathbf{x} + b = \sum_{i=1}^d w_i x_i + b\,,
\]</span></p>
<p>donde <span class="math inline">\(\mathbf{w} \in \mathbb{R}^d\)</span> es el vector de pesos y <span class="math inline">\(b \in \mathbb{R}\)</span> es el sesgo.</p>
<p>Luego, aplica una <strong>función de activación no lineal</strong>. Originalmente, se usaba la función escalón (o Heaviside) <span class="math inline">\(\phi(z) = 1\)</span> si <span class="math inline">\(z \geq 0\)</span> y <span class="math inline">\(\phi(z) = 0\)</span> si <span class="math inline">\(z &lt; 0\)</span>. Entonces la salida del perceptrón es <span class="math inline">\(\hat{y} = \phi(z)\)</span>.</p>
<p>La ecuación <span class="math inline">\(\mathbf{w}^T \mathbf{x} + b = 0\)</span> define un <strong>hiperplano</strong> en el espacio de entrada <span class="math inline">\(\mathbb{R}^d\)</span>. Este hiperplano divide el espacio en dos semiespacios, correspondiendo a las dos clases. Por lo tanto, el perceptrón original es un <strong>clasificador lineal</strong> y solo puede separar datos que sean linealmente separables.</p>
<p>Gráficamente, dibujamos el perceptrón como teniendo <span class="math inline">\(d\)</span> líneas, una por cada entrada, y un sesgo, y una línea de salida.</p>
<p>Las redes neuronales modernas utilizan funciones de activación diferenciables (casi en todas partes). Una de las más populares es la <strong>Unidad Lineal Rectificada (ReLU)</strong>:</p>
<p><span class="math display">\[
\phi_{\text{ReLU}}(z) = \max(0, z) = \begin{cases} z, &amp; \text{si } z &gt; 0\\ 0, &amp; \text{si } z \le 0 \end{cases}\,.
\]</span></p>
<ul>
<li>Si solo se usaran transformaciones lineales (sin activación no lineal o con activación identidad), una red de múltiples capas colapsaría en una única transformación lineal equivalente: <span class="math inline">\(y = W_{\text{eff}}^T\mathbf{x} + b_{\text{eff}}\)</span>. La no linealidad es esencial para la capacidad expresiva de las redes profundas, como veremos más adelante.</li>
<li>La ReLU, a pesar de su simplicidad, introduce “quiebres” (puntos no diferenciables en <span class="math inline">\(z=0\)</span>) que permiten a las redes neuronales construir <strong>funciones lineales a trozos (piecewise-linear)</strong> muy complejas.</li>
<li>Cada neurona con activación ReLU divide el espacio de entrada según el hiperplano <span class="math inline">\(\mathbf{w}^T \mathbf{x} + b = 0\)</span>: en una región la neurona está “activa” (salida <span class="math inline">\(&gt; 0\)</span>) y en la otra está “inactiva” (salida <span class="math inline">\(= 0\)</span>).</li>
</ul>
<p><em>Nota: Otras activaciones comunes incluyen la sigmoide y la tangente hiperbólica (tanh), aunque ReLU y sus variantes son a menudo preferidas en redes profundas.</em></p>
</section>
<section id="redes-con-una-capa-oculta" class="level2">
<h2 class="anchored" data-anchor-id="redes-con-una-capa-oculta">Redes con Una Capa Oculta</h2>
<p>Combinando múltiples neuronas (usualmente con activaciones como ReLU) en una “capa oculta”, podemos construir redes más potentes. Una red neuronal superficial (shallow network) con una capa oculta de <span class="math inline">\(H\)</span> neuronas y una capa de salida con una única neurona (para regresión o clasificación binaria) calcula:</p>
<p><span class="math display">\[
\hat{y}(\mathbf{x}) = \mathbf{W}_{(2)}^T \phi(\mathbf{W}_{(1)}\mathbf{x} + \mathbf{b}_{(1)}) + b_{(2)}\,,
\]</span></p>
<p>donde: - <span class="math inline">\(\mathbf{W}_{(1)} \in \mathbb{R}^{H \times d}\)</span> y <span class="math inline">\(\mathbf{b}_{(1)} \in \mathbb{R}^H\)</span> son los pesos y sesgos de la <em>capa oculta</em>. - <span class="math inline">\(\phi\)</span> es la función de activación (e.g., ReLU), aplicada elemento a elemento. - <span class="math inline">\(\mathbf{W}_{(2)} \in \mathbb{R}^H\)</span> y <span class="math inline">\(b_{(2)} \in \mathbb{R}\)</span> son los pesos y el sesgo de la <em>capa de salida</em>.</p>
<p>Gráficamente, representamos esta red como un conjunto de <span class="math inline">\(H\)</span> perceptrones, donde cada uno tiene <span class="math inline">\(d\)</span> líneas, una por cada entrada, un sesgo, y una línea de salida. Esas <span class="math inline">\(H\)</span> salidas van todas a un único perceptrón de salida, que tiene <span class="math inline">\(m\)</span> conexiones de entrada y <span class="math inline">\(1\)</span> conexión de salida.</p>
<p>Esta red divide el espacio de entrada <span class="math inline">\(\mathbb{R}^d\)</span> en múltiples <strong>regiones poliédricas convexas</strong>. Dentro de cada región, la función <span class="math inline">\(\hat{y}(\mathbf{x})\)</span> se comporta como una función afín (lineal más una constante). Los límites entre estas regiones están determinados por los hiperplanos <span class="math inline">\(\mathbf{w}_i^{(1)} \cdot \mathbf{x} + b_i^{(1)} = 0\)</span> de las neuronas ocultas.</p>
<p>El número máximo de regiones lineales que se pueden crear con <span class="math inline">\(H\)</span> neuronas en <span class="math inline">\(d\)</span> dimensiones tiene un límite superior dado por: <span class="math display">\[
R(d,H) = \sum_{k=0}^d \binom{H}{k}\,,
\]</span> Al aumentar el número de neuronas <span class="math inline">\(H\)</span>, la red puede aproximar funciones cada vez más complejas.</p>
</section>
<section id="teorema-de-aproximación-universal-para-redes-superficiales" class="level2">
<h2 class="anchored" data-anchor-id="teorema-de-aproximación-universal-para-redes-superficiales">Teorema de Aproximación Universal para Redes Superficiales</h2>
<p>La capacidad de las redes neuronales para aproximar funciones está formalizada por el <strong>Teorema de Aproximación Universal</strong>. Para redes superficiales (una capa oculta), establece (informalmente):</p>
<div class="callout callout-style-default callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Teorema de Aproximación Universal
</div>
</div>
<div class="callout-body-container callout-body">
<p>Una red neuronal feedforward con <strong>una sola capa oculta</strong> que contiene un número finito de neuronas con una función de activación <strong>no lineal</strong>, puede aproximar cualquier función continua <span class="math inline">\(f\)</span> sobre un subconjunto compacto de <span class="math inline">\(\mathbb{R}^d\)</span> con cualquier grado de precisión deseado <span class="math inline">\(\varepsilon &gt; 0\)</span>. Es decir, existe una red <span class="math inline">\(\hat{y}(\mathbf{x})\)</span> tal que: <span class="math display">\[
\sup_{\mathbf{x}} |f(\mathbf{x}) - \hat{y}(\mathbf{x})| &lt; \varepsilon\,.
\]</span></p>
</div>
</div>
<p>No demostramos el teorema, pero damos una intuición: En una dimensión, cualquier función <span class="math inline">\(f\)</span> en una región pequeña puede ser aproximada por una recta. Entonces, al usar muchas rectas, podemos aproximar cualquier función. En más dimensiones podemos hacer lo mismo con hiperplanos.</p>
<p>Este es un teorema de <strong>existencia</strong>: garantiza que <em>existe</em> una red capaz de la aproximación, pero no dice cómo encontrar sus pesos y sesgos ni cuál es el número mínimo de neuronas <span class="math inline">\(H\)</span> necesario (que puede ser muy grande, potencialmente creciendo exponencialmente con la dimensión <span class="math inline">\(d\)</span> o la complejidad de <span class="math inline">\(f\)</span>).</p>
</section>
<section id="redes-superficiales-con-múltiples-salidas-mlp-de-una-capa-oculta" class="level2">
<h2 class="anchored" data-anchor-id="redes-superficiales-con-múltiples-salidas-mlp-de-una-capa-oculta">Redes Superficiales con Múltiples Salidas (MLP de una Capa Oculta)</h2>
<p>Existen muchos casos en los que queremos predecir más de un valor, por ejemplo, predecir la temperatura y la humedad en un punto, o predecir la posición de un objeto en un video. A esto se lo llama regresión vectorial. Otro ejemplo es cuando tenemos varias clases y queremos predecir la probabilidad de pertenecer a cada una.</p>
<p>Para tareas con múltiples salidas (e.g., clasificación multiclase, regresión vectorial), la capa de salida tendrá <span class="math inline">\(m &gt; 1\)</span> neuronas. La arquitectura común es un <strong>Perceptrón Multicapa (MLP)</strong> con una capa oculta:</p>
<p><span class="math display">\[
\mathbf{y} = \phi_{\text{salida}}(\mathbf{W}_{(2)} \phi_{\text{hidden}}(\mathbf{W}^{(1)}\mathbf{x} + \mathbf{b}^{(1)}) + b^{(2)})\,,
\]</span></p>
<p>donde:</p>
<ul>
<li><span class="math inline">\(\mathbf{W}^{(1)} \in \mathbb{R}^{H \times d}\)</span>, <span class="math inline">\(\mathbf{b}^{(1)} \in \mathbb{R}^H\)</span> (Capa oculta con <span class="math inline">\(H\)</span> neuronas).</li>
<li><span class="math inline">\(\phi_{\text{hidden}}\)</span> es la activación de la capa oculta (e.g., ReLU), aplicada elemento a elemento.</li>
<li><span class="math inline">\(\mathbf{W}^{(2)} \in \mathbb{R}^{m \times H}\)</span>, <span class="math inline">\(\mathbf{b}^{(2)} \in \mathbb{R}^m\)</span> (Capa de salida con <span class="math inline">\(m\)</span> neuronas).</li>
<li><span class="math inline">\(\phi_{\text{salida}}\)</span> es la activación de la capa de salida. Como la capa de salida nos dabe dar el resultado que buscamos, es posible que la función de activación de la capa de salida sea diferente (<span class="math inline">\(\phi_{\text{salida}}\)</span>) a la de las capas ocultas. Por ejemplo, si estamos resolviendo un problema de regresión, la función de activación de la capa de salida será la identidad ya que nos interesa un valor en todos los reales. Mientras que si estamos resolviendo un problema de clasificación multiclase, la función de activación de la capa de salida será la Softmax, es decir <span class="math display">\[
  \phi_{\text{salida}}(\mathbf{z}^{(L)}) = \frac{e^{\mathbf{z}^{(L)}}}{\sum_{i=1}^{n_L} e^{z_i^{(L)}}}\,,
  \]</span> que se interpreta como la probabilidad de que el punto pertenezca a la clase <span class="math inline">\(k\)</span>.</li>
</ul>
<p>Gráficamente, representamos esta red como un conjunto de <span class="math inline">\(H\)</span> perceptrones, donde cada uno tiene <span class="math inline">\(d\)</span> conexiones de entrada y <span class="math inline">\(1\)</span> conexión de salida. Esas <span class="math inline">\(H\)</span> salidas van todas a un único perceptrón de salida, que tiene <span class="math inline">\(m\)</span> conexiones de entrada y <span class="math inline">\(1\)</span> conexión de salida.</p>
<div id="cell-fig-mlp" class="cell" data-execution_count="2">
<div class="cell-output cell-output-display">
<div id="fig-mlp" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mlp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="11_perceptron_y_redes_conexas_files/figure-html/fig-mlp-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mlp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;1: Arquitectura de una MLP con una capa oculta
</figcaption>
</figure>
</div>
</div>
</div>
<p>El número total de parámetros (pesos y sesgos) en esta red es: <span class="math display">\[
\#\text{parámetros} = \underbrace{(d \times H + H)}_{\text{Capa 1}} + \underbrace{(H \times m + m)}_{\text{Capa 2}}
\]</span> La expresividad y capacidad de la red aumentan con el número de neuronas ocultas <span class="math inline">\(H\)</span>, a costa de un mayor número de parámetros a aprender.</p>
<div id="cell-fig-decision-boundary" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_moons</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Implementación manual de red neuronal</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> relu(x):</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.maximum(<span class="dv">0</span>, x)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(x):</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Parámetros de la red (pesos y sesgos)</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>W1 <span class="op">=</span> np.array([[<span class="fl">0.5</span>, <span class="op">-</span><span class="fl">0.14</span>], [<span class="fl">0.65</span>, <span class="fl">1.52</span>], [<span class="fl">0.2</span>, <span class="op">-</span><span class="fl">0.2</span>]]) <span class="co"># Capa oculta (3 neuronas)</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>b1 <span class="op">=</span> [<span class="fl">0.49</span>, <span class="fl">0.82</span>, <span class="op">-</span><span class="fl">0.2</span>]</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>W2 <span class="op">=</span> np.random.RandomState().randn(<span class="dv">1</span>, <span class="dv">3</span>)  <span class="co"># Capa de salida</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>b2 <span class="op">=</span> np.random.RandomState().randn(<span class="dv">1</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(x):</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Capa oculta</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    x_np <span class="op">=</span> np.array(x) <span class="co"># Ensure x is a numpy array for matmul</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    z1 <span class="op">=</span> x_np <span class="op">@</span> W1.T <span class="op">+</span> b1</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    a1 <span class="op">=</span> relu(z1)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Capa de salida</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    z2 <span class="op">=</span> a1 <span class="op">@</span> W2.T <span class="op">+</span> b2</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sigmoid(z2)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Crear grid</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>xx, yy <span class="op">=</span> np.meshgrid(np.linspace(<span class="op">-</span><span class="fl">2.5</span>, <span class="fl">1.5</span>, <span class="dv">100</span>), np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">100</span>))</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Predecir en toda la grid para la salida de la red</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> np.array([predict(np.array([x_val, y_val])) <span class="cf">for</span> x_val, y_val <span class="kw">in</span> <span class="bu">zip</span>(xx.ravel(), yy.ravel())])</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> Z.reshape(xx.shape)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">6</span>))</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Graficar la salida continua de la red</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>contour <span class="op">=</span> plt.contourf(xx, yy, Z, levels<span class="op">=</span><span class="dv">50</span>, cmap<span class="op">=</span><span class="st">'inferno'</span>, alpha<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Definir colores y estilos para las intersecciones</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">'#2E86C1'</span>, <span class="st">'#E74C3C'</span>, <span class="st">'#F1C40F'</span>]</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Grafico de los hiperplanos superpuestos</span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(W1.shape[<span class="dv">0</span>]):</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>    w <span class="op">=</span> W1[i]</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>    bias <span class="op">=</span> b1[i]</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>    color <span class="op">=</span> colors[i <span class="op">%</span> <span class="bu">len</span>(colors)]</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot a slightly thicker white line first for outline effect</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">abs</span>(w[<span class="dv">1</span>]) <span class="op">&gt;</span> <span class="fl">1e-6</span>:</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>        y_hyper <span class="op">=</span> (<span class="op">-</span>w[<span class="dv">0</span>] <span class="op">*</span> xx[<span class="dv">0</span>,:] <span class="op">-</span> bias) <span class="op">/</span> w[<span class="dv">1</span>]</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>        plt.plot(xx[<span class="dv">0</span>,:], y_hyper, color<span class="op">=</span><span class="st">'white'</span>, linewidth<span class="op">=</span><span class="fl">3.5</span>) </span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>        plt.plot(xx[<span class="dv">0</span>,:], y_hyper, color<span class="op">=</span>color, linewidth<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>                 label<span class="op">=</span><span class="ss">f'Neurona </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span>) <span class="co"># Label only the colored line</span></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> <span class="bu">abs</span>(w[<span class="dv">0</span>]) <span class="op">&gt;</span> <span class="fl">1e-6</span>:</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>        x_hyper <span class="op">=</span> <span class="op">-</span>bias <span class="op">/</span> w[<span class="dv">0</span>]</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>        plt.axvline(x<span class="op">=</span>x_hyper, color<span class="op">=</span><span class="st">'white'</span>, linewidth<span class="op">=</span><span class="fl">3.5</span>)</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>        plt.axvline(x<span class="op">=</span>x_hyper, color<span class="op">=</span>color, linewidth<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>                    label<span class="op">=</span><span class="ss">f'Neurona </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> (vertical)'</span>)</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Salida de la Red y Hiperplanos de la Capa Oculta"</span>)</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$x_1$"</span>)</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$x_2$"</span>)</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'lower right'</span>, title<span class="op">=</span><span class="st">"Hiperplanos"</span>)</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>plt.axvline(x<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>plt.xlim(xx.<span class="bu">min</span>(), xx.<span class="bu">max</span>())</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>plt.ylim(yy.<span class="bu">min</span>(), yy.<span class="bu">max</span>())</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-decision-boundary" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-decision-boundary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="11_perceptron_y_redes_conexas_files/figure-html/fig-decision-boundary-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-decision-boundary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;2: Regiones lineales para una red con una capa oculta de 3 neuronas. Cada hiperplano divide el espacio en dos regiones. Se grafican las intersecciones de los hiperplanos con el espacio (<span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>). Vemos que esto genera varias regiones lineales. Se puede ver que cada región es lineal porque la salida crece linealmente (no se curva).
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="perceptrones-multicapa-mlps-profundos" class="level1">
<h1>Perceptrones Multicapa (MLPs) Profundos</h1>
<p>Extendemos el concepto de redes superficiales a arquitecturas profundas, compuestas por múltiples capas ocultas.</p>
<section id="arquitectura-multicapa" class="level2">
<h2 class="anchored" data-anchor-id="arquitectura-multicapa">Arquitectura Multicapa</h2>
<p>Un Perceptrón Multicapa (MLP) profundo consta de una secuencia de capas. Si consideramos <span class="math inline">\(L\)</span> capas en total (incluyendo la de salida), la estructura es:</p>
<ul>
<li><p><strong>Capa de Entrada:</strong> Simplemente contiene el vector de entrada <span class="math inline">\(\mathbf{x} \in \mathbb{R}^{n_0}\)</span>, donde <span class="math inline">\(n_0=d\)</span> es la dimensionalidad de los datos. Definimos <span class="math inline">\(\mathbf{a}^{(0)} = \mathbf{x}\)</span>.</p></li>
<li><p><strong>Capas Ocultas (<span class="math inline">\(\ell = 1, \dots, L-1\)</span>):</strong> Cada capa oculta <span class="math inline">\(\ell\)</span> recibe las activaciones <span class="math inline">\(\mathbf{a}^{(\ell-1)} \in \mathbb{R}^{n_{\ell-1}}\)</span> de la capa anterior y calcula sus propias activaciones <span class="math inline">\(\mathbf{a}^{(\ell)} \in \mathbb{R}^{n_\ell}\)</span> mediante:</p>
<ol type="1">
<li><strong>Transformación Afín:</strong> <span class="math inline">\(\mathbf{z}^{(\ell)} = W^{(\ell)} \mathbf{a}^{(\ell-1)} + \mathbf{b}^{(\ell)}\)</span></li>
<li><strong>Activación No Lineal:</strong> <span class="math inline">\(\mathbf{a}^{(\ell)} = \phi(\mathbf{z}^{(\ell)})\)</span></li>
</ol>
<p>Donde:</p>
<ul>
<li><span class="math inline">\(W^{(\ell)} \in \mathbb{R}^{n_\ell \times n_{\ell-1}}\)</span> es la matriz de pesos de la capa <span class="math inline">\(\ell\)</span>.</li>
<li><span class="math inline">\(\mathbf{b}^{(\ell)} \in \mathbb{R}^{n_\ell}\)</span> es el vector de sesgos de la capa <span class="math inline">\(\ell\)</span>.</li>
<li><span class="math inline">\(n_\ell\)</span> es el número de neuronas (ancho) de la capa <span class="math inline">\(\ell\)</span>.</li>
<li><span class="math inline">\(\phi\)</span> es la función de activación (e.g., ReLU: <span class="math inline">\(\phi(u) = \max(0, u)\)</span>), aplicada elemento a elemento al vector <span class="math inline">\(\mathbf{z}^{(\ell)}\)</span>.</li>
</ul></li>
<li><p><strong>Capa de Salida (<span class="math inline">\(\ell = L\)</span>):</strong> Calcula la salida final <span class="math inline">\(\mathbf{y} = \mathbf{a}^{(L)} \in \mathbb{R}^{n_L}\)</span> de manera similar <span class="math display">\[
  \mathbf{z}^{(L)} = W^{(L)} \mathbf{a}^{(L-1)} + \mathbf{b}^{(L)}
  \]</span> <span class="math display">\[
  \mathbf{y} = \mathbf{a}^{(L)} = \phi_{\text{salida}}(\mathbf{z}^{(L)})
  \]</span> donde la función de activación de la capa de salida <span class="math inline">\(\phi_{\text{salida}}\)</span> puede ser diferente a la de las capas ocultas (e.g., lineal para regresión, softmax para clasificación multiclase)</p></li>
</ul>
<p>La <strong>profundidad</strong> <span class="math inline">\(L\)</span> (número de capas computacionales) y los <strong>anchos</strong> <span class="math inline">\(n_1, \dots, n_L\)</span> definen la arquitectura de la red. Se cree que la profundidad permite a la red aprender representaciones jerárquicas de los datos, donde las capas iniciales detectan características simples y las capas posteriores las combinan en conceptos más complejos.</p>
<div id="cell-fig-mlp-two-hidden" class="cell" data-execution_count="4">
<div class="cell-output cell-output-display">
<div id="fig-mlp-two-hidden" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mlp-two-hidden-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="11_perceptron_y_redes_conexas_files/figure-html/fig-mlp-two-hidden-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mlp-two-hidden-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;3: Arquitectura de una MLP con dos capas ocultas
</figcaption>
</figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Ejemplo: Pase Hacia Adelante Manual (Forward Pass)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consideremos una MLP pequeña con <span class="math inline">\(L=3\)</span>:</p>
<ul>
<li>Entrada: <span class="math inline">\(\mathbf{x} = \begin{pmatrix} 1 \\ 2 \end{pmatrix}\)</span></li>
<li>Capa Oculta 1: <span class="math inline">\(n_1=3\)</span> neuronas, activación ReLU.</li>
<li>Capa Oculta 2: <span class="math inline">\(n_2=2\)</span> neuronas, activación ReLU.</li>
<li>Capa de Salida: <span class="math inline">\(n_3=1\)</span> neurona (salida escalar), activación Identidad.</li>
</ul>
<p>Supongamos los siguientes valores para los parámetros (matrices por filas):</p>
<ul>
<li><span class="math inline">\(W^{(1)} = \begin{pmatrix} 1 &amp; -1 \\ 0 &amp; 2 \\ 1 &amp; 1 \end{pmatrix}\)</span>, <span class="math inline">\(\mathbf{b}^{(1)} = \begin{pmatrix} 0 \\ 1 \\ -1 \end{pmatrix}\)</span></li>
<li><span class="math inline">\(W^{(2)} = \begin{pmatrix} 2 &amp; -1 &amp; 1 \\ -1 &amp; 0 &amp; 2 \end{pmatrix}\)</span>, <span class="math inline">\(\mathbf{b}^{(2)} = \begin{pmatrix} 0 \\ 0.5 \end{pmatrix}\)</span></li>
<li><span class="math inline">\(W^{(3)} = \begin{pmatrix} 1 &amp; -2 \end{pmatrix}\)</span>, <span class="math inline">\(\mathbf{b}^{(3)} = 0.5\)</span></li>
</ul>
<ol type="1">
<li><p><strong>Capa 1:</strong> <span class="math display">\[
\mathbf{z}^{(1)} = W^{(1)} \mathbf{x} + \mathbf{b}^{(1)} =
\begin{pmatrix}
   1 &amp; -1 \\
   0 &amp; 2 \\
   1 &amp; 1
\end{pmatrix}
\begin{pmatrix}
   1 \\ 2
\end{pmatrix}
+
\begin{pmatrix} 0 \\ 1 \\ -1 \end{pmatrix}
\]</span></p>
<p><span class="math display">\[
=
\begin{pmatrix}
   -1 \\
   4 \\
   3
\end{pmatrix}
+
\begin{pmatrix} 0 \\ 1 \\ -1 \end{pmatrix}
=
\begin{pmatrix}
   -1 \\
   5 \\
   2
\end{pmatrix}
\]</span></p>
<p>Aplicando ReLU: <span class="math display">\[
\mathbf{a}^{(1)} = \text{ReLU}(\mathbf{z}^{(1)}) =
\begin{pmatrix}
   0 \\
   5 \\
   2
\end{pmatrix}
\]</span></p></li>
<li><p><strong>Capa 2:</strong> <span class="math display">\[
\mathbf{z}^{(2)} = W^{(2)} \mathbf{a}^{(1)} + \mathbf{b}^{(2)} =
\begin{pmatrix}
   2 &amp; -1 &amp; 1 \\
   -1 &amp; 0 &amp; 2
\end{pmatrix}
\begin{pmatrix}
   0 \\ 5 \\ 2
\end{pmatrix}
+
\begin{pmatrix}
   0 \\ 0.5
\end{pmatrix}
\]</span> <span class="math display">\[
= \begin{pmatrix} -3 \\ 4 \end{pmatrix} + \begin{pmatrix} 0 \\ 0.5 \end{pmatrix} = \begin{pmatrix} -3 \\ 4.5 \end{pmatrix}
\]</span></p>
<p>Aplicando ReLU: <span class="math display">\[
\mathbf{a}^{(2)} = \text{ReLU}(\mathbf{z}^{(2)}) = \begin{pmatrix} 0 \\ 4.5 \end{pmatrix}
\]</span></p></li>
<li><p><strong>Capa 3 (Salida):</strong> <span class="math display">\[
\mathbf{z}^{(3)} = W^{(3)} \mathbf{a}^{(2)} + \mathbf{b}^{(3)} = \begin{pmatrix} 1 &amp; -2 \end{pmatrix} \begin{pmatrix} 0 \\ 4.5 \end{pmatrix} + 0.5
\]</span> <span class="math display">\[
= -9 + 0.5 = -8.5
\]</span></p>
<p>Como la activación de salida es identidad: <span class="math display">\[
y = \mathbf{a}^{(3)} = \mathbf{z}^{(3)} = -8.5
\]</span></p></li>
</ol>
</div>
</div>
</section>
<section id="mlps-con-relu-como-funciones-afines-por-tramos-piecewise-affine" class="level2">
<h2 class="anchored" data-anchor-id="mlps-con-relu-como-funciones-afines-por-tramos-piecewise-affine">MLPs con ReLU como Funciones Afines por Tramos (Piecewise Affine)</h2>
<p>Cuando se utiliza la activación ReLU, cada neurona <span class="math inline">\(i\)</span> en la capa <span class="math inline">\(\ell\)</span> calcula <span class="math inline">\(ReLU = \max(0, \mathbf{w}_i^{(\ell)} \cdot \mathbf{a}^{(\ell-1)} + b_i^{(\ell)})\)</span>. La condición <span class="math inline">\(\mathbf{w}_i^{(\ell)} \cdot \mathbf{a}^{(\ell-1)} + b_i^{(\ell)} = 0\)</span> define un hiperplano en el espacio de las activaciones de la capa anterior <span class="math inline">\(\mathbf{a}^{(\ell-1)}\)</span>.</p>
<p>La composición de múltiples capas de estas operaciones (transformación afín seguida de ReLU) resulta en que la función global <span class="math inline">\(f_{\text{MLP}}: \mathbb{R}^{n_0} \to \mathbb{R}^{n_L}\)</span> implementada por la red es una <strong>función continua y afín por tramos</strong> (piecewise affine). Esto significa que el espacio de entrada <span class="math inline">\(\mathbb{R}^{n_0}\)</span> se divide en un gran número de regiones (poliédricas convexas), y dentro de cada región, la función <span class="math inline">\(f_{\text{MLP}}(\mathbf{x})\)</span> se comporta como una función afín <span class="math inline">\(\mathbf{x} \mapsto A \mathbf{x} + c\)</span> (donde la matriz <span class="math inline">\(A\)</span> y el vector <span class="math inline">\(c\)</span> son constantes dentro de esa región específica).</p>
<p>La configuración de qué neuronas están activas (<span class="math inline">\(\mathbf{z}_i^{(\ell)} &gt; 0\)</span>) o inactivas (<span class="math inline">\(\mathbf{z}_i^{(\ell)} \le 0\)</span>) para una entrada <span class="math inline">\(\mathbf{x}\)</span> dada define el <strong>patrón de activación</strong>. Cada patrón de activación distinto corresponde (potencialmente) a una región diferente en el espacio de entrada donde la función es localmente afín.</p>
</section>
<section id="capacidad-de-representación-parámetros-vs.-regiones-lineales" class="level2">
<h2 class="anchored" data-anchor-id="capacidad-de-representación-parámetros-vs.-regiones-lineales">Capacidad de Representación: Parámetros vs.&nbsp;Regiones Lineales</h2>
<p>Las redes multicapa tienen mucha más capacidad de representación que las redes con una sola capa oculta. Esto se debe a que pueden alcanzar una mayor complejidad con un número menor de parámetros. Además, las redes multicapa pueden aprender representaciones jerárquicas de los datos: Las capas iniciales detectan características simples y las capas posteriores las combinan en conceptos más complejos.</p>
<ul>
<li><p><strong>Número de Parámetros:</strong> El número total de parámetros (pesos y sesgos) en una MLP con <span class="math inline">\(L\)</span> capas es: <span class="math display">\[
  \#\text{Parámetros} = \sum_{\ell=1}^L (n_\ell \times n_{\ell-1} + n_\ell)
  \]</span> (Recordando que <span class="math inline">\(n_0\)</span> es la dimensión de entrada).</p></li>
<li><p><strong>Número de Regiones Lineales:</strong> Para MLPs con activación ReLU, el número máximo de regiones lineales <span class="math inline">\(R\)</span> en las que la función divide el espacio de entrada puede crecer muy rápidamente con la profundidad. <a href="https://arxiv.org/abs/1402.1869">Montúfar et al.&nbsp;(2014)</a> demostraron que, bajo ciertas condiciones, el número de regiones puede crecer <strong>exponencialmente con la profundidad <span class="math inline">\(L\)</span></strong>, aproximadamente como <span class="math inline">\(\Omega\left(\left(\frac{n}{d}\right)^{(L-1)n_0} n^{n_0}\right)\)</span> para una red con capas ocultas de ancho <span class="math inline">\(n\)</span> y entrada de dimensión <span class="math inline">\(n_0\)</span>. La fórmula exacta es <span class="math display">\[
\left(\prod_{\ell=1}^L \left\lfloor\frac{n_\ell}{n_0}\right\rfloor^{(L-1)n_0} \right) \sum_{j=0}^{n_0} \binom{n_L}{j}
\]</span></p></li>
</ul>
<div id="cell-fig-mlp-regions" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Código</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.special <span class="im">import</span> binom</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Definimos la función para calcular el número máximo de regiones lineales</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> max_linear_regions(x_values, L, n0<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    regions <span class="op">=</span> []</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> n <span class="kw">in</span> x_values:</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Para cada valor de n (ancho de capa), calculamos el producto</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>        product_term <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, L<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Asumimos que todas las capas tienen el mismo ancho n</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>            product_term <span class="op">*=</span> math.floor(n<span class="op">/</span>n0)<span class="op">**</span>(n0<span class="op">*</span>(L<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculamos la suma de los coeficientes binomiales</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>        sum_term <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(n0<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>            sum_term <span class="op">+=</span> binom(n, j)</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># El número total de regiones es el producto por la suma</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>        regions.append(product_term <span class="op">*</span> sum_term)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> regions</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> num_params(x_values, L):</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>    params <span class="op">=</span> []</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> n <span class="kw">in</span> x_values:</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>        params.append((n<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> n)<span class="op">*</span>L)</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> params</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Valores de x (ancho de capa) para evaluar</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>x_values <span class="op">=</span> np.arange(<span class="dv">2</span>, <span class="dv">101</span>, <span class="dv">1</span>)</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculamos el número de regiones para diferentes profundidades L</span></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>regions_L1 <span class="op">=</span> max_linear_regions(x_values, L<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>regions_L2 <span class="op">=</span> max_linear_regions(x_values, L<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>regions_L3 <span class="op">=</span> max_linear_regions(x_values, L<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculamos el número de parámetros para diferentes profundidades L</span></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>params_L1 <span class="op">=</span> num_params(x_values, L<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>params_L2 <span class="op">=</span> num_params(x_values, L<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>params_L3 <span class="op">=</span> num_params(x_values, L<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Creamos la gráfica</span></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>ax1.plot(x_values, regions_L1, label<span class="op">=</span><span class="st">'L=1'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>ax1.plot(x_values, regions_L2, label<span class="op">=</span><span class="st">'L=2'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>ax1.plot(x_values, regions_L3, label<span class="op">=</span><span class="st">'L=3'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'Ancho de capa (n)'</span>)</span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Número máximo de regiones lineales'</span>)</span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Número máximo de regiones lineales vs el ancho de capa'</span>)</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>ax1.legend()</span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a>ax1.grid(<span class="va">True</span>)</span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>ax1.set_yscale(<span class="st">'log'</span>)  <span class="co"># Escala logarítmica para visualizar mejor el crecimiento exponencial</span></span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>ax2.plot(params_L1, regions_L1, label<span class="op">=</span><span class="st">'L=1'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a>ax2.plot(params_L2, regions_L2, label<span class="op">=</span><span class="st">'L=2'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>ax2.plot(params_L3, regions_L3, label<span class="op">=</span><span class="st">'L=3'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'Número de parámetros'</span>)</span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Número máximo de regiones lineales'</span>)</span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'Número de parámetros vs número de regiones lineales'</span>)</span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a>ax2.legend()</span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>)</span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a>ax2.set_yscale(<span class="st">'log'</span>)  <span class="co"># Escala logarítmica para visualizar mejor el crecimiento exponencial</span></span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-mlp-regions" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mlp-regions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="11_perceptron_y_redes_conexas_files/figure-html/fig-mlp-regions-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mlp-regions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;4: Crecimiento de la complejidad de una red con el número de parámetros
</figcaption>
</figure>
</div>
</div>
</div>
<p>En la <a href="#fig-mlp-regions" class="quarto-xref">Figura&nbsp;4</a> se muestra el crecimiento exponencial del número de regiones lineales con la profundidad de la red. Vemos que el crecimiento es exponencial con la profundidad de la red. Vemos también que a paridad de número de parámetros, una red profunda puede representar funciones con un número muy grande de regiones lineales (alta complejidad).</p>
<p>Este crecimiento exponencial sugiere que las <strong>redes profundas (mayor <span class="math inline">\(L\)</span>) pueden ser exponencialmente más eficientes en términos de representación que las redes superficiales (mayor <span class="math inline">\(n_\ell\)</span> pero <span class="math inline">\(L=1\)</span> o <span class="math inline">\(L=2\)</span>)</strong>. Es decir, una red profunda podría representar funciones con un número muy grande de regiones lineales (alta complejidad) usando comparativamente menos parámetros que una red superficial que intente lograr la misma complejidad solo aumentando su ancho. Esta es una de las principales motivaciones teóricas para usar arquitecturas profundas.</p>
</section>
</section>
<section id="funciones-de-pérdida" class="level1">
<h1>Funciones de Pérdida</h1>
<p>Vimos qué es una red neuronal y cómo, dado un conjunto de pesos y sesgos, la red puede transformar una entrada en una salida. Esta salida debe ser útil para algo. Por ejemplo, si la red es un clasificador, la salida debe ser la probabilidad de que la entrada pertenezca a una clase. Si la red es un regresor, la salida debe ser el valor verdadero de la variable que se está prediciendo. Esto lo logramos ajustando los parámetros de la red para que la salida sea lo más cercana posible a la salida verdadera. Al hablar de una salida “lo más cercana posible”, necesitamos una manera de cuantificar la discrepancia. Cuando estudiamos la regresión lineal minimizamos una función de pérdida que dedujimos de la máxima verosimilitud. Ahora veremos cómo lograr eso en general para redes neuronales. Muchas funciones de pérdida estándar pueden derivarse elegantemente del principio de Máxima Verosimilitud (Maximum Likelihood Estimation, MLE).</p>
<section id="derivación-general-desde-máxima-verosimilitud" class="level2">
<h2 class="anchored" data-anchor-id="derivación-general-desde-máxima-verosimilitud">Derivación General desde Máxima Verosimilitud</h2>
<p>La idea central de la estimación de máxima verosimilitud es encontrar los parámetros del modelo <span class="math inline">\(\boldsymbol{W}, \boldsymbol{b}\)</span> que hacen que los datos observados sean lo más probables posible. Asumimos que tenemos un conjunto de datos de <span class="math inline">\(N\)</span> muestras <span class="math inline">\((\mathbf{x}_i, y_i)\)</span>, que son independientes e identicamente distribuidas (i.i.d.), y una distribución de probabilidad <span class="math inline">\(p(y | \mathbf{x}; \boldsymbol{W}, \boldsymbol{b})\)</span> que define la probabilidad de observar la salida <span class="math inline">\(y\)</span> dada la entrada <span class="math inline">\(\mathbf{x}\)</span> y los parámetros <span class="math inline">\(\boldsymbol{W}, \boldsymbol{b}\)</span>.</p>
<p>Debido a la independencia, la probabilidad conjunta de observar todas las <span class="math inline">\(y_i\)</span> dadas las <span class="math inline">\(\mathbf{x}_i\)</span> es el producto de las probabilidades individuales: <span class="math display">\[
\mathcal{L}(\boldsymbol{W}, \boldsymbol{b}) \equiv p(\mathbf{y} | \mathbf{X}; \boldsymbol{W}, \boldsymbol{b}) = \prod_{i=1}^N p(y_i | \mathbf{x}_i; )\boldsymbol{W}, \boldsymbol{b}
\]</span> Queremos encontrar los <span class="math inline">\(\boldsymbol{W}, \boldsymbol{b}\)</span> que maximizan <span class="math inline">\(\mathcal{L}(\boldsymbol{W}, \boldsymbol{b})\)</span>.</p>
<p>Maximizar <span class="math inline">\(\mathcal{L}(\boldsymbol{W}, \boldsymbol{b})\)</span> es equivalente a maximizar su logaritmo (ya que <span class="math inline">\(\log\)</span> es una función monotóna creciente). Esto simplifica los cálculos: <span class="math display">\[
    \ell(\boldsymbol{W}, \boldsymbol{b}) = \log \mathcal{L}(\boldsymbol{W}, \boldsymbol{b}) = \log \prod_{i=1}^N p(y_i | \mathbf{x}_i; \boldsymbol{W}, \boldsymbol{b}) = \sum_{i=1}^N \log p(y_i | \mathbf{x}_i; \boldsymbol{W}, \boldsymbol{b})
    \]</span></p>
<p>Los algoritmos de optimización típicamente <em>minimizan</em> una función. Por lo tanto, definimos la función de pérdida <span class="math inline">\(\mathcal{J}(\boldsymbol{W}, \boldsymbol{b})\)</span> como la <strong>Log-Verosimilitud Negativa (NLL)</strong>: <span class="math display">\[
    \mathcal{J}(\boldsymbol{W}, \boldsymbol{b}) = -\ell(\boldsymbol{W}, \boldsymbol{b}) = -\sum_{i=1}^N \log p(y_i | \mathbf{x}_i; \boldsymbol{W}, \boldsymbol{b})
    \]</span> Minimizar <span class="math inline">\(\mathcal{J}(\boldsymbol{W}, \boldsymbol{b})\)</span> es equivalente a maximizar la verosimilitud <span class="math inline">\(\mathcal{L}(\boldsymbol{W}, \boldsymbol{b})\)</span>. A menudo, se utiliza la pérdida <em>promedio</em> por muestra: <span class="math inline">\(-\frac{1}{N}\sum_{i=1}^N \log p(y_i | \mathbf{x}_i; \boldsymbol{W}, \boldsymbol{b})\)</span>.</p>
<p>Trabajar con <span class="math inline">\(\ell(\boldsymbol{W}, \boldsymbol{b})\)</span> o <span class="math inline">\(\mathcal{J}(\boldsymbol{W}, \boldsymbol{b})\)</span> en lugar de <span class="math inline">\(\mathcal{L}(\boldsymbol{W}, \boldsymbol{b})\)</span> directamente tiene sus ventajas:</p>
<ul>
<li><strong>Estabilidad Numérica:</strong> El producto de muchas probabilidades (que suelen ser &lt; 1) puede llevar a underflow numérico. La suma de logaritmos es mucho más estable.</li>
<li><strong>Simplicidad Matemática:</strong> Las derivadas del logaritmo suelen ser más manejables, facilitando la optimización basada en gradientes. Por ejemplo, el logaritmo convierte exponentes (como en la Gaussiana) en factores y multiplicaciones en sumas.</li>
</ul>
<p>Ahora aplicaremos esta lógica para derivar algunas funciones de pérdida comunes.</p>
</section>
<section id="entropía-cruzada-para-clasificación-cross-entropy-loss" class="level2">
<h2 class="anchored" data-anchor-id="entropía-cruzada-para-clasificación-cross-entropy-loss">Entropía Cruzada para Clasificación (Cross-Entropy Loss)</h2>
<p>La entropía cruzada surge naturalmente de la estimación de máxima verosimilitud para problemas de clasificación.</p>
<section id="clasificación-binaria" class="level3">
<h3 class="anchored" data-anchor-id="clasificación-binaria">Clasificación Binaria:</h3>
<ul>
<li><p><strong>Modelo Probabilístico:</strong> Asumimos que la etiqueta <span class="math inline">\(y_i \in \{0, 1\}\)</span> sigue una distribución de Bernoulli, donde la probabilidad de éxito (<span class="math inline">\(y_i=1\)</span>) es la salida del modelo <span class="math inline">\(\hat{y}_i\)</span>, típicamente obtenida aplicando una sigmoide a la salida lineal de la red: <span class="math inline">\(\hat{y}_i = \sigma(z_i) = p(y_i=1 | \mathbf{x}_i; \boldsymbol{W}, \boldsymbol{b})\)</span>.</p></li>
<li><p><span class="math inline">\(p(y_i | \mathbf{x}_i; \boldsymbol{W}, \boldsymbol{b}) = (\hat{y}_i)^{y_i} (1 - \hat{y}_i)^{1-y_i}\)</span></p></li>
<li><p><strong>Pérdida (NLL por muestra):</strong> <span class="math display">\[
  \ell_i(\boldsymbol{W}, \boldsymbol{b}) = -\log p(y_i | \mathbf{x}_i; \boldsymbol{W}, \boldsymbol{b}) = -[y_i \log \hat{y}_i + (1-y_i) \log(1 - \hat{y}_i)]
  \]</span> Esta es la <strong>pérdida de entropía cruzada binaria</strong> para la muestra <span class="math inline">\(i\)</span>. La pérdida total es <span class="math inline">\(\mathcal{J}(\boldsymbol{W}, \boldsymbol{b}) = \sum_i \ell_i(\boldsymbol{W}, \boldsymbol{b})\)</span> (o el promedio).</p></li>
<li><p><strong>Clasificación Multiclase:</strong></p>
<ul>
<li><strong>Modelo Probabilístico:</strong> Asumimos que la etiqueta <span class="math inline">\(y_i\)</span> es un vector <em>one-hot</em> (e.g., <span class="math inline">\([0, 1, 0]\)</span> si la clase verdadera es la 2 de entre 3 clases) y sigue una distribución Categórica (o Multinoulli). El modelo produce un vector de probabilidades <span class="math inline">\(\hat{\mathbf{y}}_i = (\hat{y}_{i,1}, ..., \hat{y}_{i,C})\)</span> (típicamente usando Softmax) donde <span class="math inline">\(\hat{y}_{i,c} = p(y_{i,c}=1 | \mathbf{x}_i; \boldsymbol{W}, \boldsymbol{b})\)</span>. Donde <span class="math inline">\(C\)</span> es el número de clases.</li>
<li><span class="math inline">\(p(y_i | \mathbf{x}_i; \boldsymbol{W}, \boldsymbol{b}) = \prod_{c=1}^C (\hat{y}_{i,c})^{y_{i,c}}\)</span> (donde <span class="math inline">\(y_{i,c}\)</span> es 1 para la clase verdadera, 0 para las demás).</li>
<li><strong>Pérdida (NLL por muestra):</strong> <span class="math display">\[
  \ell_i(\boldsymbol{W}, \boldsymbol{b}) = -\log p(y_i | \mathbf{x}_i; \boldsymbol{W}, \boldsymbol{b}) = -\sum_{c=1}^C y_{i,c} \log \hat{y}_{i,c}
  \]</span> Esta es la <strong>pérdida de entropía cruzada categórica</strong> para la muestra <span class="math inline">\(i\)</span>. La pérdida total es <span class="math inline">\(\mathcal{J}(\boldsymbol{W}, \boldsymbol{b}) = \sum_i \ell_i(\boldsymbol{W}, \boldsymbol{b})\)</span> (o el promedio).</li>
</ul></li>
</ul>
</section>
</section>
<section id="error-cuadrático-medio-mse-para-regresión" class="level2">
<h2 class="anchored" data-anchor-id="error-cuadrático-medio-mse-para-regresión">Error Cuadrático Medio (MSE) para Regresión</h2>
<p>El MSE es la pérdida estándar para regresión y también deriva de la estimación de máxima verosimilitud bajo la suposición de un ruido Gaussiano.</p>
<ul>
<li><p><strong>Modelo Probabilístico:</strong> Asumimos que la etiqueta verdadera <span class="math inline">\(y_i\)</span> es igual a la predicción determinista del modelo <span class="math inline">\(\hat{y}_i = f(\mathbf{x}_i; \boldsymbol{W}, \boldsymbol{b})\)</span> más un ruido Gaussiano <span class="math inline">\(\varepsilon_i\)</span> con media cero y varianza constante <span class="math inline">\(\sigma^2\)</span>:</p>
<ul>
<li><span class="math inline">\(y_i = \hat{y}_i + \varepsilon_i\)</span>, donde <span class="math inline">\(\varepsilon_i \sim \mathcal{N}(0, \sigma^2)\)</span>.</li>
<li>Esto implica que <span class="math inline">\(y_i\)</span> sigue una distribución Gaussiana centrada en la predicción del modelo: <span class="math inline">\(p(y_i | \mathbf{x}_i; \boldsymbol{W}, \boldsymbol{b}) \sim \mathcal{N}(y_i | \hat{y}_i, \sigma^2)\)</span>.</li>
<li><span class="math inline">\(p(y_i | \mathbf{x}_i; \boldsymbol{W}, \boldsymbol{b}) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - \hat{y}_i)^2}{2\sigma^2}\right)\)</span></li>
</ul></li>
<li><p><strong>Pérdida (NLL por muestra):</strong> <span class="math display">\[
  \ell_i(\boldsymbol{W}, \boldsymbol{b}) = -\log p(y_i | \mathbf{x}_i; \boldsymbol{W}, \boldsymbol{b}) = \frac{(y_i - \hat{y}_i)^2}{2\sigma^2} + \frac{1}{2}\log(2\pi\sigma^2)
  \]</span></p></li>
<li><p><strong>Simplificación a MSE:</strong> Para encontrar el <span class="math inline">\(\boldsymbol{W}, \boldsymbol{b}\)</span> óptimo, podemos ignorar los términos que no dependen de <span class="math inline">\(\boldsymbol{W}, \boldsymbol{b}\)</span>. Si asumimos <span class="math inline">\(\sigma^2\)</span> fija (o la estimamos por separado), minimizar la NLL es equivalente a minimizar: <span class="math display">\[
\sum_{i=1}^N (y_i - \hat{y}_i)^2 \quad \text{o} \quad \frac{1}{N}\sum_{i=1}^N (y_i - \hat{y}_i)^2
\]</span> Este último es el <strong>Error Cuadrático Medio (Mean Squared Error, MSE)</strong>.</p></li>
</ul>
</section>
<section id="receta-general-para-construir-funciones-de-pérdida-usando-la-estimación-de-máxima-verosimilitud" class="level2">
<h2 class="anchored" data-anchor-id="receta-general-para-construir-funciones-de-pérdida-usando-la-estimación-de-máxima-verosimilitud">Receta General para Construir Funciones de Pérdida usando la estimación de máxima verosimilitud</h2>
<p>Puedes diseñar funciones de pérdida adaptadas a problemas específicos siguiendo estos pasos:</p>
<ol type="1">
<li><strong>Elegir un Modelo Probabilístico:</strong> Define <span class="math inline">\(p(y | \mathbf{x}; \boldsymbol{W}, \boldsymbol{b})\)</span> que capture las características de tus datos y el proceso de generación (e.g., tipo de ruido, distribución de la salida).</li>
<li><strong>Escribir la Verosimilitud:</strong> Asumiendo datos i.i.d., la verosimilitud es <span class="math inline">\(\mathcal{L}(\boldsymbol{W}, \boldsymbol{b}) = \prod_i p(y_i | \mathbf{x}_i; \boldsymbol{W}, \boldsymbol{b})\)</span>.</li>
<li><strong>Calcular la Negativa Log-Verosimilitud (NLL):</strong> <span class="math inline">\(\mathcal{J}(\boldsymbol{W}, \boldsymbol{b}) = -\sum_i \log p(y_i | \mathbf{x}_i; \boldsymbol{W}, \boldsymbol{b})\)</span>. Esta es tu función de pérdida base.</li>
<li><strong>Simplificar (Opcional):</strong> Elimina términos aditivos o factores multiplicativos constantes que no dependan de <span class="math inline">\(\boldsymbol{W}, \boldsymbol{b}\)</span>, ya que no afectan la ubicación del mínimo.</li>
<li><strong>Añadir Regularización (Opcional):</strong> Para prevenir el sobreajuste o incorporar conocimiento previo, puedes añadir un término de penalización sobre los parámetros <span class="math inline">\(\boldsymbol{W}, \boldsymbol{b}\)</span>: <span class="math display">\[
\mathcal{J}_{\text{reg}}(\boldsymbol{W}, \boldsymbol{b}) = \mathcal{J}_{\text{NLL}}(\boldsymbol{W}, \boldsymbol{b}) + \lambda R(\boldsymbol{W}, \boldsymbol{b})
\]</span> Donde <span class="math inline">\(R(\boldsymbol{W}, \boldsymbol{b})\)</span> es el regularizador (e.g., <span class="math inline">\(R(\boldsymbol{W}, \boldsymbol{b}) = \|\boldsymbol{W}\|^2 + \|\boldsymbol{b}\|^2\)</span> para regularización L2/Ridge, o <span class="math inline">\(R(\boldsymbol{W}, \boldsymbol{b}) = \|\dots\|_1\)</span> para L1/LASSO) y <span class="math inline">\(\lambda &gt; 0\)</span> es el hiperparámetro de regularización.</li>
</ol>
</section>
</section>
<section id="ejercicios-sugeridos" class="level1">
<h1>Ejercicios sugeridos</h1>
<p>3.10, 3.16, 4.6, 4.11, 5.3</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiado");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiado");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>