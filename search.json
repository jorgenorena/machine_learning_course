[
  {
    "objectID": "17_flujos_normalizantes.html",
    "href": "17_flujos_normalizantes.html",
    "title": "Flujos Normalizantes",
    "section": "",
    "text": "Esta clase se basa en el capítulo 16 del libro “Understanding Deep Learning”.\nLos flujos normalizantes son una técnica para transformar una distribución de probabilidad sencilla \\(\\rho(z)\\) en otra más compleja \\(\\rho(x)\\) a través de un cambio de variables \\(x(z)\\). Esto puede ser muy útil en varios casos: Por ejemplo si queremos deducir la distribución de probabilidad de las observaciones podemos buscar el flujo normalizante que maximiza la verosimilitud de los datos. Otra aplicación es si tenemos una distribución de probabilidad \\(\\rho(x)\\) de la cual es difícil tomar muestras aleatorias, pero que es fácil de evaluar, podemos buscar el flujo normalizante que transforma \\(\\rho(x)\\) en una distribución de probabilidad sencilla \\(\\rho(z)\\).",
    "crumbs": [
      "Home",
      "Algunas aplicaciones",
      "17. Flujos Normalizantes"
    ]
  },
  {
    "objectID": "17_flujos_normalizantes.html#aprendizaje",
    "href": "17_flujos_normalizantes.html#aprendizaje",
    "title": "Flujos Normalizantes",
    "section": "Aprendizaje",
    "text": "Aprendizaje\nNuestra transformación de variables depende de parámetros \\(\\Phi\\) que debemos aprender.\nPara hacerlo, consideremos primero el caso en el cual queremos aprender la distribución de probabilidad de los datos. Lo que queremos hacer es encontrar la distribución de probabilidad que maximiza la verosimilitud de los datos.\nRecordemos que la verosimilitud es lanprobabilidad de los datos dados los parámetros \\[\n\\Phi = \\underset{\\Phi}{\\operatorname{argmax}} \\left[\\prod_{i=1}^N \\rho(x_i|\\Phi)\\right] = -\\underset{\\Phi}{\\operatorname{argmin}} \\left[\\sum_{i=1}^N \\log \\rho(x_i|\\Phi)\\right] = \\underset{\\Phi}{\\operatorname{argmin}} \\left[\\sum_{i=1}^N \\left(\\log \\left|\\frac{\\partial f_\\Phi(z)}{\\partial z}\\right| - \\log\\rho(z_i)\\right)\\right]\n\\]\nEntonces para entrenar necesitamos minimizar la pérdida dentro de los paréntesis cuadrados de la última igualdad. Vemos que esta pérdida depende de \\(z\\), pero para cada dato conocemos sólo \\(x\\). Para obtener \\(z\\) a partir de \\(x\\) podemos usar la función inversa \\(z = f^{-1}_\\Phi(x)\\). Pero no todas las funciones son invertibles. En particular, no todas las redes neuronales se pueden invertir y debemos buscar arquitecturas para las cuales sí se pueda.",
    "crumbs": [
      "Home",
      "Algunas aplicaciones",
      "17. Flujos Normalizantes"
    ]
  },
  {
    "objectID": "17_flujos_normalizantes.html#requisitos-de-una-capa-para-un-flujo-normalizante",
    "href": "17_flujos_normalizantes.html#requisitos-de-una-capa-para-un-flujo-normalizante",
    "title": "Flujos Normalizantes",
    "section": "Requisitos de una capa para un flujo normalizante",
    "text": "Requisitos de una capa para un flujo normalizante\nPara poder usar una función \\(f_{\\Phi}\\) para nuestros propósitos necesitamos que cumpla lo siguiente\n\nExpresividad: Debe tener la suficiente libertad para poder obtener cualquier distribución de probabilidad.\nInvertible: Labpérdida depende de \\(\\boldsymbol{z}_i\\) que obtenemos invirtiendo la transformación.\nDebe ser fácil (es decir numéricamente eficiente) calcular esa inversa. Idem para el determinante.",
    "crumbs": [
      "Home",
      "Algunas aplicaciones",
      "17. Flujos Normalizantes"
    ]
  },
  {
    "objectID": "17_flujos_normalizantes.html#flujos-lineales",
    "href": "17_flujos_normalizantes.html#flujos-lineales",
    "title": "Flujos Normalizantes",
    "section": "Flujos lineales",
    "text": "Flujos lineales\nLa transformación invertible más sencilla es una transformación lineal dada por \\(\\boldsymbol{f}(\\boldsymbol{h}) = \\Omega\\boldsymbol{h} + \\boldsymbol{\\beta}\\) con \\(\\Omega\\) una matriz \\(d\\times d\\) invertible.\nEn general, la inversa y el determinante de la matriz \\(\\Omega\\) requiere \\(\\mathcal{O}(d^3)\\) operaciones lo que no es particularmente eficiente. Sin embargo, existe la llamada factorización PLU que lo reduce a \\(\\mathcal{O}(d^2)\\).\nEl problema de las transformaciones lineales es que no son suficientemenete expresivas. Por ejemplo la transformación lineal de una distribución gaussiana es de nuevo una gaussiana, aunque podemos convertir una gaussiana sin correlaciones en una con. Además la composición de varias transformaciones lineales es de nuevo una transformación lineal.",
    "crumbs": [
      "Home",
      "Algunas aplicaciones",
      "17. Flujos Normalizantes"
    ]
  },
  {
    "objectID": "17_flujos_normalizantes.html#transformaciones-elemento-por-elemento",
    "href": "17_flujos_normalizantes.html#transformaciones-elemento-por-elemento",
    "title": "Flujos Normalizantes",
    "section": "Transformaciones elemento por elemento",
    "text": "Transformaciones elemento por elemento\nOtro tipo sencillo de transformación es una que aplica la misma función a cada elemento de un vector, es decir\n\\[\n\\boldsymbol{f}(h_1, ..., h_d) = (f(h_1), ..., f(h_d))\n\\]\nEsta transformación es fácil de inviertir si \\(f\\) es invertible \\(\\boldsymbol{f}^{-1}(x_1, ..., x_d) = f^{-1}(x_1) ... f^{-1}(x_d)\\), y el determinante también es fácil de calcular\n\\[\n\\left|\\frac{\\partial\\boldsymbol{f}}{\\partial\\boldsymbol{h}}\\right| = \\prod_{i=1}^d \\left|\\frac{d f(h_i)}{d h_i}\\right|\n\\]\nEl problema de esta transformación es que no genera correlaciones entre elementos diferentes del vector. En este sentido tampoco es suficientemente expresiva.\nUno puede crear un flujo normalizante alternando capas lineales con capas elemento por elemento. Las lineales generan correlaciones y las elemento por elemento generan no-linealidades.",
    "crumbs": [
      "Home",
      "Algunas aplicaciones",
      "17. Flujos Normalizantes"
    ]
  },
  {
    "objectID": "17_flujos_normalizantes.html#flujos-autoregresivos",
    "href": "17_flujos_normalizantes.html#flujos-autoregresivos",
    "title": "Flujos Normalizantes",
    "section": "Flujos autoregresivos",
    "text": "Flujos autoregresivos\nLos flujos autoregresivos consisten en \\[\nf_i(\\boldsymbol{h}) = g\\left[h_i, \\Phi(h_1, ..., h_{i-1})\\right]\\,.\n\\]\nDonde \\(g\\) es una función que toma como entrada el valor de la variable \\(h_i\\) y los parámetros \\(\\Phi\\) que dependen de las variables anteriores \\(h_1, ..., h_{i-1}\\).\nTiene esta forma porque es fácil de invertir. Veámoslo en un ejemplo con cuatro componentes \\[\nf_1 = g\\left[h_1, \\Phi\\right]\\,,\n\\] \\[\nf_2 = g\\left[h_2, \\Phi(h_1)\\right]\\,,\n\\] \\[\nf_3 = g\\left[h_3, \\Phi(h_1, h_2)\\right]\\,,\n\\] \\[\nf_4 = g\\left[h_4, \\Phi(h_1, h_2, h_3)\\right]\\,.\n\\]\nLas inversas son fáciles si \\(g\\) es invertible \\[\nh_1 = g^{-1}\\left[f_1, \\Phi\\right]\\,,\n\\] Teniendo \\(h_1\\) podemos obtener \\(h_2\\) \\[\nh_2 = g^{-1}\\left[f_2, \\Phi(h_1)\\right]\\,,\n\\] y así sucesivamente \\[\nh_3 = g^{-1}\\left[f_3, \\Phi(h_1, h_2)\\right]\\,,\n\\] \\[\nh_4 = g^{-1}\\left[f_4, \\Phi(h_1, h_2, h_3)\\right]\\,.\n\\] Ver figura 16.7 del libro.",
    "crumbs": [
      "Home",
      "Algunas aplicaciones",
      "17. Flujos Normalizantes"
    ]
  },
  {
    "objectID": "17_flujos_normalizantes.html#flujos-residuales",
    "href": "17_flujos_normalizantes.html#flujos-residuales",
    "title": "Flujos Normalizantes",
    "section": "Flujos residuales",
    "text": "Flujos residuales\nVimos que las redes residuales ayudan a obtener arquitecturas muy profundas con un gran número de capas. Podemos lograr redes residuales con flujos normalizantes. Toman una forma un poco rara para lograr ser invertibles. Supongamos que tenemos dos dimensiones por simplicidad \\[\nf_1 = h_1 + g_1(h_2, \\Phi_1)\\,.\n\\] Teniendo \\(f_1\\) podemos obtener \\(h_2\\) \\[\nf_2 = h_2 + g_2(f_1, \\Phi_2)\\,.\n\\]\nEsta transformación es fácil de invertir. Primero calculamos \\(h_2\\) a partir de \\(f_1\\) y \\(f_2\\) \\[\nh_2 = f_2 - g_2(f_1, \\Phi_2)\\,.\n\\] Y luego teniendo \\(h_2\\) calculamos \\(h_1\\) \\[\nh_1 = f_1 - g_1(h_2, \\Phi_1)\\,.\n\\] Ver la figura 16.8 del libro.\nEsto da un sabor del tipo de trucos que hay que jugar para lograr redes invertibles. Existen más en la literatura y dado que este es un campo en rápido crecimiento, existirán muchos más en el futuro cercano.",
    "crumbs": [
      "Home",
      "Algunas aplicaciones",
      "17. Flujos Normalizantes"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Acerca de",
    "section": "",
    "text": "Versión 0.1.2.2\nEste sitio y su contenido fueron escritos por Jorge Noreña del\nInstituto de Física, Pontificia Universidad Católica de Valparaíso.\nSi tienes preguntas o comentarios, no dudes en ponerte en contacto.\nEl contenido es principalmente un resumen y traducción de los libros y recursos listados en la sección de Referencias.\nParte del contenido fue hecho con la ayuda de herramientas de IA, como Cursor, Windsurf, Ollama y ChatGPT. Las partes generadas por dichos modelos fueron editadas, revisadas y extensamente mejoradas por el autor."
  },
  {
    "objectID": "6_aprendizaje_automatico_regresion_lineal.html",
    "href": "6_aprendizaje_automatico_regresion_lineal.html",
    "title": "Introducción al Aprendizaje Automático con Regresión Lineal",
    "section": "",
    "text": "Esta disciplina existe en la intersección entre la estadística y la informática. Por lo tanto hay un par de maneras de verlo.\nUn informático definiría el aprendizaje automático por contraste con la programación clásica. En programación clásica uno le dice al computador qué hacer paso a paso. En aprendizaje automático uno en cambio define un modelo y este aprende o se ajusta a los datos para dar el resultado esperado. Un ejemplo trivial sería convertir entre grados centígrados y fahrenheit: En programación usual uno le pide al computador calcular con la fórmula de conversión. En aprendizaje automático uno tiene muchas parejas de temperaturas en ambas esalas y ajusta un modelo por ejemplo con regresión lineal. En ese sentido decimos que el modelo aprende de los datos.\nUn estadístico diría que el aprendizaje automático es una serie de nuevos modelos para trabajar con una cantidad grande de datos y/o parámetros libres. El desarrollo de estos algoritmos fue posible gracias al progreso del hardware de las últimas décadas.\nEn muchas situacines tenemos una serie de variables que medimos sobre un sistema, las llamamos \\(X_1, ..., X_n\\), y nos interesa alguna otra variable que depende de ellas \\(Y = f(X) + \\epsilon\\). El \\(\\epsilon\\) es un ruido que representa lo que está fuera de nuestro control y capacidad para medir. Puede ser error experimental en las mediciones, o el efecto de otras variables que no estamos tomando en cuenta.\nUn ejemplo de un tal problema es el siguiente: Trabajamos para una empresa que vende un producto dado. Esta empresa invierte su presupuesto de publicidad en radio, televisión y periódicos. Tenemos el número de unidades medidas en varios mercados así como la inversión en estos tres tipos de publicidad. De esta forma \\(X_1, X_2, X_3\\) es la inversión en televisión, radio y periódicos respectivamente, mientras \\(Y\\) es la cantidad de unidades medidas.\n\n\nEn este caso podemos querer determinar \\(f\\) por dos motivos: Predicción o inferencia.\nPara predicción, nos importa solamente determinar cuánto vale \\(Y\\) para alguna combinación de las variables de entrada que no hemos visto antes. Por ejemplo saber cuántas unidades se venderán para una dada inversión en publicidad. La mayor parte del aprendizaje automático se ocupa de este tipo de problemas. Lo más importante en este tipo de problemas es reducir el error \\[\n\\langle (\\hat{Y} - Y)^2\\rangle = \\langle(\\hat{f}(X) - f(X) + \\epsilon)^2 \\rangle = \\langle(f(X) - \\hat{f}(X))^2\\rangle + \\operatorname{Var}(\\epsilon)\\,.\n\\] Al primer término lo llamamos el error reducible: Podemos esperar disminuirlo encontrlando un modelo mejor. Al segundo término lo llamamos el error irreducible.\nPara la inferencia, nos interesa saber la forma de \\(f\\) para responder preguntas cualitativas sobre el fenómeno como por ejemplo: ¿Cuáles variables tienen un efecto importante? ¿Cuál es la incertidumbre de una predicción dada?\nEn física normalmente nos interesa la inferencia. Predecir no es suficiente dado que el propósito de la física es entender los sistemas que estudia. Además nos interesa tener alguna medida de la incertidumbre de nuestras predicciones y modelos. Algunos estadísticos le llaman aprendizaje estadístico al estudio de este tipo de pregutnas, en contraste con el aprendizaje automático que se preocupa sólo de obtener predicciones de la manera más eficiente posible.\n\n\n\nEn nuestra discusión anterior hemos tomado en cuenta modelos que dependen de parámetros. En física esto ocurre con mucha frecuencia ya que tenemos leyes físicas que queremos comparar con el experimento. Esas leyes nos proveen el modelo de forma natural junto con sus parámetros libres.\nEn general, los modelos que dependen de parámetros pueden dar origen a métodos muy potentes cuando son una buena descripción de los datos. Por ejemplo el ajuste lineal que vimos antes y que discutiremos más abajo puede ser un método muy potente si la relación entre los \\(X_i\\) y \\(Y\\) es lineal.\nLos modelos paraétricos suelen ser más interpretables y más fáciles de usar para hacer inferencia.\nSin embargo hay casos donde no sabemos cuál es el modelo subyacente. Aún así nos puede interesar hacer predicciones. Un ejemplo que surge en física es cuando algún detalle del detector tiene un efecto sobre la medida, pero no nos interesa o es demasiado complicado modelarlo. Entonces tratamos de aproximar el efecto con funciones muy generales que no tengan parámetros libres.\nLa ventaja de estos modelos es que para la predicción son más generales que los paramétricos. No necesitamos conocer el sistema para tratar de ajustar uno de estos modelos, y nos puede permitir predecir en situaciones complejas. Por otro lado, suelen ser menos interpretables. Son menos potentes que conocer el modelo subyacente de los datos. Mientras más sabemos sobre un sistema a priori, mejores los métodos que podemos aplicar.\n\n\n\nNormalmente los datos toman la forma de una enorme lista de mediciones de las diferentes variables de entrada. Si tenemos \\(q\\) variables de entrada y \\(n\\) mediciones, podemos organizarlas en una matriz donde cada columna corresponde a una variable medida y cada fila es una medición. Es decir \\(\\boldsymbol{X}\\) es una matriz \\(n\\times q\\). En algunos casos también tenemos las variables de salida \\(Y\\) que organizamos como un vector \\(y_i\\) con \\(n\\) componentes. Si usamos esas variables de salida para entrenar el modelo hablamos de aprendizaje supervisado. La analogía es que el modelo mira varios ejemplos para aprender.\nEn otras situaciones no tenemos una variable de salida \\(y_i\\). Por ejemplo si tenemos una aplicación que recomienda restaurantes, nos puede interesar saber cuáles son los gustos de las personas. Podemos entonces agruparlos por gusto, donde los miembros de cada grupo tienen gustos similares. En casos como este decimos que el aprendizaje es no supervisado.\nTambién hay mezclas entre ambos. Un ejemplo es reconocer imágenes de células con una cierta característica. Tenemos muchas imágenes pero alguien tiene que etiquetarlas con esa característica a mano. En ese caso podemos etiquetar algunas pocas e intentar entrenar un modelo que mezcle ambos tipos de algoritmos. Esto se llama aprendizaje semi supervisado.\n\n\n\nEn problemas de regresión las variables de salida son numeros reales. Por ejemplo la masa de una partícula o la densidad del universo.\nEn problemas de clasificación las variables de salida son categorías. Por ejemplo si un evento en un detector es una señal o un ruido instrumental.",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "6. Aprendizaje Automático y Regresión Lineal"
    ]
  },
  {
    "objectID": "6_aprendizaje_automatico_regresion_lineal.html#predicción-vs-inferencia",
    "href": "6_aprendizaje_automatico_regresion_lineal.html#predicción-vs-inferencia",
    "title": "Introducción al Aprendizaje Automático con Regresión Lineal",
    "section": "",
    "text": "En este caso podemos querer determinar \\(f\\) por dos motivos: Predicción o inferencia.\nPara predicción, nos importa solamente determinar cuánto vale \\(Y\\) para alguna combinación de las variables de entrada que no hemos visto antes. Por ejemplo saber cuántas unidades se venderán para una dada inversión en publicidad. La mayor parte del aprendizaje automático se ocupa de este tipo de problemas. Lo más importante en este tipo de problemas es reducir el error \\[\n\\langle (\\hat{Y} - Y)^2\\rangle = \\langle(\\hat{f}(X) - f(X) + \\epsilon)^2 \\rangle = \\langle(f(X) - \\hat{f}(X))^2\\rangle + \\operatorname{Var}(\\epsilon)\\,.\n\\] Al primer término lo llamamos el error reducible: Podemos esperar disminuirlo encontrlando un modelo mejor. Al segundo término lo llamamos el error irreducible.\nPara la inferencia, nos interesa saber la forma de \\(f\\) para responder preguntas cualitativas sobre el fenómeno como por ejemplo: ¿Cuáles variables tienen un efecto importante? ¿Cuál es la incertidumbre de una predicción dada?\nEn física normalmente nos interesa la inferencia. Predecir no es suficiente dado que el propósito de la física es entender los sistemas que estudia. Además nos interesa tener alguna medida de la incertidumbre de nuestras predicciones y modelos. Algunos estadísticos le llaman aprendizaje estadístico al estudio de este tipo de pregutnas, en contraste con el aprendizaje automático que se preocupa sólo de obtener predicciones de la manera más eficiente posible.",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "6. Aprendizaje Automático y Regresión Lineal"
    ]
  },
  {
    "objectID": "6_aprendizaje_automatico_regresion_lineal.html#modelos-paramétricos-vs-no-paramétricos",
    "href": "6_aprendizaje_automatico_regresion_lineal.html#modelos-paramétricos-vs-no-paramétricos",
    "title": "Introducción al Aprendizaje Automático con Regresión Lineal",
    "section": "",
    "text": "En nuestra discusión anterior hemos tomado en cuenta modelos que dependen de parámetros. En física esto ocurre con mucha frecuencia ya que tenemos leyes físicas que queremos comparar con el experimento. Esas leyes nos proveen el modelo de forma natural junto con sus parámetros libres.\nEn general, los modelos que dependen de parámetros pueden dar origen a métodos muy potentes cuando son una buena descripción de los datos. Por ejemplo el ajuste lineal que vimos antes y que discutiremos más abajo puede ser un método muy potente si la relación entre los \\(X_i\\) y \\(Y\\) es lineal.\nLos modelos paraétricos suelen ser más interpretables y más fáciles de usar para hacer inferencia.\nSin embargo hay casos donde no sabemos cuál es el modelo subyacente. Aún así nos puede interesar hacer predicciones. Un ejemplo que surge en física es cuando algún detalle del detector tiene un efecto sobre la medida, pero no nos interesa o es demasiado complicado modelarlo. Entonces tratamos de aproximar el efecto con funciones muy generales que no tengan parámetros libres.\nLa ventaja de estos modelos es que para la predicción son más generales que los paramétricos. No necesitamos conocer el sistema para tratar de ajustar uno de estos modelos, y nos puede permitir predecir en situaciones complejas. Por otro lado, suelen ser menos interpretables. Son menos potentes que conocer el modelo subyacente de los datos. Mientras más sabemos sobre un sistema a priori, mejores los métodos que podemos aplicar.",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "6. Aprendizaje Automático y Regresión Lineal"
    ]
  },
  {
    "objectID": "6_aprendizaje_automatico_regresion_lineal.html#aprendizaje-supervisado-vs-no-supervisado",
    "href": "6_aprendizaje_automatico_regresion_lineal.html#aprendizaje-supervisado-vs-no-supervisado",
    "title": "Introducción al Aprendizaje Automático con Regresión Lineal",
    "section": "",
    "text": "Normalmente los datos toman la forma de una enorme lista de mediciones de las diferentes variables de entrada. Si tenemos \\(q\\) variables de entrada y \\(n\\) mediciones, podemos organizarlas en una matriz donde cada columna corresponde a una variable medida y cada fila es una medición. Es decir \\(\\boldsymbol{X}\\) es una matriz \\(n\\times q\\). En algunos casos también tenemos las variables de salida \\(Y\\) que organizamos como un vector \\(y_i\\) con \\(n\\) componentes. Si usamos esas variables de salida para entrenar el modelo hablamos de aprendizaje supervisado. La analogía es que el modelo mira varios ejemplos para aprender.\nEn otras situaciones no tenemos una variable de salida \\(y_i\\). Por ejemplo si tenemos una aplicación que recomienda restaurantes, nos puede interesar saber cuáles son los gustos de las personas. Podemos entonces agruparlos por gusto, donde los miembros de cada grupo tienen gustos similares. En casos como este decimos que el aprendizaje es no supervisado.\nTambién hay mezclas entre ambos. Un ejemplo es reconocer imágenes de células con una cierta característica. Tenemos muchas imágenes pero alguien tiene que etiquetarlas con esa característica a mano. En ese caso podemos etiquetar algunas pocas e intentar entrenar un modelo que mezcle ambos tipos de algoritmos. Esto se llama aprendizaje semi supervisado.",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "6. Aprendizaje Automático y Regresión Lineal"
    ]
  },
  {
    "objectID": "6_aprendizaje_automatico_regresion_lineal.html#regresión-contra-clasificación",
    "href": "6_aprendizaje_automatico_regresion_lineal.html#regresión-contra-clasificación",
    "title": "Introducción al Aprendizaje Automático con Regresión Lineal",
    "section": "",
    "text": "En problemas de regresión las variables de salida son numeros reales. Por ejemplo la masa de una partícula o la densidad del universo.\nEn problemas de clasificación las variables de salida son categorías. Por ejemplo si un evento en un detector es una señal o un ruido instrumental.",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "6. Aprendizaje Automático y Regresión Lineal"
    ]
  },
  {
    "objectID": "6_aprendizaje_automatico_regresion_lineal.html#regresión-lineal-simple",
    "href": "6_aprendizaje_automatico_regresion_lineal.html#regresión-lineal-simple",
    "title": "Introducción al Aprendizaje Automático con Regresión Lineal",
    "section": "Regresión lineal simple",
    "text": "Regresión lineal simple\nPrimero reducimos el problema a predecir el número de ventas basados en el gasto en televisión. Por lo tanto nuestro modelo es \\[\nf(X) = \\beta_0 + \\beta_1 X\\,,\n\\] donde \\(\\beta_1\\) es el gasto en televisión. Ya vimos cómo estimar los coeficientes minimizando la suma de errores al cuadrado. Esto ya hace parte de muchas librerías en python.\n\nfrom sklearn.linear_model import LinearRegression\n\n# Datos \nX = datos_publicidad[[\"TV\"]]\nY = datos_publicidad[\"Sales\"]\n\n# Crear y ajustar el modelo \nmodel = LinearRegression()\nmodel.fit(X, Y)\n\n# Predecir \ny_pred = model.predict(X)\n\n# Graficar\nplt.scatter(X, Y, color='blue', label='Datos')\nplt.plot(X, y_pred, color='red', label='Regresión')\n\nplt.xlabel(\"Televisión\")\nplt.ylabel(\"Ventas\")\n\nText(0, 0.5, 'Ventas')",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "6. Aprendizaje Automático y Regresión Lineal"
    ]
  },
  {
    "objectID": "6_aprendizaje_automatico_regresion_lineal.html#determinando-la-precisión-de-los-coeficientes",
    "href": "6_aprendizaje_automatico_regresion_lineal.html#determinando-la-precisión-de-los-coeficientes",
    "title": "Introducción al Aprendizaje Automático con Regresión Lineal",
    "section": "Determinando la precisión de los coeficientes",
    "text": "Determinando la precisión de los coeficientes\nAl escribir el modelo nosotros estimamos los coeficientes. Como se calcularon a partir de los datos, tendrán una varianza. Para verlo repetimos el código de arriba pero usando un subconjunto de los datos.\n\ndatos_subconjunto_1 = datos_publicidad.sample(frac=0.2)\ndatos_subconjunto_2 = datos_publicidad.sample(frac=0.2)\n\n# Datos \nX_1 = datos_subconjunto_1[[\"TV\"]]\nY_1 = datos_subconjunto_1[\"Sales\"]\nX_2 = datos_subconjunto_2[[\"TV\"]]\nY_2 = datos_subconjunto_2[\"Sales\"]\n\n# Crear y ajustar el modelo \nmodel_1 = LinearRegression()\nmodel_1.fit(X_1, Y_1)\nmodel_2 = LinearRegression()\nmodel_2.fit(X_2, Y_2)\n\n# Predecir \ny_pred_1 = model_1.predict(X_1)\ny_pred_2 = model_2.predict(X_2)\n\n# Graficar\nplt.scatter(X_1, Y_1, color='blue', label='Datos')\nplt.plot(X_1, y_pred_1, color='red', label='Regresión 1')\nplt.scatter(X_2, Y_2, color='darkblue', label='Datos')\nplt.plot(X_2, y_pred_2, color='darkred', label='Regresión 2')\nplt.legend()\n\nplt.xlabel(\"Televisión\")\nplt.ylabel(\"Ventas\")\n\nText(0, 0.5, 'Ventas')\n\n\n\n\n\n\n\n\n\nVemos que las líneas cambian dependiendo de los datos. Esto nos muestra que estas estimaciones tienen una varianza que ya habíamos calculado \\[\n\\operatorname{Var}(\\hat{\\beta}_1) = \\frac{\\sigma^2}{\\sum(x_i - x)^2}\\,,\\quad \\operatorname{Var}(\\hat{\\beta}_0) = \\frac{\\sigma^2}{n} + \\operatorname{Var}(\\hat{\\beta}_1) \\bar{x}^2\\,.\n\\] En general no concemos \\(\\sigma^2\\), que es la varianza de \\(Y\\). Lo tenemos que estimar de los datos. Para hacerlo tomamos \\(s^2 = \\sum(y_i - \\bar{y})^2/(n-2)\\). Es \\((n-2)\\) en vez de \\((n-1)\\) en el denominador porque en este modelo tenemos dos parámetros.",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "6. Aprendizaje Automático y Regresión Lineal"
    ]
  },
  {
    "objectID": "6_aprendizaje_automatico_regresion_lineal.html#evaluar-la-precisión-de-un-modelo",
    "href": "6_aprendizaje_automatico_regresion_lineal.html#evaluar-la-precisión-de-un-modelo",
    "title": "Introducción al Aprendizaje Automático con Regresión Lineal",
    "section": "Evaluar la precisión de un modelo",
    "text": "Evaluar la precisión de un modelo\nUna manera de evaluar lo bueno o malo que es el ajuste es por medio de la estadística \\(\\chi^2\\) que vimos antes.\nOtra forma muy usada y fácil de interpretar es la suma de errores residuales. Se define como \\[\n\\text{RSE} = \\sqrt{\\frac{1}{n - 2} \\text{RSS}}\\,,\n\\] donde \\(\\text{RSS} \\equiv \\sum(y_i - \\hat{y}_i)^2\\). Esto nos dice en promedio cuánto se desvía el modelo de las mediciones. Saber si el errror es suficientemente bueno depende de lo que estemos dispuestos a tolerar.\nOtra evaluacón que muy usada que no depende del modelo es la \\(R^2\\). Definida así \\[\nR^2 = 1 - \\frac{\\text{RSS}}{\\sum (y_i - \\bar{y})^2}\\,.\n\\] El denominador del segundo término se llama la suma total de cuadrados. Nos dice la varianza de \\(y\\) en los datos. El numerador nos dice la desviación de los datos respecto al modelo. Si \\(R^2\\) es cercano a \\(1\\) quiere decir que la desviación de los daotos respecto al modelo es muy pequeña comparada con la desviación total y el modelo es bueno. Si al contrario \\(R^2\\) es cercano a cero, quiere decir que la desviación respecto al modelo es similar a la desviación total tal que el modelo no está haciendo nada (podemos lograr algo similar usando la media). A veces se dice que \\(R^2\\) es la “fracción de la varianza explicada por el modelo”. Para la regresión lineal simple, $R^2% es igual a la correlación entre el modelo y los datos.",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "6. Aprendizaje Automático y Regresión Lineal"
    ]
  },
  {
    "objectID": "6_aprendizaje_automatico_regresion_lineal.html#regresión-lineal-múltiple",
    "href": "6_aprendizaje_automatico_regresion_lineal.html#regresión-lineal-múltiple",
    "title": "Introducción al Aprendizaje Automático con Regresión Lineal",
    "section": "Regresión Lineal Múltiple",
    "text": "Regresión Lineal Múltiple\nAhora queremos ajustar un modelo que depende de varias variables de entrada. En nuestro caso son los presupuestos para los diferentes tipos de publicidad. \\[\nf(\\vec{X}) = \\beta_0 + \\beta_1 X_1 + ... + \\beta_q X_q\\,.\n\\] El ajuste de mínimos cuadrados se logra por medio de la ecuación normal como antes. Para tener más control sobre las propiedades estadísticas usamos otra librería de python.\n\nimport statsmodels.api as sm\n\nX = datos_publicidad.drop(\"Sales\", axis=1)\n\nX = sm.add_constant(X)\n\n# Ajustar el modelo \nmodel = sm.OLS(Y, X)\nresults = model.fit()\n\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Sales   R-squared:                       0.897\nModel:                            OLS   Adj. R-squared:                  0.896\nMethod:                 Least Squares   F-statistic:                     570.3\nDate:                Mon, 29 Dec 2025   Prob (F-statistic):           1.58e-96\nTime:                        12:48:04   Log-Likelihood:                -386.18\nNo. Observations:                 200   AIC:                             780.4\nDf Residuals:                     196   BIC:                             793.6\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          2.9389      0.312      9.422      0.000       2.324       3.554\nTV             0.0458      0.001     32.809      0.000       0.043       0.049\nRadio          0.1885      0.009     21.893      0.000       0.172       0.206\nNewspaper     -0.0010      0.006     -0.177      0.860      -0.013       0.011\n==============================================================================\nOmnibus:                       60.414   Durbin-Watson:                   2.084\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              151.241\nSkew:                          -1.327   Prob(JB):                     1.44e-33\nKurtosis:                       6.332   Cond. No.                         454.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nEsta librería nos da un el error estándar sobre cada parámetro. Esto nos permite calcular intervalos de confianza.\nAdemás nos presenta con una estadística \\(t\\) que viene de comparar la hipótesis nula de que ese parámetro no afecta los datos con la hipótesis que sí los afecta. En este caso corresponde a \\(t = \\hat{\\beta}_i/\\operatorname{SE}(\\hat{\\beta}_i)\\). Sabemos que esta está distribuida según la distribución de Student para grandes números de datos. De aquí podemos obtener la probabilidad de obtener un \\(t\\) así o más extremo, es decir un valor \\(p\\) que también nos lo da esta librería.\nCon esto bajo la manga podemos preguntarnos\n¿Cuáles variables son importantes?\nEstudiaremos más adelante varias maneras de seleccionar variables. Por ahora sólo diremos que si tenemos \\(p\\) coeficientes, tenemos \\(2^p\\) posibles modelos. Estos son demasiados para compararlos a todos. Por lo tanto se usan algunas estrategias aproximadas:\n\nEmpezamos por todos los modelos con un parámetro. Escogemos el mejor de esos, luego le agregamos los parámetros restantes uno a uno para quedarnos con el mejor. Seguimos así hasta que satisfacemos algún criterio. Por ejemplo que el valor \\(p\\) de todo el modelo esté por debajo de un cierto valor.\nAl revés, del modelo completo quitamos parámetros uno a uno. Nos quedamos con el mejor y le quitamos a ese los parámetros uno a uno. Paramos cuando todos los parámetros tengan un valor \\(p\\) por debajo de un cierto valor.\n\nEn nuestro ejemplo es bastante claro que podemos remover el coeficiente de periódicos.\nUna vez tengamos un buen modelo podemos hacer predicciones. A partir de las varianzas de los parámetros podemos reportar un intervalo de confianza a un nivel \\(C\\) para la prediccón (propagando errores). Según este modelo, el verdadero valor de \\(f(X)\\) estará en ese intervalo una fracción \\(C\\) de veces. *Esto no quiere decir que al realizar muchas observaciones futuras, una fracción \\(C\\) caerá en ese intervalo. Esto es porque nos falta \\(\\epsilon\\), el error irreducible (recuerde que cada observación es \\(f(X) + \\epsilon\\)). A los intervalos que incorporan este error irreducible (estimado a partir de los datos) se los llama intervalos de predicción.",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "6. Aprendizaje Automático y Regresión Lineal"
    ]
  },
  {
    "objectID": "6_aprendizaje_automatico_regresion_lineal.html#términos-no-lineales-en-regresión-lineal",
    "href": "6_aprendizaje_automatico_regresion_lineal.html#términos-no-lineales-en-regresión-lineal",
    "title": "Introducción al Aprendizaje Automático con Regresión Lineal",
    "section": "Términos No-Lineales en Regresión Lineal",
    "text": "Términos No-Lineales en Regresión Lineal\nCuando la relación entre variables es no-lineal, al parecer no podemos aplicar este método. En realidad podemos generalizarlo. Por ejemplo podríamos escribir para un solo predictor \\[\nf(X) = \\beta_0 + \\beta_1 X + \\beta_2 X^2\\,.\n\\] Más interesante aún, en el caso de varios predictores podemos incluir términos de interacción. En nuestro ejemplo podemos escribir \\[\nf(X) = \\beta_0 + \\beta_{\\text{TV}}X_{\\text{TV}} + \\beta_{\\text{radio}}X_{\\text{radio}} + \\beta_{\\text{TV}\\times\\text{radio}}X_{\\text{TV}}X_{\\text{radio}}\\,.\n\\] Lo que representa este término es que hay un efecto de sinergia entre los dos predictores. El efecto de invertir en televisión depende de si se invierte en radio o no. Veámoslo\n\nimport statsmodels.api as sm\n\nX = datos_publicidad.drop([\"Sales\", \"Newspaper\"], axis=1)\nX[\"Interaction\"] = X[\"TV\"]*X[\"Radio\"]\n\nX = sm.add_constant(X)\n\n# Ajustar el modelo \nmodel = sm.OLS(Y, X)\nresults = model.fit()\n\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Sales   R-squared:                       0.968\nModel:                            OLS   Adj. R-squared:                  0.967\nMethod:                 Least Squares   F-statistic:                     1963.\nDate:                Mon, 29 Dec 2025   Prob (F-statistic):          6.68e-146\nTime:                        12:48:04   Log-Likelihood:                -270.14\nNo. Observations:                 200   AIC:                             548.3\nDf Residuals:                     196   BIC:                             561.5\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nconst           6.7502      0.248     27.233      0.000       6.261       7.239\nTV              0.0191      0.002     12.699      0.000       0.016       0.022\nRadio           0.0289      0.009      3.241      0.001       0.011       0.046\nInteraction     0.0011   5.24e-05     20.727      0.000       0.001       0.001\n==============================================================================\nOmnibus:                      128.132   Durbin-Watson:                   2.224\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             1183.719\nSkew:                          -2.323   Prob(JB):                    9.09e-258\nKurtosis:                      13.975   Cond. No.                     1.80e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.8e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\nVemos que el ajuste mejora el \\(R^2\\). Note que el valor \\(p\\) de radio bajó un poco. Esto es porque la interacción era parte de su efecto. En general, si queremos incluir los términos de interacción, debemos incluir cada uno de sus componentes separadamente para evitar confusiones entre la interacción y la simple variable.",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "6. Aprendizaje Automático y Regresión Lineal"
    ]
  },
  {
    "objectID": "6_aprendizaje_automatico_regresion_lineal.html#problemas-con-la-regresión-lineal",
    "href": "6_aprendizaje_automatico_regresion_lineal.html#problemas-con-la-regresión-lineal",
    "title": "Introducción al Aprendizaje Automático con Regresión Lineal",
    "section": "Problemas con la regresión lineal",
    "text": "Problemas con la regresión lineal\nAlgunas situaciones que pueden ser problemáticas para la regresión lineal que hemos presentado.\n\nCorrelación entre errores: Si los errores \\(\\epsilon_i\\) no son independientes no es suficiente minimizar los cuadrados. Debemos minimizar una suma que incluya la covarianza enre mediciones. En general esto reduce los valores \\(p\\). Un ejemplo extremo es si la correlación es perfecta. Al realizar la misma medición muchas veces obtenemos el mismo valor, en realidad hemos hecho una sola medida.\nVarianza no constante (heterosedasticidad): Cuando la varianza es diferente para cada medición, de nuevo podeos resolverlo minimizando una suma de cuadrados que incluya la varianza de cada término. Esto si podemos modelarla o estimarla de alguna manera.\n“Outliers”: Si hay puntos atípicos en las mediciones, estos pueden ser errores de registro de los datos o errores sistemáticos aislados. Al estar muy porfuera de la “nube” de puntos usuales pueden modificar el ajuste y contribuir mucho al error total (no estarán descritos por el modelo de los demás puntos). Cuando se detecta un punto fuera de lo normal, con un \\(t\\) muy grande, es necesario examinarlo detenidamente para saber si es un dato genuino o un error.\nPuntos con gran palanca: Los puntos en los extremos del intervalo tienden a afectar la regresión lineal más que puntos cerca del centro. Un error en esos puntos afecta desmesuradamente el ajuste.\nColinearidad: Cuando dos variables están muy correlacionadas, esto puede inducir problemas a la hora de hacer inferencia con el modelo. Por ejemplo, el ajuste particular puede diluir el valor \\(p\\) de ambas variables, porque puede compensar con una de ellas lo que hace la otra. Para facilitar la interpretación se puede intentar remover una de esas variables y ver cómo cambian las conclusiones.",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "6. Aprendizaje Automático y Regresión Lineal"
    ]
  },
  {
    "objectID": "18_PINNs.html",
    "href": "18_PINNs.html",
    "title": "Redes Neuronales Informadas por la Física (PINNs)",
    "section": "",
    "text": "Una red neuronal informada por la física (PINN, por sus siglas en inglés) es un aproximador de funciones basado en redes neuronales que se entrena no solo con datos, sino también para satisfacer las leyes físicas (normalmente expresadas como ecuaciones diferenciales) que gobiernan el sistema. En otras palabras, una PINN incorpora las ecuaciones gobernantes (por ejemplo, una ecuación diferencial ordinaria [EDO] o una ecuación diferencial parcial [EDP]) en su función de pérdida como restricción o regularizador. Este enfoque fue introducido por Raissi, Perdikaris y Karniadakis en 2017–2019 (Raissi et al. (2017), Raissi et al. (2019)). La idea es ayudarle a la red a generalizar usando la información de la ecuación diferencial.\nPara una revisión reciente, véase Raissi et al. (2024). Este documento se basó en esa revisión, además de material de este libro.\nEste es uno de los métodos más conocidos del llamado “aprendizaje automático científico” (Scientific Machine Learning, SML). Ha recibido críticas recientemente por la fragilidad del método. Es decir, cuando se aplica a ecuaciones diferenciales nuevas, puede que el método no funcione bien. También se ha notado que si se usa para resolver ecuaciones diferenciales numéricamente puede ser más lento que los métodos tradicionales en muchos casos.\n\n\nConsidere una ecuación diferencial ordinaria simple, por ejemplo:\n\\[\ny'(t) + y(t) = 0, \\qquad y(0) = 1,\n\\]\ncuya solución analítica es \\(y(t)=e^{-t}\\). Para resolver esto usando una PINN, aproximamos la solución \\(y(t)\\) con una red neuronal \\(y_\\theta(t)\\) (con parámetros \\(\\theta\\)). La red se entrena para satisfacer tanto la EDO como cualquier dato o condición inicial/de contorno dada. Logramos esto definiendo una función de pérdida que incluye la EDO, llamada una “función de pérdida informada por la física”. Para la EDO anterior, definimos el residuo \\[\nr(t) = y_\\theta'(t) + y_\\theta(t),\n\\]\nque debería ser cero para todo \\(t\\) si \\(y_\\theta(t)\\) satisface exactamente la EDO. La pérdida de la PINN puede entonces construirse para penalizar el residuo y cualquier desviación de las condiciones iniciales:\n\\[\n\\mathcal{J}(\\theta) \\;=\\; \\underbrace{\\frac{1}{N_f}\\sum_{i=1}\n^{N_f} |r(t_i)|^2}_{\\text{pérdida del residuo de la EDO}} \\;+\\; \\underbrace{|\\,y_\\theta(0) - 1\\,|^2}_{\\text{pérdida de la condición inicial}},\n\\]\ndonde \\({t_i}\\) son un conjunto de puntos donde se evalúa la EDO, llamados puntos de colocación, en el dominio de interés (p. ej. \\(t\\in[0,T]\\)). El término \\(y_\\theta'(t)\\) se obtiene mediante diferenciación automática de la red neuronal con respecto a su entrada \\(t\\). Durante el entrenamiento, el optimizador ajusta \\(\\theta\\) para minimizar \\(\\mathcal{J}(\\theta)\\), empujando así a \\(y_\\theta(t)\\) a satisfacer tanto la EDO (haciendo \\(r(t)\\approx 0\\)) como la condición inicial. En efecto, la ley física actúa como un regularizador que guía la red hacia soluciones físicamente consistentes.\n\n\n\nResolver una EDO con una PINN típicamente implica los siguientes pasos:\n\nDefinir la Red Neuronal: Elegir una arquitectura de red neuronal para \\(y_\\theta(t)\\). Esta red tomará \\(t\\) como entrada y producirá una aproximación para \\(y(t)\\).\nConfigurar la Función de Pérdida: Formular la pérdida como la suma de un término basado en la física y términos de datos. Para una EDO, el término físico es el MSE (error cuadrático medio) del residuo de la EDO \\(r(t)=0\\) sobre un conjunto de puntos de colocación \\({t_i}\\), y los términos de datos imponen condiciones iniciales o de contorno (y cualquier punto de datos observado adicional si está disponible).\nDiferenciación Automática: Habilitar la autodiferenciación para \\(t\\) de modo que la salida de la red pueda diferenciarse con respecto a \\(t\\). Esto permite calcular \\(y_\\theta'(t)\\) exactamente (con precisión de máquina) mediante retropropagación, en lugar de usar diferencias finitas.\nOptimizar: Inicializar los parámetros de la red y usar un optimizador (p. ej. descenso de gradiente o Adam) para minimizar la pérdida. Esto requerirá evaluar el residuo y sus gradientes repetidamente para ajustar \\(\\theta\\).\nResultado: Después del entrenamiento, la red neuronal \\(y_\\theta(t)\\) sirve como una solución sustituta de la EDO. Se puede evaluar \\(y_\\theta(t)\\) en cualquier \\(t\\) deseado para obtener la solución (la red neuronal es una interpolación de la solución).\n\nPara ilustrar, a continuación se muestra un ejemplo simple usando PyTorch para la EDO \\(y' + y = 0,; y(0)=1\\). Definimos una red pequeña y la entrenamos con la pérdida informada por la física:\n\nimport torch\nimport torch.nn as nn\n\n# Definir una red neuronal simple para y(t)\nclass RedEDO(nn.Module):\n    def __init__(self):\n        super(RedEDO, self).__init__()\n        self.red = nn.Sequential(\n            nn.Linear(1, 20), nn.Tanh(),\n            nn.Linear(20, 20), nn.Tanh(),\n            nn.Linear(20, 1)\n        )\n    def forward(self, t):\n        return self.red(t)\n\n# Instanciar la red y el optimizador\nmodelo = RedEDO()\noptimizador = torch.optim.Adam(modelo.parameters(), lr=1e-3)\nT_max = 5.0\n\n# Bucle de entrenamiento (entrenamiento informado por la física para EDO)\nfor epoca in range(10000):\n    # Muestrear puntos de colocación en el dominio [0, T]\n    t_coloc = torch.rand(32, 1) * T_max\n    t_coloc.requires_grad_(True)  # habilitar gradiente respecto a t\n    # Calcular la salida de la red y su derivada temporal mediante autograd\n    y_pred = modelo(t_coloc)  # y_θ(t)\n    dy_dt = torch.autograd.grad(y_pred, t_coloc,\n                                grad_outputs=torch.ones_like(y_pred),\n                                create_graph=True)[0]  # y'_θ(t)\n    # Residuo físico para la EDO: f(t) = y'_θ + y_θ\n    f = dy_dt + y_pred\n    perdida_fisica = torch.mean(f**2)  # residuo de la EDO\n    # Pérdida por condición inicial en t=0: (y_θ(0) - 1)^2\n    y0_pred = modelo(torch.zeros(1, 1))  # salida en t=0\n    perdida_ci = (y0_pred - 1.0) ** 2\n    # Pérdida total y paso de optimización\n    perdida = perdida_fisica + perdida_ci\n    optimizador.zero_grad()\n    perdida.backward()\n    optimizador.step()\n\nEn este código, el término torch.autograd.grad(..., create_graph=True) se utiliza para calcular \\(y_\\theta'(t)\\) mediante diferenciación automática. La pérdida física impulsa a la red a satisfacer \\(y' + y = 0\\), mientras que la pérdida de la condición inicial hace tender \\(y_\\theta(0)\\) a \\(1\\). Después del entrenamiento, la salida de la red \\(y_\\theta(t)\\) se aproximará mucho a la solución verdadera \\(e^{-t}\\), ver figura Figura 1. Al entrenar en puntos de colocación muestreados del dominio, la PINN efectivamente “aprende” la solución continua sin pasar explícitamente por el tiempo. Este enfoque puede manejar problemas directos (encontrar \\(y(t)\\) dada la ecuación diferencial y la condición inicial) así como problemas inversos (por ejemplo inferir parámetros en la EDO a partir de datos) dentro del mismo marco.\n\n\nCódigo\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nt_vals = np.linspace(0, 5, 200)\ny_exact = np.exp(-t_vals)\n\n# Evaluar la red neuronal entrenada (modelo) en los valores de t\nt_tensor = torch.from_numpy(t_vals.reshape(-1, 1)).float()\nwith torch.no_grad():\n    y_modelo = modelo(t_tensor).cpu().numpy().flatten()\n\nresiduos = np.abs(y_modelo - y_exact)\n\nfig, axs = plt.subplots(2, 1, figsize=(6, 5), sharex=True, \n                       gridspec_kw={\"height_ratios\": [2, 1]})\n\n# Panel superior: Soluciones\naxs[0].plot(t_vals, y_exact, label=\"$e^{-t}$\", color=\"C1\")\naxs[0].plot(t_vals, y_modelo, label=\"PINN $y_\\\\theta(t)$\", color=\"C0\", linestyle=\"--\")\naxs[0].set_ylabel(\"$y(t)$\")\naxs[0].set_title(\"Solución exacta y PINN para $y(t) = e^{-t}$\")\naxs[0].legend()\naxs[0].grid(True, alpha=0.3)\n\n# Panel inferior: Residuos\naxs[1].plot(t_vals, residuos, color=\"C2\")\naxs[1].axhline(0, color=\"gray\", linestyle=\"--\", linewidth=1)\naxs[1].set_xlabel(\"$t$\")\naxs[1].set_ylabel(\"Error absoluto\")\naxs[1].set_title(\"Residuo PINN - Exacto\")\naxs[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigura 1: Solución exacta y PINN para \\(y(t) = e^{-t}\\)\n\n\n\n\n\nVale la pena señalar que este concepto de usar redes neuronales para resolver ecuaciones diferenciales tiene sus raíces en trabajos anteriores (p. ej. Lagaris et al., 1998). La novedad es la integración perfecta de herramientas modernas de autodiferenciación, entrenamiento eficiente en puntos de colocación y capacidad para incorporar datos adicionales o parámetros desconocidos.",
    "crumbs": [
      "Home",
      "Algunas aplicaciones",
      "18. Redes Neuronales Informadas por la Física (PINNs)"
    ]
  },
  {
    "objectID": "18_PINNs.html#idea-básica-para-una-edo",
    "href": "18_PINNs.html#idea-básica-para-una-edo",
    "title": "Redes Neuronales Informadas por la Física (PINNs)",
    "section": "",
    "text": "Considere una ecuación diferencial ordinaria simple, por ejemplo:\n\\[\ny'(t) + y(t) = 0, \\qquad y(0) = 1,\n\\]\ncuya solución analítica es \\(y(t)=e^{-t}\\). Para resolver esto usando una PINN, aproximamos la solución \\(y(t)\\) con una red neuronal \\(y_\\theta(t)\\) (con parámetros \\(\\theta\\)). La red se entrena para satisfacer tanto la EDO como cualquier dato o condición inicial/de contorno dada. Logramos esto definiendo una función de pérdida que incluye la EDO, llamada una “función de pérdida informada por la física”. Para la EDO anterior, definimos el residuo \\[\nr(t) = y_\\theta'(t) + y_\\theta(t),\n\\]\nque debería ser cero para todo \\(t\\) si \\(y_\\theta(t)\\) satisface exactamente la EDO. La pérdida de la PINN puede entonces construirse para penalizar el residuo y cualquier desviación de las condiciones iniciales:\n\\[\n\\mathcal{J}(\\theta) \\;=\\; \\underbrace{\\frac{1}{N_f}\\sum_{i=1}\n^{N_f} |r(t_i)|^2}_{\\text{pérdida del residuo de la EDO}} \\;+\\; \\underbrace{|\\,y_\\theta(0) - 1\\,|^2}_{\\text{pérdida de la condición inicial}},\n\\]\ndonde \\({t_i}\\) son un conjunto de puntos donde se evalúa la EDO, llamados puntos de colocación, en el dominio de interés (p. ej. \\(t\\in[0,T]\\)). El término \\(y_\\theta'(t)\\) se obtiene mediante diferenciación automática de la red neuronal con respecto a su entrada \\(t\\). Durante el entrenamiento, el optimizador ajusta \\(\\theta\\) para minimizar \\(\\mathcal{J}(\\theta)\\), empujando así a \\(y_\\theta(t)\\) a satisfacer tanto la EDO (haciendo \\(r(t)\\approx 0\\)) como la condición inicial. En efecto, la ley física actúa como un regularizador que guía la red hacia soluciones físicamente consistentes.",
    "crumbs": [
      "Home",
      "Algunas aplicaciones",
      "18. Redes Neuronales Informadas por la Física (PINNs)"
    ]
  },
  {
    "objectID": "18_PINNs.html#procedimiento-de-entrenamiento",
    "href": "18_PINNs.html#procedimiento-de-entrenamiento",
    "title": "Redes Neuronales Informadas por la Física (PINNs)",
    "section": "",
    "text": "Resolver una EDO con una PINN típicamente implica los siguientes pasos:\n\nDefinir la Red Neuronal: Elegir una arquitectura de red neuronal para \\(y_\\theta(t)\\). Esta red tomará \\(t\\) como entrada y producirá una aproximación para \\(y(t)\\).\nConfigurar la Función de Pérdida: Formular la pérdida como la suma de un término basado en la física y términos de datos. Para una EDO, el término físico es el MSE (error cuadrático medio) del residuo de la EDO \\(r(t)=0\\) sobre un conjunto de puntos de colocación \\({t_i}\\), y los términos de datos imponen condiciones iniciales o de contorno (y cualquier punto de datos observado adicional si está disponible).\nDiferenciación Automática: Habilitar la autodiferenciación para \\(t\\) de modo que la salida de la red pueda diferenciarse con respecto a \\(t\\). Esto permite calcular \\(y_\\theta'(t)\\) exactamente (con precisión de máquina) mediante retropropagación, en lugar de usar diferencias finitas.\nOptimizar: Inicializar los parámetros de la red y usar un optimizador (p. ej. descenso de gradiente o Adam) para minimizar la pérdida. Esto requerirá evaluar el residuo y sus gradientes repetidamente para ajustar \\(\\theta\\).\nResultado: Después del entrenamiento, la red neuronal \\(y_\\theta(t)\\) sirve como una solución sustituta de la EDO. Se puede evaluar \\(y_\\theta(t)\\) en cualquier \\(t\\) deseado para obtener la solución (la red neuronal es una interpolación de la solución).\n\nPara ilustrar, a continuación se muestra un ejemplo simple usando PyTorch para la EDO \\(y' + y = 0,; y(0)=1\\). Definimos una red pequeña y la entrenamos con la pérdida informada por la física:\n\nimport torch\nimport torch.nn as nn\n\n# Definir una red neuronal simple para y(t)\nclass RedEDO(nn.Module):\n    def __init__(self):\n        super(RedEDO, self).__init__()\n        self.red = nn.Sequential(\n            nn.Linear(1, 20), nn.Tanh(),\n            nn.Linear(20, 20), nn.Tanh(),\n            nn.Linear(20, 1)\n        )\n    def forward(self, t):\n        return self.red(t)\n\n# Instanciar la red y el optimizador\nmodelo = RedEDO()\noptimizador = torch.optim.Adam(modelo.parameters(), lr=1e-3)\nT_max = 5.0\n\n# Bucle de entrenamiento (entrenamiento informado por la física para EDO)\nfor epoca in range(10000):\n    # Muestrear puntos de colocación en el dominio [0, T]\n    t_coloc = torch.rand(32, 1) * T_max\n    t_coloc.requires_grad_(True)  # habilitar gradiente respecto a t\n    # Calcular la salida de la red y su derivada temporal mediante autograd\n    y_pred = modelo(t_coloc)  # y_θ(t)\n    dy_dt = torch.autograd.grad(y_pred, t_coloc,\n                                grad_outputs=torch.ones_like(y_pred),\n                                create_graph=True)[0]  # y'_θ(t)\n    # Residuo físico para la EDO: f(t) = y'_θ + y_θ\n    f = dy_dt + y_pred\n    perdida_fisica = torch.mean(f**2)  # residuo de la EDO\n    # Pérdida por condición inicial en t=0: (y_θ(0) - 1)^2\n    y0_pred = modelo(torch.zeros(1, 1))  # salida en t=0\n    perdida_ci = (y0_pred - 1.0) ** 2\n    # Pérdida total y paso de optimización\n    perdida = perdida_fisica + perdida_ci\n    optimizador.zero_grad()\n    perdida.backward()\n    optimizador.step()\n\nEn este código, el término torch.autograd.grad(..., create_graph=True) se utiliza para calcular \\(y_\\theta'(t)\\) mediante diferenciación automática. La pérdida física impulsa a la red a satisfacer \\(y' + y = 0\\), mientras que la pérdida de la condición inicial hace tender \\(y_\\theta(0)\\) a \\(1\\). Después del entrenamiento, la salida de la red \\(y_\\theta(t)\\) se aproximará mucho a la solución verdadera \\(e^{-t}\\), ver figura Figura 1. Al entrenar en puntos de colocación muestreados del dominio, la PINN efectivamente “aprende” la solución continua sin pasar explícitamente por el tiempo. Este enfoque puede manejar problemas directos (encontrar \\(y(t)\\) dada la ecuación diferencial y la condición inicial) así como problemas inversos (por ejemplo inferir parámetros en la EDO a partir de datos) dentro del mismo marco.\n\n\nCódigo\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nt_vals = np.linspace(0, 5, 200)\ny_exact = np.exp(-t_vals)\n\n# Evaluar la red neuronal entrenada (modelo) en los valores de t\nt_tensor = torch.from_numpy(t_vals.reshape(-1, 1)).float()\nwith torch.no_grad():\n    y_modelo = modelo(t_tensor).cpu().numpy().flatten()\n\nresiduos = np.abs(y_modelo - y_exact)\n\nfig, axs = plt.subplots(2, 1, figsize=(6, 5), sharex=True, \n                       gridspec_kw={\"height_ratios\": [2, 1]})\n\n# Panel superior: Soluciones\naxs[0].plot(t_vals, y_exact, label=\"$e^{-t}$\", color=\"C1\")\naxs[0].plot(t_vals, y_modelo, label=\"PINN $y_\\\\theta(t)$\", color=\"C0\", linestyle=\"--\")\naxs[0].set_ylabel(\"$y(t)$\")\naxs[0].set_title(\"Solución exacta y PINN para $y(t) = e^{-t}$\")\naxs[0].legend()\naxs[0].grid(True, alpha=0.3)\n\n# Panel inferior: Residuos\naxs[1].plot(t_vals, residuos, color=\"C2\")\naxs[1].axhline(0, color=\"gray\", linestyle=\"--\", linewidth=1)\naxs[1].set_xlabel(\"$t$\")\naxs[1].set_ylabel(\"Error absoluto\")\naxs[1].set_title(\"Residuo PINN - Exacto\")\naxs[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigura 1: Solución exacta y PINN para \\(y(t) = e^{-t}\\)\n\n\n\n\n\nVale la pena señalar que este concepto de usar redes neuronales para resolver ecuaciones diferenciales tiene sus raíces en trabajos anteriores (p. ej. Lagaris et al., 1998). La novedad es la integración perfecta de herramientas modernas de autodiferenciación, entrenamiento eficiente en puntos de colocación y capacidad para incorporar datos adicionales o parámetros desconocidos.",
    "crumbs": [
      "Home",
      "Algunas aplicaciones",
      "18. Redes Neuronales Informadas por la Física (PINNs)"
    ]
  },
  {
    "objectID": "18_PINNs.html#formulación-general",
    "href": "18_PINNs.html#formulación-general",
    "title": "Redes Neuronales Informadas por la Física (PINNs)",
    "section": "Formulación general:",
    "text": "Formulación general:\nConsideremos la siguiente clase de EDPs (extenderlo a otras es directo):\n\\[\n\\partial_t u(t,x) + \\mathcal{N}[u(t,x);\\lambda] = 0, \\qquad x \\in \\Omega,\\;\\; t \\in [0,T],\n\\]\ndonde \\(u(t,x)\\) es la solución desconocida, \\(\\mathcal{N}[\\cdot;\\lambda]\\) es un operador diferencial (no lineal) (que representa la parte espacial de la EDP, con parámetros \\(\\lambda\\) como constantes físicas), y \\(\\Omega\\) es el conjunto de puntos en el espacio donde queremos resolver el problema. Esta forma general puede representar una amplia gama de fenómenos físicos (difusión, advección, leyes de conservación, etc.). Uno de los primeros problemas resueltos con PINNs es la ecuación de Burgers 1D \\(\\partial_t u + u\\partial_x u - \\nu \\partial_x^2 u=0\\). Esta ecuación se ajusta a esta forma con \\(\\mathcal{N}[u] = -u\\partial_x u + \\nu \\partial_x^2 u\\).\nPara resolver tal EDP con una PINN, procedemos de manera similar al caso de las EDO:\n\nDefinimos una red neuronal \\(u_\\theta(t,x)\\) que toma \\((t,x)\\) como entrada y produce un escalar \\(u\\) (la solución predicha). La red es nuestra función de prueba para la solución de la EDP.\nLuego definimos el residuo de la EDP \\(r(t,x)\\) sustituyendo la red en la EDP. Para la forma general anterior, sea\n\n\\[\nr(t,x) \\equiv \\partial_t u_\\theta(t,x) + \\mathcal{N}[\\,u_\\theta(t,x)\\,;\\lambda],\n\\]\nque por construcción debería ser cero en todas partes si \\(u_\\theta\\) satisface exactamente la EDP.\n\nLa función de pérdida se construye para minimizar el residuo cuadrático medio de la EDP y los errores al cumplir las condiciones iniciales/de contorno (y cualquier otro dato). Típicamente escribimos\n\n\\[\n\\mathcal{J}(\\theta) \\;=\\; \\mathcal{J}_{\\text{EDP}}(\\theta) + \\mathcal{J}_{\\text{CI}}(\\theta) + \\mathcal{J}_{\\text{CC}}(\\theta) + \\mathcal{J}_{\\text{datos}}(\\theta),\n\\]\ndonde\n\n\\(\\mathcal{J}_{\\text{EDP}} = \\frac{1}{N_f}\\sum_{i=1}^{N_f} |r(t_i, x_i)|^2\\) es la pérdida física evaluada en un conjunto de \\(N_f\\) puntos de colocación \\((t_i,x_i)\\) en el dominio (a veces elegidos aleatoriamente o en una cuadrícula);\n\\(\\mathcal{J}_{\\text{CI}}\\) es un término que impone la condición inicial (por ejemplo \\(\\frac{1}{N_{ci}}\\sum |u_\\theta(0,x_j) - u_0(x_j)|^2\\) para el perfil inicial dado \\(u_0(x)\\));\n\\(\\mathcal{J}_{\\text{CC}}\\) impone condiciones de contorno (recordemos que ciertos tipos de EDPs requieren ambas condiciones iniciales así como condiciones en el borde del dominio \\(\\Omega\\));\n\\(\\mathcal{J}_{\\text{datos}}\\) (opcional) puede imponer cualquier observación/punto de datos interior adicional que la solución deba igualar.\n\nEl entrenamiento de la PINN busca entonces minimizar \\(\\mathcal{J}(\\theta)\\), encontrando así una \\(u_\\theta(t,x)\\) que simultáneamente (1) se ajuste a cualquier restricción de datos/inicial/de contorno y (2) haga que el residuo de la EDP \\(r(t,x)\\) sea pequeño en todas partes. Ajustando los pesos relativos de estos términos, se pueden manejar casos donde ciertas restricciones son más importantes. Se ha notado que en muchos casos conviene multiplicar la pérdida de condiciones iniciales o de borde por un número grande para obligar a la red a cumplirlas. Se pueden también introducir factores de ponderación para equilibrar la pérdida del residuo frente a la pérdida de datos si es necesario.",
    "crumbs": [
      "Home",
      "Algunas aplicaciones",
      "18. Redes Neuronales Informadas por la Física (PINNs)"
    ]
  },
  {
    "objectID": "18_PINNs.html#diferenciación-automática-para-edps",
    "href": "18_PINNs.html#diferenciación-automática-para-edps",
    "title": "Redes Neuronales Informadas por la Física (PINNs)",
    "section": "Diferenciación automática para EDPs",
    "text": "Diferenciación automática para EDPs\nUna ventaja clave de las PINNs es la facilidad para calcular derivadas parciales usando diferenciación automática (DA) incorporada en los paquetes modernos como Pytorch. La red neuronal \\(u_\\theta(t,x)\\) es una función compuesta (de la entrada a través de las capas de la red); por lo tanto, podemos obtener \\(\\partial_t u\\), \\(\\partial_x u\\), \\(\\partial_{xx} u\\), etc., aplicando autodiferenciación (retropropagación) tal como calcularíamos gradientes para el entrenamiento de la red. Esto significa que no necesitamos derivar fórmulas de diferencias finitas o derivadas simbólicas de la EDP. El paquete calculará directamente las derivadas exactas de la salida de la red con respecto a sus entradas. Este enfoque produce alta precisión para el residuo y evita los errores de truncamiento asociados con la diferenciación numérica. En esencia, la PINN trata el operador diferencial \\(\\mathcal{N}\\) como parte del grafo computacional de la red, usando la regla de la cadena para evaluarlo.",
    "crumbs": [
      "Home",
      "Algunas aplicaciones",
      "18. Redes Neuronales Informadas por la Física (PINNs)"
    ]
  },
  {
    "objectID": "18_PINNs.html#ejemplo-ecuación-de-burgers",
    "href": "18_PINNs.html#ejemplo-ecuación-de-burgers",
    "title": "Redes Neuronales Informadas por la Física (PINNs)",
    "section": "Ejemplo (ecuación de Burgers)",
    "text": "Ejemplo (ecuación de Burgers)\nPara concretar, supongamos que queremos resolver la ecuación de Burgers 1D \\(\\partial_t u + u \\partial_x u - \\nu \\partial_{xx} u = 0\\) con una PINN. Configuramos una red neuronal \\(u_\\theta(t,x)\\) e incluimos la EDP de Burgers en la pérdida. Un fragmento del código de entrenamiento podría verse así:\n# Muestrea puntos de colocación en el dominio (t en [0,T], x en [a,b])\nt_coloc = torch.rand(N, 1) * T_max\nx_coloc = torch.rand(N, 1) * (b - a) + a\nt_coloc.requires_grad_(), x_coloc.requires_grad_()  # habilita derivadas parciales\n\n# Salida de la PINN para estos puntos\nu_predicho = modelo(torch.hstack([t_coloc, x_coloc]))  # u_θ(t,x)\n\n# Calcula derivadas parciales usando autograd\nu_t   = torch.autograd.grad(u_predicho, t_coloc, torch.ones_like(u_predicho), create_graph=True)[0]\nu_x   = torch.autograd.grad(u_predicho, x_coloc, torch.ones_like(u_predicho), create_graph=True)[0]\nu_xx  = torch.autograd.grad(u_x,       x_coloc, torch.ones_like(u_x),        create_graph=True)[0]\n\n# Residuo de la EDP r(t,x) = u_t + u * u_x - ν * u_xx\nr = u_t + u_predicho * u_x - nu * u_xx\nperdida_fisica = torch.mean(r**2)\n\n# Pérdida de la condición inicial (por ejemplo, u(0,x) = u0(x))\nu_inicial = modelo(torch.hstack([torch.zeros(N0,1), x0_muestras]))\nperdida_ci = torch.mean((u_inicial - u0(x0_muestras))**2)\n\n# Pérdida de la condición de frontera (por ejemplo, Dirichlet u(t,a)=g1(t), u(t,b)=g2(t))\nu_izq  = modelo(torch.hstack([t_coloc, a*torch.ones_like(t_coloc)]))\nu_der  = modelo(torch.hstack([t_coloc, b*torch.ones_like(t_coloc)]))\nperdida_cf = torch.mean((u_izq - g1(t_coloc))**2 + (u_der - g2(t_coloc))**2)\n\n# Pérdida total: física + inicial + frontera\nperdida_total = perdida_fisica + perdida_ci + perdida_cf\nAquí vemos cómo se utilizan los puntos de colocación \\((t_{\\text{colloc}},x_{\\text{colloc}})\\) para evaluar el residuo de la EDP en todo el dominio, y cómo la diferenciación automática produce \\(\\partial_t u\\), \\(\\partial_x u\\), \\(\\partial_{xx} u\\) para calcular el residuo \\(f(t,x)\\). La función de pérdida penaliza el residuo (haciendo que la solución satisfaga la EDP) e impone las condiciones iniciales y de contorno. Al minimizar esta pérdida, la red aprende una solución \\(u_\\theta(t,x)\\) que satisface aproximadamente la EDP en todas partes y cumple aproximadamente las restricciones.",
    "crumbs": [
      "Home",
      "Algunas aplicaciones",
      "18. Redes Neuronales Informadas por la Física (PINNs)"
    ]
  },
  {
    "objectID": "18_PINNs.html#alternativas-para-la-condición-de-contorno",
    "href": "18_PINNs.html#alternativas-para-la-condición-de-contorno",
    "title": "Redes Neuronales Informadas por la Física (PINNs)",
    "section": "Alternativas para la condición de contorno",
    "text": "Alternativas para la condición de contorno\nEn el ejemplo anterior, impusimos condiciones de contorno en la pérdida (a menudo llamado cumplimiento “suave”). En muchas implementaciones de PINN, esto funciona bien ponderando fuertemente la pérdida de CC para asegurar la precisión en los contornos.\nUna alternativa es el cumplimiento “duro”, donde incorporamos la condición de contorno en el ansatz de la red neuronal para que se satisfaga exactamente por construcción (por ejemplo añadiendo una función de penalización o usando una arquitectura especial).\nPara la EDO considerada en la sección anterior, podemos definir \\(y_\\theta(t) = 1 + t u_\\theta(t)\\) donde \\(u_\\theta(t)\\) es la red neuronal. Entonces \\(y_\\theta(t)\\) satisface la condición de contorno \\(y_\\theta(0) = 1\\) por construcción y no necesitamos agregar un término correspondiente a la pérdida.\nPara una EDP, se puede construir \\(u_\\theta(t,x) = g(x,t) + H(x,t)\\tilde{u}\\theta(t,x)\\) donde \\(g\\) es una función simple que satisface las condiciones de contorno y \\(H\\) es una función que se anula en el contorno, asegurando que \\(u\\theta\\) coincida con \\(g\\) en el contorno para cualquier \\(\\tilde{u}_\\theta\\).",
    "crumbs": [
      "Home",
      "Algunas aplicaciones",
      "18. Redes Neuronales Informadas por la Física (PINNs)"
    ]
  },
  {
    "objectID": "18_PINNs.html#precisión-y-convergencia",
    "href": "18_PINNs.html#precisión-y-convergencia",
    "title": "Redes Neuronales Informadas por la Física (PINNs)",
    "section": "Precisión y Convergencia",
    "text": "Precisión y Convergencia\nLos métodos numéricos tradicionales tienen un comportamiento de convergencia bien entendido. Por ejemplo las diferencias finitas tienen un cierto orden de precisión dependiendo de la resolución de la malla y el esquema (refinar la malla produce sistemáticamente menos error, en el curso de métodos numéricos diríamos que el error de un cierto método escala como \\(h^n\\)).\nLas PINNs, al estar basadas en optimización, no tienen teoremas asociados que garantizan el mismo tipo de convergencia con el tamaño de la red o las iteraciones de entrenamiento de una manera simple. En la práctica, lograr una precisión de alto orden puede ser difícil para las PINNs. Un método clásico (de elementos finitos o espectral) de alto orden bien ajustado podría alcanzar un error muy bajo con un esfuerzo computacional moderado, mientras que una PINN podría requerir una red muy grande y un entrenamiento prolongado para igualar esa precisión.\nLa literatura informa que las PINNs a menudo tienen menor precisión que los esquemas de alto orden para problemas directos, especialmente en casos donde la solución tiene gradientes agudos o características multiescala. Existen intentos de mejorar la precisión de las PINN mediante muestreo adaptativo, mejores arquitecturas y formulaciones variacionales (p. ej. vpPINNs).",
    "crumbs": [
      "Home",
      "Algunas aplicaciones",
      "18. Redes Neuronales Informadas por la Física (PINNs)"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aprendizaje Automático para Física",
    "section": "",
    "text": "Este curso está diseñado específicamente para estudiantes de física que desean comprender y aplicar técnicas de aprendizaje automático en su investigación. Nuestro enfoque se centra en construir una comprensión intuitiva y profunda de los fundamentos.\n\n\nEl curso se divide en cuatro partes principales, las escritas hasta ahora son:\n\nFundamentos de Probabilidad y Estadística\n\nTeoría de probabilidad (en construcción)\nDistribuciones de probabilidad y estimación\nVerosimilitud y métodos de inferencia\nMínimos cuadrados e intervalos de confianza\nTesteo de hipótesis y teoría de la información\n\nBases del Aprendizaje Automático\n\nRegresión lineal y no lineal\nMétodos de clasificación\nSelección y validación de modelos\nÁrboles de decisión y métodos de ensemble\nMáquinas de vectores de soporte y aprendizaje no supervisado\n\nRedes Neuronales\n\nEl perceptrón multicapa\nDescenso de gradiente y diferenciación automática\nEvaluación de modelos y regularización de redes neuronales\nRedes convolucionales\nRedes Residuales\nTransformadores y el mecanismo de atención\n\n\n\n\n\nAl finalizar este curso, los estudiantes serán capaces de:\n\nComprender los fundamentos matemáticos detrás de los métodos de machine learning\nIdentificar qué técnicas son apropiadas para diferentes problemas en física\nImplementar y adaptar algoritmos de aprendizaje automático para aplicaciones específicas\nEvaluar críticamente los resultados y limitaciones de diferentes métodos\nComprender los principios fundamentales de las redes neuronales y su aplicación en problemas físicos\n\n\n\n\nEl curso combina teoría rigurosa con ejemplos prácticos de física. Cada tema incluye:\n\nFundamentos matemáticos\nImplementación en Python\nEjemplos y aplicaciones en física\nEjercicios prácticos\n\n\n\n\nEstas notas están en constante actualización. En el futuro, exploraremos temas avanzados como:\n\nInferencia basada en simulaciones\nRedes neuronales con restricciones físicas (Physics-Informed Neural Networks)\n\n\n\n\n\nCálculo multivariable\nÁlgebra lineal básica\nProgramación básica en Python\nFísica universitaria general\n\n¡Esperamos que este curso les proporcione las herramientas necesarias para aplicar el aprendizaje automático en sus investigaciones en física!"
  },
  {
    "objectID": "index.html#estructura-del-curso",
    "href": "index.html#estructura-del-curso",
    "title": "Aprendizaje Automático para Física",
    "section": "",
    "text": "El curso se divide en cuatro partes principales, las escritas hasta ahora son:\n\nFundamentos de Probabilidad y Estadística\n\nTeoría de probabilidad (en construcción)\nDistribuciones de probabilidad y estimación\nVerosimilitud y métodos de inferencia\nMínimos cuadrados e intervalos de confianza\nTesteo de hipótesis y teoría de la información\n\nBases del Aprendizaje Automático\n\nRegresión lineal y no lineal\nMétodos de clasificación\nSelección y validación de modelos\nÁrboles de decisión y métodos de ensemble\nMáquinas de vectores de soporte y aprendizaje no supervisado\n\nRedes Neuronales\n\nEl perceptrón multicapa\nDescenso de gradiente y diferenciación automática\nEvaluación de modelos y regularización de redes neuronales\nRedes convolucionales\nRedes Residuales\nTransformadores y el mecanismo de atención"
  },
  {
    "objectID": "index.html#objetivos-del-curso",
    "href": "index.html#objetivos-del-curso",
    "title": "Aprendizaje Automático para Física",
    "section": "",
    "text": "Al finalizar este curso, los estudiantes serán capaces de:\n\nComprender los fundamentos matemáticos detrás de los métodos de machine learning\nIdentificar qué técnicas son apropiadas para diferentes problemas en física\nImplementar y adaptar algoritmos de aprendizaje automático para aplicaciones específicas\nEvaluar críticamente los resultados y limitaciones de diferentes métodos\nComprender los principios fundamentales de las redes neuronales y su aplicación en problemas físicos"
  },
  {
    "objectID": "index.html#metodología",
    "href": "index.html#metodología",
    "title": "Aprendizaje Automático para Física",
    "section": "",
    "text": "El curso combina teoría rigurosa con ejemplos prácticos de física. Cada tema incluye:\n\nFundamentos matemáticos\nImplementación en Python\nEjemplos y aplicaciones en física\nEjercicios prácticos"
  },
  {
    "objectID": "index.html#próximos-temas",
    "href": "index.html#próximos-temas",
    "title": "Aprendizaje Automático para Física",
    "section": "",
    "text": "Estas notas están en constante actualización. En el futuro, exploraremos temas avanzados como:\n\nInferencia basada en simulaciones\nRedes neuronales con restricciones físicas (Physics-Informed Neural Networks)"
  },
  {
    "objectID": "index.html#requisitos",
    "href": "index.html#requisitos",
    "title": "Aprendizaje Automático para Física",
    "section": "",
    "text": "Cálculo multivariable\nÁlgebra lineal básica\nProgramación básica en Python\nFísica universitaria general\n\n¡Esperamos que este curso les proporcione las herramientas necesarias para aplicar el aprendizaje automático en sus investigaciones en física!"
  },
  {
    "objectID": "index.html#ejercicios",
    "href": "index.html#ejercicios",
    "title": "Aprendizaje Automático para Física",
    "section": "Ejercicios",
    "text": "Ejercicios\nAl final de cada clase hay ejercicios sugeridos. Los números se refieren al libro correspondiente en cada unidad. Por ejemplo, si se escribe 1.1 en una de las clases de la unidad de Probabilidad y Estadística, se refiere al ejercicio 1.1 del libro “Statistics for Physical Sciences”."
  },
  {
    "objectID": "index.html#otros-libros-recomendados",
    "href": "index.html#otros-libros-recomendados",
    "title": "Aprendizaje Automático para Física",
    "section": "Otros libros recomendados",
    "text": "Otros libros recomendados\n\n“Deep Learning”, I. Goodfellow, Y. Bengio, A. Courville, MIT Press (2016).\n“Dive into Deep Learning”, A. Chen, S. Bird, L. Catlett, Dive into Deep Learning (2021). Disponible libremente en https://d2l.ai/"
  },
  {
    "objectID": "index.html#otros-recursos",
    "href": "index.html#otros-recursos",
    "title": "Aprendizaje Automático para Física",
    "section": "Otros recursos",
    "text": "Otros recursos\n\nStanford Statistical Learning with Python\nVideos basados en “Understanding Deep Learning” por T. Elsayed"
  },
  {
    "objectID": "13_evaluacion_y_regularizacion_de_nn.html",
    "href": "13_evaluacion_y_regularizacion_de_nn.html",
    "title": "Evaluación de modelos y regularización",
    "section": "",
    "text": "Hemos visto cómo entrenar una red neuronal para que aprenda a predecir. Ahora vamos a repasar cómo evaluar el rendimiento de una red neuronal. Usaremos las mismas técnicas que vimos para evaluar el rendimiento de cualquier modelo de aprendizaje supervisado. Sin embargo, las redes neuronales tienen un par de particularidades, como el llamado “descenso doble” (double descent).\nLuego veremos cómo regularizar las redes neuronales para evitar el sobreajuste y mejorar su generalización. De nuevo empezaremos con un repaso de lo visto para cualquier modelo de aprendizaje supervisado. En el sentido estricto, la regularización se refiere a métodos para restringir el espacio de búsqueda de modelos, es decir, para limitar la complejidad del modelo. Sin embargo, en la práctica, la regularización suele referirse a métodos que reducen el error de generalización. Veremos algunos de esos métodos al final de estas notas.\n\n\nRecordemos que el error de generalización consiste en el error que comete un modelo entrenado con un conjunto de datos al intentar predecir el valor de una nueva observación. Este error viene de tres fuentes:\n\nVarianza: Mide la variabilidad del modelo.\nSesgo: Mide el error sistemático del modelo.\nError irreducible: Mide el error que no puede ser reducido por el modelo.\n\n\n\nPrimero repasemos de manera intuitiva cada una de estas fuentes de error.\n\nVarianza: Mide la variabilidad del modelo. Es decir, cuánto cambia la predicción del modelo al cambiar el conjunto de datos de entrenamiento. Un modelo con alta varianza es poco fiable: El hecho que la predicción cambie mucho al cambiar el conjunto de datos de entrenamiento indica que el modelo está ajustando demasiado a las particularidades de esos datos. No nos esperamos que haga una buena predicción en un dato no visto.\nSesgo: Mide el error sistemático del modelo. Es decir, cuánto se desvía la predicción del modelo de la verdadera relación entre las variables. Un modelo con alto sesgo subestima o sobreestima sistemáticamente el valor de las predicciones. Usualmente ocurre cuando el modelo es demasiado simple para el problema que estamos modelando. De nuevo, un modelo con alto sesgo es poco fiable: Si el modelo es demasiado simple, no puede capturar la verdadera relación entre las variables, y por lo tanto no es capaz de hacer predicciones precisas.\nError irreducible: Mide el error que no puede ser reducido por el modelo. Es decir, el error que existiría incluso si tuviéramos acceso a un conjunto de datos infinito. Este error es intrínseco al problema que estamos modelando. Este puede venir por errores en las mediciones de las variables, o porque la respuesta que queremos predecir depende de variables no observadas, o tiene una naturaleza estocástica.\n\n\n\n\nVeamos matemáticamente cómo aparecen estas tres fuentes de error. Supongamos que tenemos un conjunto de datos de entrenamiento \\(D = \\{(x_i, y_i)\\}_{i=1}^n\\), donde \\(x_i\\) es un vector de características y \\(y_i\\) es la respuesta que queremos predecir. Supongamos que la respuesta \\(y\\) sigue una distribución de probabilidad \\(P(y|x)\\) con media \\(\\mu(x)\\) y varianza fija \\(\\sigma^2\\). Calculemos la pérdida de error cuadrático medio (MSE) de un modelo \\(h\\) que intenta predecir \\(y\\) a partir de \\(x\\). La predicción del modelo \\(f(x;\\theta)\\) es una función de los parámetros \\(\\theta\\) del modelo.\n\\[\n\\begin{multline}\n\\langle\\mathcal{J}(h)\\rangle_{y|x} = \\langle(f(x;\\theta) - y)^2\\rangle_{y|x} = \\langle(f(x;\\theta) - \\mu(x))^2 + 2(f(x;\\theta) - \\mu(x))(y - \\mu(x)) + (y - \\mu(x))^2\\rangle_{y|x} \\\\ = \\langle(f(x;\\theta) - \\mu(x))^2\\rangle_{y|x} + \\sigma^2\\,.\n\\end{multline}\n\\]\nAquí hemos logrado separar el error en dos términos: La diferencia cuadrática media entre la predicción y la media, más una varianza en los datos. Esta varianza de los datos es intrínseca al problema, y no puede ser reducida por el modelo. Podemos intentar reducir el primer término, ya que podemos mejorar nuestra predicción \\(f(x;\\theta)\\) para que se acerque más a la media \\(\\mu(x)\\). Note que para simplificar el álgebra, hemos tomado el valor esperado teniendo los datos de entrenamiento fijos, tal que sólo \\(y\\) varía de acuerdo a la distribución \\(P(y|x)\\).\nEse error reducible se puede descomponer adicionalmente\n\\[\n\\langle\\mathcal{J}(h)\\rangle = \\langle(f(x;\\theta) - \\langle f(x;\\theta)\\rangle)^2\\rangle + \\langle(\\langle f(x;\\theta)\\rangle - \\mu(x))^2\\rangle + \\sigma^2\\,.\n\\]\nEl primer término representa la fluctuación de la predicción del modelo alrededor de su valor esperado. Es decir, mide la varianza del modelo o cuánto cambia la predicción del modelo al cambiar el conjunto de datos de entrenamiento. El segundo término representa el error sistemático del modelo, es decir, cuánto se desvía la predicción del modelo de la verdadera relación entre las variables. Este representa el sesgo del modelo.\n\n\n\nPodemos intentar reducir el error debido al sesgo y la varianza. Para obtener un mejor sesgo, podemos usar un modelo más complejo. Sin embargo, un modelo más complejo tiende a ajustarse a las particularidades del conjunto de datos de entrenamiento, lo que aumenta la varianza. La manera de resolver este problema es mediante el uso de modelos más sencillos que no sobreajusten, o mediante el uso de técnicas de regularización que amortigüen el efecto de los muchos parámetros libres de los modelos más complejos.\nUna alternativa que reduce la varianza es aumentar el conjunto de datos de entrenamiento. Esto mejora los resultados ya que la varianza se reduce con más datos.\nRecientemente se ha observado un fenómeno conocido como “descenso doble” (double descent), en el cual el modelo tiene un menor error de generalización para algunos modelos extremadamente complejos. Se cree que esto se debe a una regularización implícita, como discutimos en la siguiente sección.\n\n\n\n\nPara las redes neuronales, la tensión entre sesgo y varianza tiene un aspecto interesante. Lo que dice el aprendizaje automático clásico es que al aumentar el número de parámetros del modelo, éste se ajusta más al ruido y por lo tanto el error de generalización aumenta. Esto podría ser un problema para las redes neuronales que tienen un número enorme de parámetros, a veces mucho mayor que el número de observaciones de entrenamiento.\nSin embargo, se ha observado que para ciertos modelos, el error de generalización puede disminuir con el aumento del número de parámetros. Esto se llama “descenso doble” (double descent). Es decir, el error de generalización tiene tres regiones:\n\nCuando la red tiene muy pocos parámetros, el error de generalización es alto debido a que el modelo no tiene suficiente libertad para ajustarse a los datos. El modelo tiene un alto sesgo. A medida que se aumentan el número de parámetros, el sesgo baja y el error de generalización disminuye hasta un mínimo.\nSi se sigue aumentando el número de parámetros, el error de generalización aumenta. Esto se debe a que el modelo se ajusta más al ruido y por lo tanto el error de generalización aumenta. El modelo tiene una alta varianza. Esto ocurre hasta un punto en el que el número de parámetros es comparable al número de datos.\nSi se sigue aumentando el número de parámetros, ¡el error de generalización disminuye de nuevo!. Puede incluso alcanzar valores menores que el mínimo encontrado en la primera región. Esto es lo que se llama descenso doble (double descent).\n\nEl origen de este fenómeno no se entiende del todo bien. Se cree que es una mezcla entre una regularización implícita y la alta dimensionalidad de los datos.\nAlta dimensionalidad: En algunos problemas, el número de dimensiones de los datos es alto. Los espacios de alta dimensionalidad tienen propiedades poco intuitivas. Por ejemplo, la mayoría del volumen de una hiperesfera de alta dimensión está cerca de la superficie (la mayoría del jugo de una hipernaranja viene de cerca de la cáscara). Si lanzamos puntos de forma aleatoria en un espacio de alta dimensionalidad, sus vectores de posición son casi ortogonales. Además, la distancia entre dos puntos típicos crece exponencialmente con el número de dimensiones. Entonces, si ajustamos un modelo con muchos parámetros a muchos datos, sigue siendo verdad que el modelo se ajustará al ruido. Sin embargo, si tomamos un nuevo dato, este estará lejos de todos los datos de entrenamiento. Por lo tanto, el modelo deberá interpolar para el nuevo dato.\nRegularización implícita: Cuando el número de parámetros es muy grande, las redes neuronales tienden a producir funciones suaves en las regiones donde los datos son escasos. Arriba dijimos que cuando la dimensión es alta la red se ve obligada a interpolar ya que un dato típico está lejos de todos los datos de entrenamiento. Por lo tanto, la interpolación para un nuevo dato será en una región donde el modelo da una función suave con poca varianza. Por esto el error de generalización es menor de lo esperado. Aún no se sabe del todo bien por qué las redes neuronales tienden a producir funciones suaves en las regiones de interpolación.\nLa figura 8.10 del libro ilustra el fenómeno de descenso doble.\n\n\n\nLas redes neuronales que hemos visto tienen varios hiperparámetros: Número de capas escondidas, número de neuronas por cada capa, tasa de aprendizaje y otros dependiendo del algoritmo de descenso de gradiente. Además veremos muchas otras arquitecturas que pueden ser usadas para diferentes problemas. Entonces necesitamos alguna manera de escoger la arquitectura y los hiperparámetros adecuados para cada caso.\nEl problema es que el espacio de parámetros es de una dimensión muy alta y además algunos de estos no son continuos (no podemos tener 3.58 capas escondidas). Esto hace que no podamos usar algo como descenso de gradiente en dicho espacio.\nUna manera de hacerlo es estimar la distribución de probabilidad del error de generalización como función de los parámetros. Es decir, asumimos que dado un conjunto de hiperparámetros \\(p\\), el error de generalización es una variable aleatoria dada por una distribución. Si tuviéramos una manera de estimarla, podríamos escoger los que maximizan esa probabilidad. Lamentablemente, esto es un problema extremadamente difícil.\nLo mejor que podemos hacer es ajustar el modelo para una serie de datos, medir el desempeño en estos y usar esto para ajustar un modelo de esa distribución. Luego se escogen nuevos hiperparámetros a partir de esa distribución estimada y se itera. Esto se hace bien sea con estrategias bayesianas, árboles de deisión, entre otros.\nOtra forma de explorar el espacio de parámetros es el hiberbanda. En este, ajustamos muchos modelos por muy pocas épocas. Luego se escogen la fracción \\(\\eta\\) de los mejores entre ellos y se aumenta el número de epocas por \\(1/\\eta\\), y se itera.\nExisten librerías que implementan ambos métodos.",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "13. Evaluación y Regularización de redes neuronales"
    ]
  },
  {
    "objectID": "13_evaluacion_y_regularizacion_de_nn.html#repaso-de-varianza-sesgo-y-error-irreducible",
    "href": "13_evaluacion_y_regularizacion_de_nn.html#repaso-de-varianza-sesgo-y-error-irreducible",
    "title": "Evaluación de modelos y regularización",
    "section": "",
    "text": "Recordemos que el error de generalización consiste en el error que comete un modelo entrenado con un conjunto de datos al intentar predecir el valor de una nueva observación. Este error viene de tres fuentes:\n\nVarianza: Mide la variabilidad del modelo.\nSesgo: Mide el error sistemático del modelo.\nError irreducible: Mide el error que no puede ser reducido por el modelo.\n\n\n\nPrimero repasemos de manera intuitiva cada una de estas fuentes de error.\n\nVarianza: Mide la variabilidad del modelo. Es decir, cuánto cambia la predicción del modelo al cambiar el conjunto de datos de entrenamiento. Un modelo con alta varianza es poco fiable: El hecho que la predicción cambie mucho al cambiar el conjunto de datos de entrenamiento indica que el modelo está ajustando demasiado a las particularidades de esos datos. No nos esperamos que haga una buena predicción en un dato no visto.\nSesgo: Mide el error sistemático del modelo. Es decir, cuánto se desvía la predicción del modelo de la verdadera relación entre las variables. Un modelo con alto sesgo subestima o sobreestima sistemáticamente el valor de las predicciones. Usualmente ocurre cuando el modelo es demasiado simple para el problema que estamos modelando. De nuevo, un modelo con alto sesgo es poco fiable: Si el modelo es demasiado simple, no puede capturar la verdadera relación entre las variables, y por lo tanto no es capaz de hacer predicciones precisas.\nError irreducible: Mide el error que no puede ser reducido por el modelo. Es decir, el error que existiría incluso si tuviéramos acceso a un conjunto de datos infinito. Este error es intrínseco al problema que estamos modelando. Este puede venir por errores en las mediciones de las variables, o porque la respuesta que queremos predecir depende de variables no observadas, o tiene una naturaleza estocástica.\n\n\n\n\nVeamos matemáticamente cómo aparecen estas tres fuentes de error. Supongamos que tenemos un conjunto de datos de entrenamiento \\(D = \\{(x_i, y_i)\\}_{i=1}^n\\), donde \\(x_i\\) es un vector de características y \\(y_i\\) es la respuesta que queremos predecir. Supongamos que la respuesta \\(y\\) sigue una distribución de probabilidad \\(P(y|x)\\) con media \\(\\mu(x)\\) y varianza fija \\(\\sigma^2\\). Calculemos la pérdida de error cuadrático medio (MSE) de un modelo \\(h\\) que intenta predecir \\(y\\) a partir de \\(x\\). La predicción del modelo \\(f(x;\\theta)\\) es una función de los parámetros \\(\\theta\\) del modelo.\n\\[\n\\begin{multline}\n\\langle\\mathcal{J}(h)\\rangle_{y|x} = \\langle(f(x;\\theta) - y)^2\\rangle_{y|x} = \\langle(f(x;\\theta) - \\mu(x))^2 + 2(f(x;\\theta) - \\mu(x))(y - \\mu(x)) + (y - \\mu(x))^2\\rangle_{y|x} \\\\ = \\langle(f(x;\\theta) - \\mu(x))^2\\rangle_{y|x} + \\sigma^2\\,.\n\\end{multline}\n\\]\nAquí hemos logrado separar el error en dos términos: La diferencia cuadrática media entre la predicción y la media, más una varianza en los datos. Esta varianza de los datos es intrínseca al problema, y no puede ser reducida por el modelo. Podemos intentar reducir el primer término, ya que podemos mejorar nuestra predicción \\(f(x;\\theta)\\) para que se acerque más a la media \\(\\mu(x)\\). Note que para simplificar el álgebra, hemos tomado el valor esperado teniendo los datos de entrenamiento fijos, tal que sólo \\(y\\) varía de acuerdo a la distribución \\(P(y|x)\\).\nEse error reducible se puede descomponer adicionalmente\n\\[\n\\langle\\mathcal{J}(h)\\rangle = \\langle(f(x;\\theta) - \\langle f(x;\\theta)\\rangle)^2\\rangle + \\langle(\\langle f(x;\\theta)\\rangle - \\mu(x))^2\\rangle + \\sigma^2\\,.\n\\]\nEl primer término representa la fluctuación de la predicción del modelo alrededor de su valor esperado. Es decir, mide la varianza del modelo o cuánto cambia la predicción del modelo al cambiar el conjunto de datos de entrenamiento. El segundo término representa el error sistemático del modelo, es decir, cuánto se desvía la predicción del modelo de la verdadera relación entre las variables. Este representa el sesgo del modelo.\n\n\n\nPodemos intentar reducir el error debido al sesgo y la varianza. Para obtener un mejor sesgo, podemos usar un modelo más complejo. Sin embargo, un modelo más complejo tiende a ajustarse a las particularidades del conjunto de datos de entrenamiento, lo que aumenta la varianza. La manera de resolver este problema es mediante el uso de modelos más sencillos que no sobreajusten, o mediante el uso de técnicas de regularización que amortigüen el efecto de los muchos parámetros libres de los modelos más complejos.\nUna alternativa que reduce la varianza es aumentar el conjunto de datos de entrenamiento. Esto mejora los resultados ya que la varianza se reduce con más datos.\nRecientemente se ha observado un fenómeno conocido como “descenso doble” (double descent), en el cual el modelo tiene un menor error de generalización para algunos modelos extremadamente complejos. Se cree que esto se debe a una regularización implícita, como discutimos en la siguiente sección.",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "13. Evaluación y Regularización de redes neuronales"
    ]
  },
  {
    "objectID": "13_evaluacion_y_regularizacion_de_nn.html#descenso-doble-double-descent",
    "href": "13_evaluacion_y_regularizacion_de_nn.html#descenso-doble-double-descent",
    "title": "Evaluación de modelos y regularización",
    "section": "",
    "text": "Para las redes neuronales, la tensión entre sesgo y varianza tiene un aspecto interesante. Lo que dice el aprendizaje automático clásico es que al aumentar el número de parámetros del modelo, éste se ajusta más al ruido y por lo tanto el error de generalización aumenta. Esto podría ser un problema para las redes neuronales que tienen un número enorme de parámetros, a veces mucho mayor que el número de observaciones de entrenamiento.\nSin embargo, se ha observado que para ciertos modelos, el error de generalización puede disminuir con el aumento del número de parámetros. Esto se llama “descenso doble” (double descent). Es decir, el error de generalización tiene tres regiones:\n\nCuando la red tiene muy pocos parámetros, el error de generalización es alto debido a que el modelo no tiene suficiente libertad para ajustarse a los datos. El modelo tiene un alto sesgo. A medida que se aumentan el número de parámetros, el sesgo baja y el error de generalización disminuye hasta un mínimo.\nSi se sigue aumentando el número de parámetros, el error de generalización aumenta. Esto se debe a que el modelo se ajusta más al ruido y por lo tanto el error de generalización aumenta. El modelo tiene una alta varianza. Esto ocurre hasta un punto en el que el número de parámetros es comparable al número de datos.\nSi se sigue aumentando el número de parámetros, ¡el error de generalización disminuye de nuevo!. Puede incluso alcanzar valores menores que el mínimo encontrado en la primera región. Esto es lo que se llama descenso doble (double descent).\n\nEl origen de este fenómeno no se entiende del todo bien. Se cree que es una mezcla entre una regularización implícita y la alta dimensionalidad de los datos.\nAlta dimensionalidad: En algunos problemas, el número de dimensiones de los datos es alto. Los espacios de alta dimensionalidad tienen propiedades poco intuitivas. Por ejemplo, la mayoría del volumen de una hiperesfera de alta dimensión está cerca de la superficie (la mayoría del jugo de una hipernaranja viene de cerca de la cáscara). Si lanzamos puntos de forma aleatoria en un espacio de alta dimensionalidad, sus vectores de posición son casi ortogonales. Además, la distancia entre dos puntos típicos crece exponencialmente con el número de dimensiones. Entonces, si ajustamos un modelo con muchos parámetros a muchos datos, sigue siendo verdad que el modelo se ajustará al ruido. Sin embargo, si tomamos un nuevo dato, este estará lejos de todos los datos de entrenamiento. Por lo tanto, el modelo deberá interpolar para el nuevo dato.\nRegularización implícita: Cuando el número de parámetros es muy grande, las redes neuronales tienden a producir funciones suaves en las regiones donde los datos son escasos. Arriba dijimos que cuando la dimensión es alta la red se ve obligada a interpolar ya que un dato típico está lejos de todos los datos de entrenamiento. Por lo tanto, la interpolación para un nuevo dato será en una región donde el modelo da una función suave con poca varianza. Por esto el error de generalización es menor de lo esperado. Aún no se sabe del todo bien por qué las redes neuronales tienden a producir funciones suaves en las regiones de interpolación.\nLa figura 8.10 del libro ilustra el fenómeno de descenso doble.",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "13. Evaluación y Regularización de redes neuronales"
    ]
  },
  {
    "objectID": "13_evaluacion_y_regularizacion_de_nn.html#búsqueda-de-hiperparámetros",
    "href": "13_evaluacion_y_regularizacion_de_nn.html#búsqueda-de-hiperparámetros",
    "title": "Evaluación de modelos y regularización",
    "section": "",
    "text": "Las redes neuronales que hemos visto tienen varios hiperparámetros: Número de capas escondidas, número de neuronas por cada capa, tasa de aprendizaje y otros dependiendo del algoritmo de descenso de gradiente. Además veremos muchas otras arquitecturas que pueden ser usadas para diferentes problemas. Entonces necesitamos alguna manera de escoger la arquitectura y los hiperparámetros adecuados para cada caso.\nEl problema es que el espacio de parámetros es de una dimensión muy alta y además algunos de estos no son continuos (no podemos tener 3.58 capas escondidas). Esto hace que no podamos usar algo como descenso de gradiente en dicho espacio.\nUna manera de hacerlo es estimar la distribución de probabilidad del error de generalización como función de los parámetros. Es decir, asumimos que dado un conjunto de hiperparámetros \\(p\\), el error de generalización es una variable aleatoria dada por una distribución. Si tuviéramos una manera de estimarla, podríamos escoger los que maximizan esa probabilidad. Lamentablemente, esto es un problema extremadamente difícil.\nLo mejor que podemos hacer es ajustar el modelo para una serie de datos, medir el desempeño en estos y usar esto para ajustar un modelo de esa distribución. Luego se escogen nuevos hiperparámetros a partir de esa distribución estimada y se itera. Esto se hace bien sea con estrategias bayesianas, árboles de deisión, entre otros.\nOtra forma de explorar el espacio de parámetros es el hiberbanda. En este, ajustamos muchos modelos por muy pocas épocas. Luego se escogen la fracción \\(\\eta\\) de los mejores entre ellos y se aumenta el número de epocas por \\(1/\\eta\\), y se itera.\nExisten librerías que implementan ambos métodos.",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "13. Evaluación y Regularización de redes neuronales"
    ]
  },
  {
    "objectID": "13_evaluacion_y_regularizacion_de_nn.html#regularización-explícita",
    "href": "13_evaluacion_y_regularizacion_de_nn.html#regularización-explícita",
    "title": "Evaluación de modelos y regularización",
    "section": "Regularización explícita",
    "text": "Regularización explícita\nLa regularización explícita consiste en agregarle términos a la función de pérdida que penalicen los parámetros que se salen de un cierto rango. Por ejemplo, vimos la regularización L2. En redes neuronales, es frecuente usar la pérdida modificadoa \\[\n\\mathcal{J} + \\lambda \\sum_{ij} W_{ij}^2\\,.\n\\] Esto hace que la red prefiera pesos pequeños. Es decir, el modelo tiene un rango restringido para los pesos. Al reducir su libertad, ayudamos a que no haga sobreajuste.\nMatemáticamente la regularización se puede ver como una probabilidad previa para los parámetros. Es decir, nuestra verosimilitud ahora consiste en \\(e^{-\\mathcal{J}}P(W)\\), donde \\(P(W)\\) es la distribución previa de los pesos (gaussiana en el caso de L2).",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "13. Evaluación y Regularización de redes neuronales"
    ]
  },
  {
    "objectID": "13_evaluacion_y_regularizacion_de_nn.html#regularización-implícita",
    "href": "13_evaluacion_y_regularizacion_de_nn.html#regularización-implícita",
    "title": "Evaluación de modelos y regularización",
    "section": "Regularización implícita",
    "text": "Regularización implícita\nPara las redes neuronales se cree que además hay una regularización implícita. Es decir, la red aprende a regularizar los pesos en el sentido discutido en la sección de doble descenso. Además, el método de descenso de gradiente también contribuye a esta regularización.\n\nRegularización por descenso de gradiente\nRecordemos que el descenso de gradiente consiste en seguir la dirección de máximo descenso en el espacio de parámetros. Estrictamente esto es resolver la ecuación diferencial \\[\n\\frac{d \\theta_i}{dt} = -\\nabla_i \\mathcal{J}\\,,\n\\] donde \\(\\theta_i\\) son los parámetros. Esto se aproxima actualizando los parámetros haciendo \\(\\theta_{i+1} = \\theta_i - \\eta\\nabla_i \\mathcal{J}\\) (corresponde al método de Euler). Pero resulta que esa aproximación tiene un error tal que no se sigue la trayectoria exactamente. Demostramos más abajo que la trayectoria minimiza la pérdida modificada \\[\n\\tilde{\\mathcal{J}} = \\mathcal{J} + \\frac{\\eta}{4}\\left|\\nabla_\\theta \\mathcal{J}\\right|^2 + \\frac{\\eta}{4B}\\sum_{i=1}^B\\left|\\nabla_\\theta\\mathcal{J}_b - \\nabla_\\theta\\mathcal{J}\\right|^2\\,,\n\\tag{1}\\] donde \\(B\\) es el número de lotes, y \\(\\mathcal{J}_b\\) es la pérdida para el lote \\(b\\). Entonces la trayectoria verdadera se aleja de las regiones con gradiente grande, prefiriendo trayectorias más planas. Además va a preferir parámetros estables, que dan pérdidas similares para los diferentes lotes (es decir sobre conjuntos distintos de datos), reduciendo la varianza.\nTambién aprendemos que a mayor tasa de aprendizaje, mayor esta regularización. Así mismo, para números pequeños de elementos por lote mayor la regularización. Esto se ha observado empíricamente: Cuando la memoria aumentó mucho, aumentar el número de elementos por mini-lote no aportaba la ganancia esperada tal vez porque se perdía esta regularización.\n\n\n\n\n\n\nDeducción de la Ecuación 1\n\n\n\n\n\nAsumamos que realizamos el descenso de gradiente sin mini-lotes. El descenso de gradiente es sólo la aproximación de Euler de la trayectoria que sigue el gradiente. Busquemos la ecuación diferencial que describe la trayectoria seguida por el descenso de gradiente aproximado. Es decir, escribimos\n\\[\n\\frac{d}{dt} \\boldsymbol{\\theta} = -\\nabla_\\boldsymbol{\\theta}\\mathcal{J} + \\eta \\boldsymbol{g}\\,\n\\]\ndonde \\(\\boldsymbol{g}\\) es un vector tal que la trayectoria seguida por esta ecuación es igual a la aproximación discreta de Euler, y \\(\\eta\\) es el tamaño de paso que controla el error de aproximación. Ahora usamos la aproximación de Taylor para calcular\n\\[\n\\boldsymbol{\\theta}(t+\\eta) = \\boldsymbol{\\theta}(t) + \\eta \\frac{d}{dt}\\boldsymbol{\\theta}(t) + \\frac{1}{2}\\eta^2 \\frac{d^2}{dt^2}\\boldsymbol{\\theta}(t) + \\mathcal{O}(\\eta^3)\\,.\n\\]\nAhora reemplazamos la ecuación diferencial en la ecuación anterior:\n\\[\n\\boldsymbol{\\theta}(t+\\eta) = \\boldsymbol{\\theta}(t) + \\eta \\left(-\\nabla_\\boldsymbol{\\theta}\\mathcal{J} + \\eta\\boldsymbol{g}\\right) + \\frac{1}{2}\\eta^2 \\frac{d}{dt}\\left(-\\nabla_\\boldsymbol{\\theta}\\mathcal{J} + \\eta\\boldsymbol{g}\\right) + \\mathcal{O}(\\eta^3)\\,.\n\\]\nDesarrollando el término de segundo orden:\n\\[\n\\boldsymbol{\\theta}(t+\\eta) = \\boldsymbol{\\theta}(t) - \\eta \\nabla_\\boldsymbol{\\theta}\\mathcal{J} + \\eta^2 \\left(- \\frac{1}{2} \\frac{d}{dt}\\nabla_\\boldsymbol{\\theta}\\mathcal{J} + \\boldsymbol{g}\\right) + \\mathcal{O}(\\eta^3)\\,.\n\\]\nDesarrollando la derivada temporal del gradiente de la pérdida:\n\\[\n\\begin{align}\n\\boldsymbol{\\theta}(t+\\eta) &= \\boldsymbol{\\theta}(t) - \\eta \\nabla_\\boldsymbol{\\theta}\\mathcal{J} + \\eta^2 \\left(- \\frac{1}{2} \\frac{d}{dt}\\boldsymbol{\\theta}\\cdot\\nabla_\\boldsymbol{\\theta}\\left(\\nabla_\\boldsymbol{\\theta}\\mathcal{J}\\right) + \\boldsymbol{g}\\right) + \\mathcal{O}(\\eta^3) \\\\\n&= \\boldsymbol{\\theta}(t) - \\eta \\nabla_\\boldsymbol{\\theta}\\mathcal{J} + \\eta^2 \\left(\\frac{1}{2} \\nabla_{\\boldsymbol{\\theta}}\\mathcal{J}\\cdot\\nabla_\\boldsymbol{\\theta}\\left(\\nabla_\\boldsymbol{\\theta}\\mathcal{J}\\right) + \\boldsymbol{g}\\right) + \\mathcal{O}(\\eta^3)\\,,\n\\end{align}\n\\]\nNote que los primeros dos términos son iguales a la aproximación usada para el descenso de gradiente. Entonces, para que la trayectoria seguida por la ecuación diferencial sea la misma que la trayectoria seguida por el descenso de gradiente, debemos tener\n\\[\n\\boldsymbol{g} = -\\frac{1}{2}\\nabla_\\boldsymbol{\\theta}\\mathcal{J}\\cdot\\nabla_\\boldsymbol{\\theta}\\left(\\nabla_{\\boldsymbol{\\theta}}\\mathcal{J}\\right)\\,.\n\\]\nEntonces la trayectoria del descenso de gradiente está bien descrita por la ecuación diferencial\n\\[\n\\frac{d}{dt}\\boldsymbol{\\theta} = -\\nabla_\\boldsymbol{\\theta}\\mathcal{J} - \\frac{\\eta}{2}\\nabla_\\boldsymbol{\\theta}\\mathcal{J}\\cdot\\nabla_\\boldsymbol{\\theta}\\left(\\nabla_{\\boldsymbol{\\theta}}\\mathcal{J}\\right) = -\\nabla_\\boldsymbol{\\theta}\\left[\\mathcal{J} + \\frac{\\eta}{4}\\left|\\nabla_\\boldsymbol{\\theta}\\mathcal{J}\\right|^2\\right]\\,.\n\\]\nEs decir, la verdadera trayectoria del descenso de gradiente aproximado de forma discreta (por pasos) es la trayectoria que minimiza la pérdida modificada por el segundo término dentro del paréntesis cuadrado, que aparece en la Ecuación 1.\nFinalmente, si entrenamos usando mini-lotes, esto introducirá una varianza en la estimación del gradiente de la pérdida. Podemos aproximar esa varianza por medio del último término de la Ecuación 1.",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "13. Evaluación y Regularización de redes neuronales"
    ]
  },
  {
    "objectID": "13_evaluacion_y_regularizacion_de_nn.html#métodos-para-reducir-el-error-de-generalización",
    "href": "13_evaluacion_y_regularizacion_de_nn.html#métodos-para-reducir-el-error-de-generalización",
    "title": "Evaluación de modelos y regularización",
    "section": "Métodos para reducir el error de generalización",
    "text": "Métodos para reducir el error de generalización\nEl término regularización se ha extendido para referirse a técnicas que reducen el error de generalización. Algunas de las más usadas son:\nParado temprano (early stopping)\nEste método consiste en detener el entrenamiento antes de que el modelo comience a sobreajustarse. Podemos usar un conjunto de validación para monitorear el rendimiento del modelo, es decir, medimos la pérdida en el conjunto de validación como aproximación al error de generalización. La pérdida sobre el conjunto de entrenamiento seguirá disminuyendo durante el entrenamiento, mientras que la pérdida sobre el conjunto de validación comenzará a aumentar cuando el modelo comienza a sobreajustarse o permanecerá constante cuando el modelo no está ganando más información. Para evitar ese sobreajuste, podemos detener el entrenamiento cuando observamos que la pérdida sobre el conjunto de validación comienza a aumentar o deja de mejorar.\nEnsamblado\nEsta técnica implica combinar múltiples modelos para hacer predicciones. Cada modelo se entrena de manera independiente y luego sus predicciones se combinan, generalmente mediante promedio o votación. Esto reduce la varianza y mejora la generalización, ya que los errores de los modelos individuales tienden a cancelarse entre sí. Esto lo vimos cuando estudiamo árboles de decisión. Como se discutió en esa clase, las principales técnicas son bagging y boosting.\nAumentación de datos\nEste método consiste en aumentar artificialmente el tamaño del conjunto de entrenamiento mediante la creación de nuevas muestras a partir de las existentes. Un ejemplo de esto en el caso de clasificación de imágenes es aplicando transformaciones a las imágenes del conjunto de entrenamiento que preservan la etiqueta, como rotaciones, traslaciones o cambios de escala. Ayuda a reducir el sobreajuste al exponer el modelo a una mayor variedad de ejemplos.\nDropout\nEsta técnica consiste en desactivar aleatoriamente un cierto porcentaje de neuronas durante cada iteración del entrenamiento. Esto fuerza a la red a aprender características más robustas que no dependan de neuronas específicas, reduciendo así el sobreajuste y mejorando la generalización. Es decir, durante cada época, un porcentaje de las neuronas se desactivan aleatoriamente. El conjunto que se desactiva cambia de una época a otra. Esto hace que el modelo aprenda características que son robustas y por lo tanto reduce la varianza. Intutitivamente es como promediar sobre muchos modelos.\nAgregar ruido a los pesos\nEste método implica añadir pequeñas perturbaciones aleatorias a los pesos de la red durante el entrenamiento. Esto puede ayudar a que el modelo sea más robusto en el sentido que un pequeño cambio en los pesos no afecte mucho la predicción. Esto reduce la varianza y por ende el sobreajuste.\nTransfer learning y multi-task learning\nEl transfer learning consiste en utilizar un modelo pre-entrenado en una tarea similar y ajustarlo para la tarea específica, aprovechando así el conocimiento adquirido previamente. El multi-task learning implica entrenar un modelo para realizar múltiples tareas simultáneamente, lo que puede mejorar la generalización al forzar al modelo a aprender representaciones más generales y robustas. Veremos ejemplos de esto en clases posteriores.",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "13. Evaluación y Regularización de redes neuronales"
    ]
  },
  {
    "objectID": "10_svm_y_no_supervisado.html",
    "href": "10_svm_y_no_supervisado.html",
    "title": "SVM y Aprendizaje No Supervisado",
    "section": "",
    "text": "En esta clase vamos a ver otros algoritmos de aprendizaje automático que no son redes neuronales. Existe una vasta (y creciente) literatura sobre este tema y no vamos a poder cubrirla en esta asignatura. El estudiante puede tomar esta clase como motivación para explorar más a fondo el aprendizaje automático.",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "10. SVM y Aprendizaje No Supervisado"
    ]
  },
  {
    "objectID": "10_svm_y_no_supervisado.html#clasificador-de-margen-máximo",
    "href": "10_svm_y_no_supervisado.html#clasificador-de-margen-máximo",
    "title": "SVM y Aprendizaje No Supervisado",
    "section": "Clasificador de Margen Máximo",
    "text": "Clasificador de Margen Máximo\nEste clasificador busca un hiperplano que separa las observaciones de las dos clases de manera que el margen (distancia mínima) entre el hiperplano y las observaciones más cercanas de cada clase sea lo más grande posible. Es decir, para cada punto de datos calculamos su distancia al hiperplano y el margen es la distancia mínima entre el hiperplano y las observaciones más cercanas de cada clase.\nLa optimización del hiperplano de margen máximo es:\n\\[\n\\text{maximizar } M \\quad\\text{sujeto a}\\quad \\sum_{j=1}^{p} \\beta_j^2 = 1, \\quad y_i(\\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip}) \\geq M\\,,\n\\]\ndonde \\(M\\) es el margen y \\(y_i\\) es la clase de la observación \\(i\\) (\\(y_i \\in \\{-1, 1\\}\\)).\n\n\nCódigo\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\n\n# ---------------------------\n# 1. Datos sintéticos\n# ---------------------------\nnp.random.seed(42)\nX_pos = np.random.randn(20, 2) + [2, 2]     # clase +1\nX_neg = np.random.randn(20, 2) + [-2, -2]   # clase −1\nX = np.vstack([X_pos, X_neg])\ny = np.hstack([np.ones(20), -np.ones(20)])\n\n# ---------------------------\n# 2. Entrenamiento SVM lineal\n#    (margen máximo: C grande)\n# ---------------------------\nclf = svm.SVC(kernel=\"linear\", C=1e5)  # C → ∞ ≈ margen duro\nclf.fit(X, y)\n\nw = clf.coef_[0]          # vector normal al hiperplano\nb = clf.intercept_[0]     # término independiente\nsv = clf.support_vectors_ # vectores soporte\n\n# ---------------------------\n# 3. Gráfica\n# ---------------------------\nfig, ax = plt.subplots(figsize=(6, 6))\n\n# Puntos\nax.scatter(X_pos[:, 0], X_pos[:, 1], color=\"steelblue\", label=\"Clase +1\")\nax.scatter(X_neg[:, 0], X_neg[:, 1], color=\"darkorange\", label=\"Clase −1\")\n\n# Vectores soporte\nax.scatter(sv[:, 0], sv[:, 1], s=120, facecolors=\"none\",\n           edgecolors=\"black\", linewidths=1.5, label=\"Vectores soporte\")\n\n# Recta de decisión y márgenes\nxx = np.linspace(*ax.get_xlim(), 400)\nyy = -(w[0] * xx + b) / w[1]\nmargin = 1 / np.linalg.norm(w)\nyy_up   = yy + w[0] / w[1] * margin\nyy_down = yy - w[0] / w[1] * margin\n\nax.plot(xx, yy, \"k-\", linewidth=2)\nax.plot(xx, yy_up, \"k--\", linewidth=1)\nax.plot(xx, yy_down, \"k--\", linewidth=1)\n\nax.set_xlabel(\"$x_1$\")\nax.set_ylabel(\"$x_2$\")\nax.set_title(\"Clasificador de margen máximo\")\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigura 1: Clasificador de margen máximo\n\n\n\n\n\nEn la Figura 1 se muestra el clasificador de margen máximo para un conjunto de datos sintéticos. Encerramos en círculos los puntos con la distancia mínima al hiperplano. Estos se llaman vectores de soporte. Los vectores soporte son las observaciones más cercanas al hiperplano y determinan su posición. Una particularidad de los vectores soporte es que si eliminamos todos los demás puntos, el hiperplano no cambia. Es decir, el hiperplano es invariante a la eliminación de puntos no soporte.\nEl clasificador de margen máximo tiene una gran varianza porque la frontera de decisión es muy sensible a pequeños cambios en los vectores de soporte.",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "10. SVM y Aprendizaje No Supervisado"
    ]
  },
  {
    "objectID": "10_svm_y_no_supervisado.html#clasificador-de-vectores-soporte",
    "href": "10_svm_y_no_supervisado.html#clasificador-de-vectores-soporte",
    "title": "SVM y Aprendizaje No Supervisado",
    "section": "Clasificador de Vectores Soporte",
    "text": "Clasificador de Vectores Soporte\nEl clasificador de vectores soporte (clasificador de margen suave) permite algunas violaciones del margen. Para ello, se introduce un parámetro \\(C\\) que controla la compensación entre el ancho del margen y las violaciones del margen. Se puede ver como un presupuesto para el número de violaciones del margen.\n\\[\n\\begin{align}\n\\text{maximizar } M \\quad\\text{sujeto a} &\\\\\n&\\sum_{j=1}^{p} \\beta_j^2 = 1, \\\\\n&y_i(\\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip}) \\geq M(1 - \\varepsilon_i), \\\\\n&\\varepsilon_i \\geq 0,\\quad \\sum_{i=1}^{n} \\varepsilon_i \\leq C\n\\end{align}\n\\]\nEn la Figura 2 se muestra el clasificador de margen suave para un conjunto de datos sintéticos. Vemos que el margen es más ancho y permite algunas violaciones del margen.\n\n\nCódigo\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\n\n# ---------------------------\n# 1. Datos sintéticos con solapamiento\n# ---------------------------\nnp.random.seed(42)\nX_pos = np.random.randn(20, 2) + [1, 1]     # clase +1\nX_neg = np.random.randn(20, 2) + [-1, -1]   # clase −1\nX = np.vstack([X_pos, X_neg])\ny = np.hstack([np.ones(20), -np.ones(20)])\n\n# ---------------------------\n# 2. Entrenamiento SVM con margen suave\n#    (C pequeño permite más violaciones)\n#    CUIDADO: El parámetro C de esta clase es el inverso del \n#    introducido en la ecuación (ver advertencia)\n# ---------------------------\nclf = svm.SVC(kernel=\"linear\", C=0.1)  # C pequeño = margen suave\nclf.fit(X, y)\n\nw = clf.coef_[0]          # vector normal al hiperplano\nb = clf.intercept_[0]     # término independiente\nsv = clf.support_vectors_ # vectores soporte\n\n# ---------------------------\n# 3. Gráfica\n# ---------------------------\nfig, ax = plt.subplots(figsize=(6, 6))\n\n# Puntos\nax.scatter(X_pos[:, 0], X_pos[:, 1], color=\"steelblue\", label=\"Clase +1\")\nax.scatter(X_neg[:, 0], X_neg[:, 1], color=\"darkorange\", label=\"Clase −1\")\n\n# Vectores soporte\nax.scatter(sv[:, 0], sv[:, 1], s=120, facecolors=\"none\",\n           edgecolors=\"black\", linewidths=1.5, label=\"Vectores soporte\")\n\n# Recta de decisión y márgenes\nxx = np.linspace(*ax.get_xlim(), 400)\nyy = -(w[0] * xx + b) / w[1]\nmargin = 1 / np.linalg.norm(w)\nyy_up   = yy + w[0] / w[1] * margin\nyy_down = yy - w[0] / w[1] * margin\n\nax.plot(xx, yy, \"k-\", linewidth=2)\nax.plot(xx, yy_up, \"k--\", linewidth=1)\nax.plot(xx, yy_down, \"k--\", linewidth=1)\n\n# Resaltar puntos que violan el margen\nfor i in range(len(X)):\n    if clf.decision_function([X[i]])[0] * y[i] &lt; 1:\n        ax.scatter(X[i, 0], X[i, 1], s=100, facecolors=\"none\",\n                  edgecolors=\"red\", linewidths=2)\n\nax.set_xlabel(\"$x_1$\")\nax.set_ylabel(\"$x_2$\")\nax.set_title(\"Clasificador de margen suave (C=0.1)\")\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigura 2: Clasificador de margen suave con violaciones\n\n\n\n\n\nEl parámetro \\(C\\) controla la compensación entre sesgo y varianza del modelo.\n\nSi \\(C\\) es pequeño, cada vez que un punto se mete en la calle (o queda mal clasificado) el costo en la función objetivo es alto; el algoritmo prefiere reducir el margen y mover la recta para sacar a todos los puntos de la calle. El modelo se ajusta muy bien a los datos de entrenamiento (bajo sesgo) pero pequeños cambios en los datos alteran mucho la frontera (alta varianza).\nSi \\(C\\) es grande, las violaciones casi no “cuentan”: el optimizador puede aceptar que algunos puntos queden dentro o incluso al otro lado de la calle con tal de ensanchar el margen. Esto actúa como regularización: la frontera depende menos de detalles locales (menor varianza), pero al ser más rígida puede no capturar complejidades reales del dato (mayor sesgo).\n\nEn otras palabras, \\(C\\) controla cuán caro es equivocarse: caro = modelo flexible que busca cero errores; barato = modelo moderado que sacrifica exactitud para ganar robustez.\n\n\n\n\n\n\nAdvertencia\n\n\n\nEn scikit-learn, el parámetro C está definido de manera inversa a como lo hemos definido en estas notas. En scikit-learn, un valor grande de C corresponde a un margen más estrecho y menos violaciones, mientras que un valor pequeño de C corresponde a un margen más ancho y más violaciones. Es decir, el C de scikit-learn es aproximadamente inversamente proporcional al \\(C\\) definido en estas notas.",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "10. SVM y Aprendizaje No Supervisado"
    ]
  },
  {
    "objectID": "10_svm_y_no_supervisado.html#máquinas-de-vectores-de-soporte-svm-no-lineales",
    "href": "10_svm_y_no_supervisado.html#máquinas-de-vectores-de-soporte-svm-no-lineales",
    "title": "SVM y Aprendizaje No Supervisado",
    "section": "Máquinas de Vectores de Soporte (SVM) no lineales",
    "text": "Máquinas de Vectores de Soporte (SVM) no lineales\nLas Máquinas de Vectores de Soporte (SVM) extienden el clasificador de vectores soporte usando núcleos (kernels) para manejar problemas de clasificación no lineales.\nPara límites no lineales, el espacio de características se amplía con funciones de los predictores (por ejemplo, términos cuadráticos).\n\n\nCódigo\nfrom sklearn.datasets import make_moons\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\nfrom matplotlib.colors import ListedColormap\n\nX, y  = make_moons(n_samples=100, noise=0.15, random_state=42)\n\npolynomial_svm_clf = make_pipeline(\n    PolynomialFeatures(degree=3),\n    StandardScaler(),\n    LinearSVC(C=10, max_iter=10_000, random_state=42)\n)\npolynomial_svm_clf.fit(X, y)\n\nplt.scatter(X[y==0,0], X[y==0,1])\nplt.scatter(X[y==1,0], X[y==1,1])\n\n# Create a mesh grid for plotting the decision boundary\nx_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\ny_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n                     np.arange(y_min, y_max, 0.01))\n\n# Predict the class of each point on the mesh grid\nZ = polynomial_svm_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n# Reshape Z to a 2D array\nZ = Z.reshape(xx.shape)\n\n# Create a colormap for plotting the decision boundary\ncmap = ListedColormap(['red', 'green'])\n\n# Plot the decision boundary\nplt.contourf(xx, yy, Z, cmap=cmap, alpha=0.2)\n\n\n\n\n\n\n\n\nFigura 3: SVM no lineal\n\n\n\n\n\nEn la Figura 3 se muestra el límite de decisión de una SVM con un núcleo polinomial de grado 3. Esto agrega términos cuadráticos y cúbicos a la frontera de decisión. En dos dimensiones, esto funciona bien, pero en dimensiones más altas el número de términos polinomiales crece muy rápidamente. Por ejemplo, en 10 dimensiones, hay \\(\\binom{10}{2} = 45\\) términos cuadráticos y \\(\\binom{10}{3} = 240\\) términos cúbicos.\n\nTruco del núcleo (kernel trick)\nEl truco del núcleo es una técnica matemática que permite a las Máquinas de Vectores de Soporte trabajar eficientemente en espacios de alta dimensión sin tener que calcular explícitamente las transformaciones de los datos. En lugar de transformar los datos a un espacio de mayor dimensión y luego calcular el producto punto (lo cual sería computacionalmente costoso), el truco del núcleo permite calcular directamente el producto punto en el espacio transformado.\nLa idea clave es que las Máquinas de Vectores de Soporte solo necesitan calcular productos punto entre los datos. El truco del núcleo permite calcular estos productos punto en el espacio transformado sin tener que conocer la transformación explícita.\nPor ejemplo, consideremos el núcleo polinómico de grado 2:\n\\[\nK(x, z) = (1 + x^T z)^2\n\\]\nPara datos en \\(\\mathbb{R}^2\\), este núcleo corresponde a una transformación \\(\\phi\\) que mapea los datos a un espacio de 6 dimensiones:\n\\[\n\\phi(x) = (1, \\sqrt{2}x_1, \\sqrt{2}x_2, x_1^2, x_2^2, \\sqrt{2}x_1x_2)\n\\]\nEl truco del núcleo permite calcular \\(K(x, z) = \\phi(x)^T \\phi(z)\\) sin tener que calcular explícitamente \\(\\phi(x)\\) y \\(\\phi(z)\\). Esto es especialmente útil cuando la dimensión del espacio transformado es muy grande o incluso infinita.\nLos núcleos más comunes son:\n\nNúcleo lineal:\n\\[\nK(x_i, x_{i'}) = \\sum_{j=1}^{p} x_{ij} x_{i'j}\n\\]\nNúcleo polinómico (grado \\(d\\)):\n\\[\nK(x_i, x_{i'}) = \\left(1 + \\sum_{j=1}^{p} x_{ij} x_{i'j}\\right)^d\n\\]\nNúcleo radial (RBF): \\[\nK(x_i, x_{i'}) = \\exp\\left(-\\gamma \\sum_{j=1}^{p}(x_{ij}-x_{i'j})^2\\right)\n\\]\n\nEl núcleo radial es particularmente interesante porque corresponde a una transformación a un espacio de dimensión infinita, lo que sería imposible de calcular explícitamente. Sin embargo, gracias al truco del núcleo, podemos trabajar con este espacio de manera eficiente. Tiene la forma de una Gaussiana, donde \\(\\gamma\\) controla la varianza de la Gaussiana. Es decir, para \\(\\gamma\\) grande la campana es estrecha y para \\(\\gamma\\) pequeño la campana es más amplia. Cuando la campana es estrecha, cada punto es afectado por puntos cercanos, tal que la influencia de cada punto se vuelve más local. Esto hace que la frontera de decisión sea más ondulada y el modelo sea más flexible pero también más susceptible a sobreajuste. Por el contrario, cuando la campana es más amplia, cada punto es afectado por más puntos, lo que hace que la frontera de decisión sea más rígida y menos susceptible a sobreajuste pero con un mayor sesgo.\nLa función de decisión de las Máquinas de Vectores de Soporte con núcleo \\(K\\) es:\n\\[\nf(x) = \\beta_0 + \\sum_{i \\in S} \\alpha_i K(x, x_i)\n\\]\nEsto es equivalente al hiperplano \\(\\beta_0 + \\sum_{j=1}^{p} \\beta_j \\phi_j(x) = 0\\) en el espacio de características transformado.\nComo antes, solo los vectores soporte (observaciones cercanas al margen o que lo violan) afectan el límite de decisión.\nEn pocas palabras, el núcleo o kernel es equivalente a calcular el producto punto en un espacio de dimensión elevada. Este producto se usa en lugar de calcular la transformación explícita. No lo demostramos aquí por falta de tiempo, pero el producto punto es lo único que se necesita para calcular el límite de decisión.\n\n\nCódigo\nfrom sklearn.svm import SVC\n\ndef plot_regions_rbf_example(gamma, C, ax):\n    # Train the classifier\n    rbf_kernel_svm_clf = make_pipeline(StandardScaler(),\n    SVC(kernel=\"rbf\", gamma=gamma, C=C))\n    rbf_kernel_svm_clf.fit(X, y)\n\n    ax.scatter(X[y==0,0], X[y==0,1])\n    ax.scatter(X[y==1,0], X[y==1,1])\n\n    # Create a mesh grid for plotting the decision boundary\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n                         np.arange(y_min, y_max, 0.01))\n\n    # Predict the class of each point on the mesh grid\n    Z = rbf_kernel_svm_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    # Reshape Z to a 2D array\n    Z = Z.reshape(xx.shape)\n\n    # Create a colormap for plotting the decision boundary\n    cmap = ListedColormap(['red', 'green'])\n\n    # Plot the decision boundary\n    ax.contourf(xx, yy, Z, cmap=cmap, alpha=0.2)\n    ax.set_title(f'gamma={gamma}, C={C}')\n\n# Create a 2x2 grid of plots\nfig, axes = plt.subplots(2, 2, figsize=(6, 6))\n\n# Define the parameter combinations\nparams = [\n    (0.1, 0.001),\n    (0.1, 1000),\n    (5, 0.001),\n    (5, 1000)\n]\n\n# Plot each combination\nfor (gamma, C), ax in zip(params, axes.ravel()):\n    plot_regions_rbf_example(gamma, C, ax)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigura 4: SVM con núcleo radial\n\n\n\n\n\nEn la Figura 4 se muestra el límite de decisión de un SVM con núcleo radial para diferentes valores de \\(C\\) y \\(\\gamma\\). Para valores pequeños de \\(C\\) se permiten muchas violaciones del margen. Para valores grandes de \\(C\\) se permiten pocas violaciones del margen pero se puede terminar en sobreajuste. Al aumentar \\(\\gamma\\) la campana Gaussiana se hace más estrecha. La influencia de cada instancia se vuelve más local, lo que lleva a una frontera más ondulada. Si el modelo sobreajusta, reduce \\(\\gamma\\) y viceversa.\n\n\nSVM multiclase\nLas SVM definen una frontera de decisión para separar dos clases. Cuando tenemos más de dos clases, podemos intentar las siguientes estrategias:\n\nUno contra uno: Para cada pareja de clases se construye una SVM. En total se construyen \\(\\binom{K}{2}\\) SVM para clasificación con \\(K\\) clases. Cada punto se asigna a la clase que ocurre más veces en esas comparaciones. Cada SVM usa pocos puntos ya que solo se usan las observaciones de las dos clases que se están separando. Sin embargo, el número de SVMs crece rápidamente con \\(K\\), lo que puede llevar a predicciones lentas (necesita clasificar muchas veces y tomar la más frecuente).\nUno contra todos: Para cada clase, se construye una SVM que determina la frontera entre esa clase y todas las demás. Se construyen \\(K\\) SVM, cada una frente a todas las demás. Cada punto se asigna a la clase que ocurre más veces en esas comparaciones. Sin embargo puede tener problemas cuando alguna clase se solapa con otras varias.",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "10. SVM y Aprendizaje No Supervisado"
    ]
  },
  {
    "objectID": "10_svm_y_no_supervisado.html#análisis-de-componentes-principales-pca",
    "href": "10_svm_y_no_supervisado.html#análisis-de-componentes-principales-pca",
    "title": "SVM y Aprendizaje No Supervisado",
    "section": "Análisis de componentes principales (PCA)",
    "text": "Análisis de componentes principales (PCA)\nEl análisis de componentes principales (PCA) es una técnica de aprendizaje no supervisado utilizada para simplificar conjuntos de datos correlacionados de alta dimensión, reduciendo su dimensionalidad mientras se conserva la mayor cantidad posible de varianza. PCA identifica las direcciones (componentes principales) a lo largo de las cuales los datos varían más y proyecta los datos originales sobre estas direcciones.\n\nDefinición de los Componentes Principales\nLa intuición es la siguiente: La matriz de covarianza contiene la varianza de cada variable y su correlación con otras variables. Es una matriz simétrica, es decir, \\(\\boldsymbol{\\Sigma}_{ij} = \\boldsymbol{\\Sigma}_{ji}\\). Por lo tanto, siempre es posible diagonalizarla. Esto quiere decir que podemos encontrar autovectores y autovalores de la matriz de covarianza. Estos autovectores son ortogonales entre ellos, en otras palabras, si cambianos la lista de variables \\(\\{x_1, ..., x_q\\}\\) por una lista de autovectores \\(\\{\\phi_1, ..., \\phi_q\\}\\), la matriz de covarianza de estos será diagonal. Esto es equivalente a cambiar de variables en el espacio de parámetros para obtener nuevas variables no correlacionadas.\nLos componentes principales se definen a través de los autovectores de la matriz de covarianza. En los datos esta es \\(\\mathbf{X}^T\\mathbf{X}\\), donde la componente \\(X_{ij}\\) como siempre se refiere a la medición \\(i\\), variable \\(j\\) (cada fila es una medición y cada columna es una variable).\nEl primer vector de componentes principales, \\(\\boldsymbol{\\phi}_1\\), es el autovector de \\(\\mathbf{X}^T\\mathbf{X}\\) asociado al mayor autovalor. Representa la dirección en el espacio de características a lo largo de la cual los datos varían más. Es decir, la dirección en la que hay más información.\nFormalmente, el primer componente principal se expresa como:\n\\[\nZ_1 = \\mathbf{X}\\boldsymbol{\\phi}_1\n\\]\ndonde \\(\\boldsymbol{\\phi}_1\\) satisface:\n\\[\n\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\phi}_1 = \\lambda_1\\boldsymbol{\\phi}_1\n\\]\nsiendo \\(\\lambda_1\\) el mayor autovalor. Al ser un autovector, \\(\\boldsymbol{\\phi}_1\\) está definido salvo una constante multiplicativa.\nEl siguiente componente principal \\(\\boldsymbol{\\phi}_2\\) es el autovector asociado al siguiente autovalor más grande, y así sucesivamente. El autovalor de la matriz de covarianza representa la varianza de los datos en esa dirección. Escogemos los \\(m\\) autovectores de mayor autovalor para que los componentes principales expliquen la mayor variabilidad posible de los datos.\nLos autovectores de una matriz simétrica son ortogonales entre sí, es decir, \\(\\boldsymbol{\\phi}_i^T\\boldsymbol{\\phi}_j = 0\\) para \\(i \\neq j\\). Por lo tanto, los componentes principales no están correlacionados entre sí.\nEn este sentido, el PCA con \\(m\\) componentes busca las \\(m\\) direcciones no correlacionadas que explican la mayor variabilidad posible de los datos.\n\n\nProporción de Varianza Explicada (PVE)\nLa proporción de varianza explicada por cada componente principal está dada por:\n\\[\n\\text{PVE}_m = \\frac{\\sum_{i=1}^{n} z_{im}^2}{\\sum_{j=1}^{p}\\sum_{i=1}^{n} x_{ij}^2}\n\\]\nLa PVE ayuda a determinar cuántos componentes representan suficientemente los datos. Típicamente, se seleccionan componentes hasta que se observa un “codo” en el gráfico de scree.\n\n\nConsideraciones Prácticas\n\nEscalado de las Variables\nGeneralmente, las variables requieren ser escaladas a varianza unitaria, especialmente si están medidas en distintas escalas, para asegurar que ninguna variable influya desproporcionadamente en el PCA.\n\n\nUnicidad\nLos componentes principales son únicos salvo un cambio de signo. Por lo tanto, los resultados de diferentes paquetes de software pueden diferir en los signos pero representarán las mismas direcciones en el espacio de características.\n\n\n\nSelección del Número de Componentes\nDeterminar el número de componentes principales es generalmente subjetivo. Prácticas comunes incluyen:\n\nInspeccionar gráficos de scree para encontrar un “codo”. Es decir, graficamos \\(\\text{PVE}_i\\) para \\(i=1,\\dots,q\\). La cantidad de varianza explicada subirá a medida que agregamos más componentes, pero llegará un punto donde ganaremos poco al agregar el siguiente. A esto se lo llama el “codo”.\nElegir suficientes componentes para capturar una proporción acumulada significativa de varianza. Por ejemplo, si queremos capturar el 95% de la varianza, usamos \\(\\sum_{i=1}^{m} \\text{PVE}_i \\geq 0.95\\).\n\nEn contextos supervisados, el número de componentes puede seleccionarse mediante validación cruzada.\n\n\nUsos Adicionales del PCA\nMás allá de la visualización y la reducción de dimensionalidad, PCA puede utilizarse para:\n\nPreprocesamiento de datos para regresión o clasificación.\nImputación de datos faltantes, es decir, completar valores faltantes en la matrix de datos \\(\\mathbf{X}\\).\nMejorar la estabilidad y reducir el ruido en varios métodos analíticos.\n\n\n\nEjemplo de PCA para compresión de imágenes\nEn este ejemplo, usamos PCA para comprimir una imagen de un dígito escrito a mano. Usamos los datos de dígitos del paquete sklearn.\n\n\nCódigo\nfrom sklearn.datasets import fetch_openml\n\nmnist = fetch_openml('mnist_784', as_frame=False)\nX_train, y_train = mnist.data[:60_000], mnist.target[:60_000]\nX_test, y_test = mnist.data[60_000:], mnist.target[60_000:]\n\nindexes = np.random.random_integers(0, len(X_train), 9)\nsample = X_train[indexes].reshape(3,3,28,28)\nplt.imshow(np.block([[sample[i,j] for i in range(3)] for j in range(3)]), cmap='gray_r')\n\n\n/tmp/ipykernel_113981/122722974.py:7: DeprecationWarning: This function is deprecated. Please call randint(0, 60000 + 1) instead\n  indexes = np.random.random_integers(0, len(X_train), 9)\n\n\n\n\n\n\n\n\nFigura 5: Dígitos de MNIST\n\n\n\n\n\nEn scikit-learn, el PCA se puede aplicar con la clase PCA del módulo decomposition. Podemos especificar que el número de componentes principales explique una proporción de la varianza. Pediremos el 95% de la varianza.\n\n\nCódigo\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=0.95)\nX_reduced = pca.fit_transform(X_train)\n\nX_recovered = pca.inverse_transform(X_reduced)\nsample = X_recovered[indexes].reshape(3,3,28,28)\nplt.imshow(np.block([[sample[i,j] for i in range(3)] for j in range(3)]), cmap='gray_r')\n\n\n\n\n\n\n\n\nFigura 6: PCA para compresión de imágenes",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "10. SVM y Aprendizaje No Supervisado"
    ]
  },
  {
    "objectID": "10_svm_y_no_supervisado.html#métodos-de-clustering",
    "href": "10_svm_y_no_supervisado.html#métodos-de-clustering",
    "title": "SVM y Aprendizaje No Supervisado",
    "section": "Métodos de Clustering",
    "text": "Métodos de Clustering\n\nIntroducción\nEl clustering es un conjunto amplio de técnicas para encontrar subgrupos (clusters) en un conjunto de datos. El objetivo es dividir las observaciones en grupos distintos de tal manera que las observaciones dentro de un mismo grupo sean similares entre sí y diferentes a las de otros grupos. La definición de similitud o diferencia depende del contexto y del tipo específico de datos.\nAunque tanto el clustering como PCA buscan simplificar datos, sus mecanismos son diferentes:\n\nPCA busca una representación de baja dimensión que explique gran parte de la varianza.\nClustering busca subgrupos homogéneos entre las observaciones.\n\n\n\nClustering K-means\nEl clustering K-means es un método sencillo para particionar un conjunto de datos en \\(K\\) clusters distintos y no superpuestos. Es necesario especificar previamente el número \\(K\\) de clusters.\nDado un conjunto de observaciones $ x_{ij} $, se busca minimizar la variación intra-cluster:\n\\[\n\\min_{C_1, \\dots, C_K} \\sum_{k=1}^{K} \\frac{1}{|C_k|}\\sum_{i,i' \\in C_k}\\sum_{j=1}^{p}(x_{ij}-x_{i'j})^2\n\\]\ndonde \\(C_k\\) representa el conjunto de índices de las observaciones en el cluster \\(k\\).\n\nAlgoritmo K-means\n\nAsignar aleatoriamente cada observación a uno de los \\(K\\) clusters.\nIterar hasta que las asignaciones no cambien:\n\nCalcular el centroide de cada cluster (vector promedio).\nReasignar cada observación al cluster cuyo centroide esté más cerca (en términos de distancia euclidiana).\n\n\nEl resultado es una solución óptima local (puede no encontrar la solución óptima global), por lo que conviene repetir el algoritmo múltiples veces desde diferentes inicializaciones aleatorias.\nEl paso 1 es crucial para el resultado final. Si se elige mal, el algoritmo puede converger a un resultado subóptimo. Existen varias estrategias que mejoran la elección inicial. Por ejemplo, se puede elegir el centroide inicial como el punto más alejado de los centroides de los clusters ya existentes, que es lo que hace el algoritmo KMeans++ implementado en sklearn por defecto cuando se usa el método KMeans.\n\n\n\nElección de la medida de disimilitud\nLa distancia euclidiana es común, pero pueden preferirse otras medidas como la basada en correlación, que considera similares aquellas observaciones con perfiles correlacionados, independientemente de sus magnitudes absolutas.\n\n\nCuestiones prácticas\n\nEscalado: Se recomienda escalar las variables a desviación estándar uno, especialmente cuando están medidas en escalas distintas. Esto es porque la distancia es sensible a la escala de las variables.\nRobustez: Los resultados del clustering pueden ser sensibles a decisiones como tipo de vínculo, escalado y selección del número de clusters. Por lo tanto, es recomendable evaluar la robustez del clustering mediante diferentes configuraciones y subconjuntos del conjunto de datos.\n\n\n\nEjemplo de clustering K-means\n\n\nCódigo\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\nk = 5\nX, y = make_blobs(n_samples=500, centers=k, n_features=2, random_state=42)\n# make the blobs: y contains the cluster IDs, but we\n# will not use them; that's what we want to predict\n\nkmeans = KMeans(n_clusters=k, random_state=42)\ny_pred = kmeans.fit_predict(X)\n\nplt.scatter(X[y==0,0], X[y==0,1])\nplt.scatter(X[y==1,0], X[y==1,1])\nplt.scatter(X[y==2,0], X[y==2,1])\nplt.scatter(X[y==3,0], X[y==3,1])\nplt.scatter(X[y==4,0], X[y==4,1])\n\n# Create a mesh grid for plotting the decision boundary\nx_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\ny_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n                     np.arange(y_min, y_max, 0.01))\n\n# Predict the class of each point on the mesh grid\nZ = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n# Reshape Z to a 2D array\nZ = Z.reshape(xx.shape)\n\n# Create a colormap for plotting the decision boundary\n#cmap = ListedColormap(['red', 'green', 'blue', 'yellow', 'gray'])\n\n# Plot the decision boundary\nplt.contourf(xx, yy, Z, cmap='jet', alpha=0.2)\n\n\n\n\n\n\n\n\nFigura 7: Clustering K-means para datos ficticios\n\n\n\n\n\nEn la figura Figura 7 se muestra el resultado del clustering K-means para datos ficticios. Vemos que funciona bien para este caso.\n\n\nEjemplo de clustering para segmentación de imágenes\nAhora queremos segmentar imágenes. Es decir, queremos agrupar píxeles similares entre sí. Esto ocurre frecuentemente en física, donde podemos tener imágenes de un experimento y nos gustaría identificar objetos o regiones. Nuestro ejemplo será reducir los colores de una imagen. Cada píxel tiene un color, que es un vector de tres componentes (rojo, verde, azul). Queremos agrupar píxeles similares entre sí en el espacio de colores.\n\n\nCódigo\nfrom PIL import Image\nfrom pathlib import Path\n\nIMAGES_PATH = Path('data')\nfilename = 'ladybug.png'\nfilepath = IMAGES_PATH / filename\nimage = np.asarray(Image.open(filepath))\n\nplt.imshow(image)\n\n\n\n\n\n\n\n\nFigura 8: Imagen de muestra que queremos segmentar\n\n\n\n\n\nEn la figura Figura 8 se muestra una imagen de muestra que queremos segmentar.\n\n\nCódigo\ndef cluster_image(n_clusters):\n    X = image.reshape(-1, 3) # Convertir en tres matrices, una por color\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(X) # entrenar un clustering k-medias\n    img_segmentada = kmeans.cluster_centers_[kmeans.labels_] # reemplazar cada píxel por su etiqueta de cluster\n    img_segmentada = img_segmentada.reshape(image.shape).astype(int) # convertir de nuevo en una imagen\n\n    plt.imshow(img_segmentada)\n\ncluster_image(10)\n\n\n\n\n\n\n\n\nFigura 9: Imagen segmentada en 10 clusters, coloreada con los centroides de los clusters\n\n\n\n\n\nEn la figura Figura 9 se muestra el resultado del clustering K-means para la imagen de la figura Figura 8. Cada cluster se colorea con el color del centroide, es decir, con el color del punto central del cluster. Hagámoslo para menos clusters\n\n\nCódigo\ncluster_image(5)\n\n\n\n\n\n\n\n\nFigura 10: Imagen segmentada en 5 clusters, coloreada con los centroides de los clusters\n\n\n\n\n\nEn la figura Figura 10 se muestra el resultado del clustering K-means para la imagen de la figura Figura 8 con 5 clusters.",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "10. SVM y Aprendizaje No Supervisado"
    ]
  },
  {
    "objectID": "14_redes_convolucionales.html",
    "href": "14_redes_convolucionales.html",
    "title": "Redes Convolucionales",
    "section": "",
    "text": "Hasta ahora hemos estudiado los perceptrones multicapa, en las cuales las neuronas de una capa están conectadas a todas las neuronas de la capa anterior. Estas redes pueden en principio “aprender” cualquier conexión entre las diferentes capas. Sin embargo, en general un modelo que incorpora más información sobre el problema de interés funciona mejor.\nUn ejemplo famoso es la clasificación de imágenes. Si queremos clasificar un objeto en una imagen la posición de este objeto no es relevante para la clasificación. Así mismo, en el estudio de análisis de imágenes se usan filtros que procesan pixeles cercanos entre sí, lo que permite capturar patrones locales. Las redes convolucionales combinan estas intuiciones.\nEste tipo de redes dio uno de los primeros éxitos de las redes neuronales. Se aplicaron a la clasificación de dígitos escritos por humanos (el problema de la base de datos MNIST), durante los años 90.",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "14. Redes Convolucionales"
    ]
  },
  {
    "objectID": "14_redes_convolucionales.html#convolución-en-una-dimensión",
    "href": "14_redes_convolucionales.html#convolución-en-una-dimensión",
    "title": "Redes Convolucionales",
    "section": "Convolución en una dimensión",
    "text": "Convolución en una dimensión\nUna convolución transforma un vector de entrada \\(\\boldsymbol{x}\\) en un vector de salida \\(\\boldsymbol{z}\\) de la misma dimensión tal que \\[\nz_i = \\sum_{j = -k}^k w_j x_{i+j}\n\\] {#eq:convolucion-1d} para todo \\(i\\). El vector \\(\\boldsymbol{w}\\) se llama kernel (núcleo) o filtro. El rango de valores de \\(j\\) está dado por \\([-k, k]\\), y su tamaño es \\(2k+1\\). Se usan kernels relativamente pequeños, típicamente de tamaño 3 o 5.\nEste tipo de operaciones es muy usada en el análisis de series temporales. Es decir, el elemento \\(x_i\\) del vector \\(\\boldsymbol{x}\\) es la medición de una variable física en el instante \\(t_i\\). Este tipo de filtros y convoluciones ayudan a extraer patrones locales en ese tipo de series o señales.\nEste tipo de convolución se ilustra en la figura 10.2 (a) y (b) del libro. Un ejemplo de un kernel con otro tamaño es el de la figura 10.3 (c) del libro.\nVemos que los pesos \\(w_j\\) son los mismos para todo \\(i\\). Esto hace que la convolución sea invariante bajo traslaciones.\nLa idea de esta operación es que producimos varios filtros de la señal \\(\\boldsymbol{x}\\), cada uno con un kernel diferente. En análisis de señales clásico, se usan filtros específicos para extraer ciertas características. En este caso dejamos libres los pesos \\(w_j\\) para que la red aprenda los filtros que mejor se ajusten al problema.",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "14. Redes Convolucionales"
    ]
  },
  {
    "objectID": "14_redes_convolucionales.html#padding-rellenado",
    "href": "14_redes_convolucionales.html#padding-rellenado",
    "title": "Redes Convolucionales",
    "section": "Padding (Rellenado)",
    "text": "Padding (Rellenado)\nAl ver la @eq:convolucion-1d, notamos que los primeros elementos del vector \\(\\boldsymbol{z}\\) pueden depender de elementos de vector \\(\\boldsymbol{x}\\) que no están definidos. Por ejemplo, \\(z_0\\) depende de \\(x_{-1}\\) que no está definido. Existen varias estrategias para lidiar con este problema:\n\nRellenado válido: Se llama así porque se usan sólo los elementos de \\(\\boldsymbol{x}\\) que están definidos. Si algún elemento de \\(\\boldsymbol{z}\\) depende de un elemento de \\(\\boldsymbol{x}\\) que no está definido, se descarta. Esto se ilustra en la figura 10.2 (d) del libro.\nRellenado de ceros: Se llama así porque se agregan ceros a los bordes de \\(\\boldsymbol{x}\\) para que todos los elementos de \\(\\boldsymbol{z}\\) dependan de elementos de \\(\\boldsymbol{x}\\) que están definidos. Esto se ilustra en la figura 10.2 (c) del libro.",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "14. Redes Convolucionales"
    ]
  },
  {
    "objectID": "14_redes_convolucionales.html#paso-stride-y-agrupamiento-pooling.",
    "href": "14_redes_convolucionales.html#paso-stride-y-agrupamiento-pooling.",
    "title": "Redes Convolucionales",
    "section": "Paso (stride) y agrupamiento (pooling).",
    "text": "Paso (stride) y agrupamiento (pooling).\nComo discutiremos más adelante, es usual que el vector \\(\\boldsymbol{z}\\) tenga menos elementos que el vector \\(\\boldsymbol{x}\\). Esto se puede hacer de dos maneras:\n\nPaso (stride): Como cada elemento de \\(\\boldsymbol{z}\\) depende de \\(2k+1\\) elementos de \\(\\boldsymbol{x}\\), no perdemos mucha información si calculamos sólo algunos de los elementos de \\(\\boldsymbol{z}\\). Es decir, definimos \\(z_i = \\sum_{j = -k}^k w_j x_{i\\cdot p-j}\\) donde \\(p\\) es el paso. Como vemos, \\(\\boldsymbol{z}\\) tendrá aproximadamente \\(\\frac{n}{p}\\) elementos. Esto se ilustra en las figuras 10.3 (a) y (b) del libro.\nAgrupamiento (pooling): Otra manera de reducir la dimensión es mediante un agrupamiento o pooling. Los más comunes son:\n\nMax pooling: Se obtiene \\(z_i = \\max_{j = -k}^k y_{i\\cdot p-j}\\), donde \\(y\\) es el resultado de una convolución con un kernel de tamaño \\(2k+1\\). Es decir, se toma el valor máximo de la convolución.\nAverage pooling: Se obtiene \\(z_i = \\frac{1}{2k+1} \\sum_{j = -k}^k y_{i\\cdot p-j}\\), donde \\(y\\) es el resultado de una convolución con un kernel de tamaño \\(2k+1\\). Es decir, se toma el promedio de la convolución.",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "14. Redes Convolucionales"
    ]
  },
  {
    "objectID": "14_redes_convolucionales.html#capas-convolucionales",
    "href": "14_redes_convolucionales.html#capas-convolucionales",
    "title": "Redes Convolucionales",
    "section": "Capas convolucionales",
    "text": "Capas convolucionales\nUna capa convolucional consiste en realizar una convolución de la señal de entrada \\(\\boldsymbol{x}\\) con un kernel \\(\\boldsymbol{w}\\), sumar un sesgo \\(\\beta\\) y aplicar una función de activación \\(a\\) para producir la salida \\(h\\). En realidad se producen varios canales \\(h_i\\) a partir de la misma señal de entrada \\(\\boldsymbol{x}\\), cada uno con un kernel diferente: \\[\nh_{ic} = a\\left(\\sum_{j = -k}^k w_{cj} x_{i+j} + \\beta_{c}\\right)\\,,\n\\] donde \\(c\\) es el canal, \\(i\\) se refiere a cada pixel, y \\(j\\) es el índice de la convolución. Esto se ilustra en la figura 10.5 del libro.\nSi tenemos \\(C\\) canales de salida, y un tamaño de kernel \\(2k+1\\), entonces la capa convolucional tiene \\(C(2k+2)\\) parámetros. Si comparamos con una capa densa, que tiene \\(C\\) neuronas y \\(n\\) entradas, entonces la capa densa tiene \\(C(n + 1)\\) parámetros. El tamaño del kernel es mucho menor que el tamaño de la señal de entrada \\(2k+1 \\ll n\\). Por lo tanto, la capa convolucional tiene muchos menos parámetros que la capa densa. A pesar de tener menos parámetros, logra capturar la forma de las señales locales de forma mucho más eficaz. Esto es porque le hemos dado información adicional sobre el problema: La invarianza bajo traslaciones. Esta diferencia se ilustra en la figura 10.4 del libro.\nEn el libro se ilustra un ejemplo que llama MNIST-1D. Nosotros usaremos el ejemplo más clásico con los datos MNIST usual en 2D.",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "14. Redes Convolucionales"
    ]
  },
  {
    "objectID": "14_redes_convolucionales.html#campo-receptivo-receptive-field",
    "href": "14_redes_convolucionales.html#campo-receptivo-receptive-field",
    "title": "Redes Convolucionales",
    "section": "Campo receptivo (receptive field)",
    "text": "Campo receptivo (receptive field)\nEl campo receptivo es el área de la imagen que se utiliza para calcular el valor de una neurona. En el caso de una capa densa (completamente conectada), el campo receptivo es toda la imagen.\nEn el caso de una sola capa convolucional, un pixel dado \\(i,j\\) depende de un área de los canales de entrada de tamaño \\(2k+1\\). Por lo tanto, el campo receptivo es de tamaño \\(2k+1\\). Si hay más capas convolucionales, el campo receptivo crece porque cada pixel de un filtro de entrada a su vez depende de varios pixeles de la capa anterior.",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "14. Redes Convolucionales"
    ]
  },
  {
    "objectID": "11_perceptron_y_redes_conexas.html",
    "href": "11_perceptron_y_redes_conexas.html",
    "title": "El perceptrón, redes completamente conexas y pérdidas",
    "section": "",
    "text": "Cada una de estas arquitecturas responde a una estructura de datos o problema específico, y se han vuelto herramientas clave en física computacional:\n\nRedes completamente conectadas (MLPs): consisten en capas de neuronas donde cada una está conectada a todas las de la capa anterior. Son adecuadas para datos tabulares o de baja dimensión. Se utilizan para aproximación de funciones, regresión y clasificación simple. Son la base de muchas otras arquitecturas más complejas.\nRedes convolucionales: se basan en aplicar filtros (kernels) que exploran estructuras locales en los datos. Son especializadas en datos espaciales como imágenes. Son comunes en reconocimiento de patrones y análisis de mapas de temperatura o distribución de materia en cosmología.\nRedes recurrentes y variantes: incorporan ciclos en su arquitectura para mantener una memoria de estados anteriores. Se usan para procesamiento secuencial y temporal. Útiles para series temporales físicas, como evolución temporal de sistemas o señales en experimentos.\nRedes residuales y profundas: introducen conexiones identitarias (skip connections) que permiten pasar información sin alteración a capas posteriores. Facilitan el entrenamiento de redes muy profundas al evitar el problema del desvanecimiento del gradiente. Son utilizadas en física para modelos predictivos muy complejos con datos jerárquicos o multiescalares.\nTransformadores: utilizan mecanismos de atención que asignan pesos a diferentes partes de la entrada según su relevancia contextual. Son la arquitectura dominante para datos secuenciales, procesamiento de lenguaje y más allá. Recientemente aplicados en física para modelar correlaciones de largo alcance y aprendizaje de representaciones de sistemas físicos.\n\n\n\n\nCada una de las siguientes áreas ha sido impulsada recientemente por el uso de redes neuronales, en muchos casos superando métodos tradicionales:\n\nPredicción de parámetros cosmológicos a partir de mapas del cielo: redes profundas permiten inferir parámetros fundamentales del modelo cosmológico a partir de datos como el fondo cósmico de microondas o mapas de lentes gravitacionales.\nReconstrucción de dinámica de fluidos mediante operadores neuronales: los operadores neuronales permiten modelar sistemas complejos como turbulencia sin resolver explícitamente todas las escalas del sistema.\nReducción de ruido en datos experimentales (e.g., espectros): redes autoencoder y convolucionales permiten eliminar ruido instrumental y mejorar la calidad de señales físicas.\nResolución de EDPs en geometrías complejas: métodos como PINNs (Physics-Informed Neural Networks) resuelven ecuaciones diferenciales directamente en dominios arbitrarios, incluyendo condiciones de frontera no triviales.",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "11. Perceptrón, Redes Conexas y Pérdidas"
    ]
  },
  {
    "objectID": "11_perceptron_y_redes_conexas.html#arquitecturas-importantes",
    "href": "11_perceptron_y_redes_conexas.html#arquitecturas-importantes",
    "title": "El perceptrón, redes completamente conexas y pérdidas",
    "section": "",
    "text": "Cada una de estas arquitecturas responde a una estructura de datos o problema específico, y se han vuelto herramientas clave en física computacional:\n\nRedes completamente conectadas (MLPs): consisten en capas de neuronas donde cada una está conectada a todas las de la capa anterior. Son adecuadas para datos tabulares o de baja dimensión. Se utilizan para aproximación de funciones, regresión y clasificación simple. Son la base de muchas otras arquitecturas más complejas.\nRedes convolucionales: se basan en aplicar filtros (kernels) que exploran estructuras locales en los datos. Son especializadas en datos espaciales como imágenes. Son comunes en reconocimiento de patrones y análisis de mapas de temperatura o distribución de materia en cosmología.\nRedes recurrentes y variantes: incorporan ciclos en su arquitectura para mantener una memoria de estados anteriores. Se usan para procesamiento secuencial y temporal. Útiles para series temporales físicas, como evolución temporal de sistemas o señales en experimentos.\nRedes residuales y profundas: introducen conexiones identitarias (skip connections) que permiten pasar información sin alteración a capas posteriores. Facilitan el entrenamiento de redes muy profundas al evitar el problema del desvanecimiento del gradiente. Son utilizadas en física para modelos predictivos muy complejos con datos jerárquicos o multiescalares.\nTransformadores: utilizan mecanismos de atención que asignan pesos a diferentes partes de la entrada según su relevancia contextual. Son la arquitectura dominante para datos secuenciales, procesamiento de lenguaje y más allá. Recientemente aplicados en física para modelar correlaciones de largo alcance y aprendizaje de representaciones de sistemas físicos.",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "11. Perceptrón, Redes Conexas y Pérdidas"
    ]
  },
  {
    "objectID": "11_perceptron_y_redes_conexas.html#aplicaciones-a-la-física",
    "href": "11_perceptron_y_redes_conexas.html#aplicaciones-a-la-física",
    "title": "El perceptrón, redes completamente conexas y pérdidas",
    "section": "",
    "text": "Cada una de las siguientes áreas ha sido impulsada recientemente por el uso de redes neuronales, en muchos casos superando métodos tradicionales:\n\nPredicción de parámetros cosmológicos a partir de mapas del cielo: redes profundas permiten inferir parámetros fundamentales del modelo cosmológico a partir de datos como el fondo cósmico de microondas o mapas de lentes gravitacionales.\nReconstrucción de dinámica de fluidos mediante operadores neuronales: los operadores neuronales permiten modelar sistemas complejos como turbulencia sin resolver explícitamente todas las escalas del sistema.\nReducción de ruido en datos experimentales (e.g., espectros): redes autoencoder y convolucionales permiten eliminar ruido instrumental y mejorar la calidad de señales físicas.\nResolución de EDPs en geometrías complejas: métodos como PINNs (Physics-Informed Neural Networks) resuelven ecuaciones diferenciales directamente en dominios arbitrarios, incluyendo condiciones de frontera no triviales.",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "11. Perceptrón, Redes Conexas y Pérdidas"
    ]
  },
  {
    "objectID": "11_perceptron_y_redes_conexas.html#el-perceptrón-definición-y-clasificación-lineal",
    "href": "11_perceptron_y_redes_conexas.html#el-perceptrón-definición-y-clasificación-lineal",
    "title": "El perceptrón, redes completamente conexas y pérdidas",
    "section": "El Perceptrón: Definición y Clasificación Lineal",
    "text": "El Perceptrón: Definición y Clasificación Lineal\nEl perceptrón, es un algoritmo para clasificación binaria supervisada. Calcula una salida \\(z\\) como una combinación lineal de las entradas \\(\\mathbf{x} \\in \\mathbb{R}^d\\), más un sesgo \\(b\\):\n\\[\nz = \\mathbf{w}^T \\mathbf{x} + b = \\sum_{i=1}^d w_i x_i + b\\,,\n\\]\ndonde \\(\\mathbf{w} \\in \\mathbb{R}^d\\) es el vector de pesos y \\(b \\in \\mathbb{R}\\) es el sesgo.\nLuego, aplica una función de activación no lineal. Originalmente, se usaba la función escalón (o Heaviside) \\(\\phi(z) = 1\\) si \\(z \\geq 0\\) y \\(\\phi(z) = 0\\) si \\(z &lt; 0\\). Entonces la salida del perceptrón es \\(\\hat{y} = \\phi(z)\\).\nLa ecuación \\(\\mathbf{w}^T \\mathbf{x} + b = 0\\) define un hiperplano en el espacio de entrada \\(\\mathbb{R}^d\\). Este hiperplano divide el espacio en dos semiespacios, correspondiendo a las dos clases. Por lo tanto, el perceptrón original es un clasificador lineal y solo puede separar datos que sean linealmente separables.\nGráficamente, dibujamos el perceptrón como teniendo \\(d\\) líneas, una por cada entrada, y un sesgo, y una línea de salida.\nLas redes neuronales modernas utilizan funciones de activación diferenciables (casi en todas partes). Una de las más populares es la Unidad Lineal Rectificada (ReLU):\n\\[\n\\phi_{\\text{ReLU}}(z) = \\max(0, z) = \\begin{cases} z, & \\text{si } z &gt; 0\\\\ 0, & \\text{si } z \\le 0 \\end{cases}\\,.\n\\]\n\nSi solo se usaran transformaciones lineales (sin activación no lineal o con activación identidad), una red de múltiples capas colapsaría en una única transformación lineal equivalente: \\(y = W_{\\text{eff}}^T\\mathbf{x} + b_{\\text{eff}}\\). La no linealidad es esencial para la capacidad expresiva de las redes profundas, como veremos más adelante.\nLa ReLU, a pesar de su simplicidad, introduce “quiebres” (puntos no diferenciables en \\(z=0\\)) que permiten a las redes neuronales construir funciones lineales a trozos (piecewise-linear) muy complejas.\nCada neurona con activación ReLU divide el espacio de entrada según el hiperplano \\(\\mathbf{w}^T \\mathbf{x} + b = 0\\): en una región la neurona está “activa” (salida \\(&gt; 0\\)) y en la otra está “inactiva” (salida \\(= 0\\)).\n\nNota: Otras activaciones comunes incluyen la sigmoide y la tangente hiperbólica (tanh), aunque ReLU y sus variantes son a menudo preferidas en redes profundas.",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "11. Perceptrón, Redes Conexas y Pérdidas"
    ]
  },
  {
    "objectID": "11_perceptron_y_redes_conexas.html#redes-con-una-capa-oculta",
    "href": "11_perceptron_y_redes_conexas.html#redes-con-una-capa-oculta",
    "title": "El perceptrón, redes completamente conexas y pérdidas",
    "section": "Redes con Una Capa Oculta",
    "text": "Redes con Una Capa Oculta\nCombinando múltiples neuronas (usualmente con activaciones como ReLU) en una “capa oculta”, podemos construir redes más potentes. Una red neuronal superficial (shallow network) con una capa oculta de \\(H\\) neuronas y una capa de salida con una única neurona (para regresión o clasificación binaria) calcula:\n\\[\n\\hat{y}(\\mathbf{x}) = \\mathbf{W}_{(2)}^T \\phi(\\mathbf{W}_{(1)}\\mathbf{x} + \\mathbf{b}_{(1)}) + b_{(2)}\\,,\n\\]\ndonde: - \\(\\mathbf{W}_{(1)} \\in \\mathbb{R}^{H \\times d}\\) y \\(\\mathbf{b}_{(1)} \\in \\mathbb{R}^H\\) son los pesos y sesgos de la capa oculta. - \\(\\phi\\) es la función de activación (e.g., ReLU), aplicada elemento a elemento. - \\(\\mathbf{W}_{(2)} \\in \\mathbb{R}^H\\) y \\(b_{(2)} \\in \\mathbb{R}\\) son los pesos y el sesgo de la capa de salida.\nGráficamente, representamos esta red como un conjunto de \\(H\\) perceptrones, donde cada uno tiene \\(d\\) líneas, una por cada entrada, un sesgo, y una línea de salida. Esas \\(H\\) salidas van todas a un único perceptrón de salida, que tiene \\(m\\) conexiones de entrada y \\(1\\) conexión de salida.\nEsta red divide el espacio de entrada \\(\\mathbb{R}^d\\) en múltiples regiones poliédricas convexas. Dentro de cada región, la función \\(\\hat{y}(\\mathbf{x})\\) se comporta como una función afín (lineal más una constante). Los límites entre estas regiones están determinados por los hiperplanos \\(\\mathbf{w}_i^{(1)} \\cdot \\mathbf{x} + b_i^{(1)} = 0\\) de las neuronas ocultas.\nEl número máximo de regiones lineales que se pueden crear con \\(H\\) neuronas en \\(d\\) dimensiones tiene un límite superior dado por: \\[\nR(d,H) = \\sum_{k=0}^d \\binom{H}{k}\\,,\n\\] Al aumentar el número de neuronas \\(H\\), la red puede aproximar funciones cada vez más complejas.",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "11. Perceptrón, Redes Conexas y Pérdidas"
    ]
  },
  {
    "objectID": "11_perceptron_y_redes_conexas.html#teorema-de-aproximación-universal-para-redes-superficiales",
    "href": "11_perceptron_y_redes_conexas.html#teorema-de-aproximación-universal-para-redes-superficiales",
    "title": "El perceptrón, redes completamente conexas y pérdidas",
    "section": "Teorema de Aproximación Universal para Redes Superficiales",
    "text": "Teorema de Aproximación Universal para Redes Superficiales\nLa capacidad de las redes neuronales para aproximar funciones está formalizada por el Teorema de Aproximación Universal. Para redes superficiales (una capa oculta), establece (informalmente):\n\n\n\n\n\n\nTeorema de Aproximación Universal\n\n\n\nUna red neuronal feedforward con una sola capa oculta que contiene un número finito de neuronas con una función de activación no lineal, puede aproximar cualquier función continua \\(f\\) sobre un subconjunto compacto de \\(\\mathbb{R}^d\\) con cualquier grado de precisión deseado \\(\\varepsilon &gt; 0\\). Es decir, existe una red \\(\\hat{y}(\\mathbf{x})\\) tal que: \\[\n\\sup_{\\mathbf{x}} |f(\\mathbf{x}) - \\hat{y}(\\mathbf{x})| &lt; \\varepsilon\\,.\n\\]\n\n\nNo demostramos el teorema, pero damos una intuición: En una dimensión, cualquier función \\(f\\) en una región pequeña puede ser aproximada por una recta. Entonces, al usar muchas rectas, podemos aproximar cualquier función. En más dimensiones podemos hacer lo mismo con hiperplanos.\nEste es un teorema de existencia: garantiza que existe una red capaz de la aproximación, pero no dice cómo encontrar sus pesos y sesgos ni cuál es el número mínimo de neuronas \\(H\\) necesario (que puede ser muy grande, potencialmente creciendo exponencialmente con la dimensión \\(d\\) o la complejidad de \\(f\\)).",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "11. Perceptrón, Redes Conexas y Pérdidas"
    ]
  },
  {
    "objectID": "11_perceptron_y_redes_conexas.html#redes-superficiales-con-múltiples-salidas-mlp-de-una-capa-oculta",
    "href": "11_perceptron_y_redes_conexas.html#redes-superficiales-con-múltiples-salidas-mlp-de-una-capa-oculta",
    "title": "El perceptrón, redes completamente conexas y pérdidas",
    "section": "Redes Superficiales con Múltiples Salidas (MLP de una Capa Oculta)",
    "text": "Redes Superficiales con Múltiples Salidas (MLP de una Capa Oculta)\nExisten muchos casos en los que queremos predecir más de un valor, por ejemplo, predecir la temperatura y la humedad en un punto, o predecir la posición de un objeto en un video. A esto se lo llama regresión vectorial. Otro ejemplo es cuando tenemos varias clases y queremos predecir la probabilidad de pertenecer a cada una.\nPara tareas con múltiples salidas (e.g., clasificación multiclase, regresión vectorial), la capa de salida tendrá \\(m &gt; 1\\) neuronas. La arquitectura común es un Perceptrón Multicapa (MLP) con una capa oculta:\n\\[\n\\mathbf{y} = \\phi_{\\text{salida}}(\\mathbf{W}_{(2)} \\phi_{\\text{hidden}}(\\mathbf{W}^{(1)}\\mathbf{x} + \\mathbf{b}^{(1)}) + b^{(2)})\\,,\n\\]\ndonde:\n\n\\(\\mathbf{W}^{(1)} \\in \\mathbb{R}^{H \\times d}\\), \\(\\mathbf{b}^{(1)} \\in \\mathbb{R}^H\\) (Capa oculta con \\(H\\) neuronas).\n\\(\\phi_{\\text{hidden}}\\) es la activación de la capa oculta (e.g., ReLU), aplicada elemento a elemento.\n\\(\\mathbf{W}^{(2)} \\in \\mathbb{R}^{m \\times H}\\), \\(\\mathbf{b}^{(2)} \\in \\mathbb{R}^m\\) (Capa de salida con \\(m\\) neuronas).\n\\(\\phi_{\\text{salida}}\\) es la activación de la capa de salida. Como la capa de salida nos dabe dar el resultado que buscamos, es posible que la función de activación de la capa de salida sea diferente (\\(\\phi_{\\text{salida}}\\)) a la de las capas ocultas. Por ejemplo, si estamos resolviendo un problema de regresión, la función de activación de la capa de salida será la identidad ya que nos interesa un valor en todos los reales. Mientras que si estamos resolviendo un problema de clasificación multiclase, la función de activación de la capa de salida será la Softmax, es decir \\[\n  \\phi_{\\text{salida}}(\\mathbf{z}^{(L)}) = \\frac{e^{\\mathbf{z}^{(L)}}}{\\sum_{i=1}^{n_L} e^{z_i^{(L)}}}\\,,\n  \\] que se interpreta como la probabilidad de que el punto pertenezca a la clase \\(k\\).\n\nGráficamente, representamos esta red como un conjunto de \\(H\\) perceptrones, donde cada uno tiene \\(d\\) conexiones de entrada y \\(1\\) conexión de salida. Esas \\(H\\) salidas van todas a un único perceptrón de salida, que tiene \\(m\\) conexiones de entrada y \\(1\\) conexión de salida.\n\n\n\n\n\n\n\n\nFigura 1: Arquitectura de una MLP con una capa oculta\n\n\n\n\n\nEl número total de parámetros (pesos y sesgos) en esta red es: \\[\n\\#\\text{parámetros} = \\underbrace{(d \\times H + H)}_{\\text{Capa 1}} + \\underbrace{(H \\times m + m)}_{\\text{Capa 2}}\n\\] La expresividad y capacidad de la red aumentan con el número de neuronas ocultas \\(H\\), a costa de un mayor número de parámetros a aprender.\n\n\nCódigo\nfrom sklearn.datasets import make_moons\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Implementación manual de red neuronal\ndef relu(x):\n    return np.maximum(0, x)\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# Parámetros de la red (pesos y sesgos)\nW1 = np.array([[0.5, -0.14], [0.65, 1.52], [0.2, -0.2]]) # Capa oculta (3 neuronas)\nb1 = [0.49, 0.82, -0.2]\nW2 = np.random.RandomState().randn(1, 3)  # Capa de salida\nb2 = np.random.RandomState().randn(1)\n\ndef predict(x):\n    # Capa oculta\n    x_np = np.array(x) # Ensure x is a numpy array for matmul\n    z1 = x_np @ W1.T + b1\n    a1 = relu(z1)\n    # Capa de salida\n    z2 = a1 @ W2.T + b2\n    return sigmoid(z2)\n\n# Crear grid\nxx, yy = np.meshgrid(np.linspace(-2.5, 1.5, 100), np.linspace(-3, 2, 100))\n\n# Predecir en toda la grid para la salida de la red\nZ = np.array([predict(np.array([x_val, y_val])) for x_val, y_val in zip(xx.ravel(), yy.ravel())])\nZ = Z.reshape(xx.shape)\n\n# Plot\nplt.figure(figsize=(8,6))\n\n# Graficar la salida continua de la red\ncontour = plt.contourf(xx, yy, Z, levels=50, cmap='inferno', alpha=0.9)\n\n# Definir colores y estilos para las intersecciones\ncolors = ['#2E86C1', '#E74C3C', '#F1C40F']\n\n# Grafico de los hiperplanos superpuestos\nfor i in range(W1.shape[0]):\n    w = W1[i]\n    bias = b1[i]\n    color = colors[i % len(colors)]\n    # Plot a slightly thicker white line first for outline effect\n    if abs(w[1]) &gt; 1e-6:\n        y_hyper = (-w[0] * xx[0,:] - bias) / w[1]\n        plt.plot(xx[0,:], y_hyper, color='white', linewidth=3.5) \n        plt.plot(xx[0,:], y_hyper, color=color, linewidth=2,\n                 label=f'Neurona {i+1}') # Label only the colored line\n    elif abs(w[0]) &gt; 1e-6:\n        x_hyper = -bias / w[0]\n        plt.axvline(x=x_hyper, color='white', linewidth=3.5)\n        plt.axvline(x=x_hyper, color=color, linewidth=2,\n                    label=f'Neurona {i+1} (vertical)')\n\nplt.title(\"Salida de la Red y Hiperplanos de la Capa Oculta\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$x_2$\")\nplt.legend(loc='lower right', title=\"Hiperplanos\")\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\nplt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.show()\n\n\n\n\n\n\n\n\nFigura 2: Regiones lineales para una red con una capa oculta de 3 neuronas. Cada hiperplano divide el espacio en dos regiones. Se grafican las intersecciones de los hiperplanos con el espacio (\\(x_1\\), \\(x_2\\)). Vemos que esto genera varias regiones lineales. Se puede ver que cada región es lineal porque la salida crece linealmente (no se curva).",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "11. Perceptrón, Redes Conexas y Pérdidas"
    ]
  },
  {
    "objectID": "11_perceptron_y_redes_conexas.html#arquitectura-multicapa",
    "href": "11_perceptron_y_redes_conexas.html#arquitectura-multicapa",
    "title": "El perceptrón, redes completamente conexas y pérdidas",
    "section": "Arquitectura Multicapa",
    "text": "Arquitectura Multicapa\nUn Perceptrón Multicapa (MLP) profundo consta de una secuencia de capas. Si consideramos \\(L\\) capas en total (incluyendo la de salida), la estructura es:\n\nCapa de Entrada: Simplemente contiene el vector de entrada \\(\\mathbf{x} \\in \\mathbb{R}^{n_0}\\), donde \\(n_0=d\\) es la dimensionalidad de los datos. Definimos \\(\\mathbf{a}^{(0)} = \\mathbf{x}\\).\nCapas Ocultas (\\(\\ell = 1, \\dots, L-1\\)): Cada capa oculta \\(\\ell\\) recibe las activaciones \\(\\mathbf{a}^{(\\ell-1)} \\in \\mathbb{R}^{n_{\\ell-1}}\\) de la capa anterior y calcula sus propias activaciones \\(\\mathbf{a}^{(\\ell)} \\in \\mathbb{R}^{n_\\ell}\\) mediante:\n\nTransformación Afín: \\(\\mathbf{z}^{(\\ell)} = W^{(\\ell)} \\mathbf{a}^{(\\ell-1)} + \\mathbf{b}^{(\\ell)}\\)\nActivación No Lineal: \\(\\mathbf{a}^{(\\ell)} = \\phi(\\mathbf{z}^{(\\ell)})\\)\n\nDonde:\n\n\\(W^{(\\ell)} \\in \\mathbb{R}^{n_\\ell \\times n_{\\ell-1}}\\) es la matriz de pesos de la capa \\(\\ell\\).\n\\(\\mathbf{b}^{(\\ell)} \\in \\mathbb{R}^{n_\\ell}\\) es el vector de sesgos de la capa \\(\\ell\\).\n\\(n_\\ell\\) es el número de neuronas (ancho) de la capa \\(\\ell\\).\n\\(\\phi\\) es la función de activación (e.g., ReLU: \\(\\phi(u) = \\max(0, u)\\)), aplicada elemento a elemento al vector \\(\\mathbf{z}^{(\\ell)}\\).\n\nCapa de Salida (\\(\\ell = L\\)): Calcula la salida final \\(\\mathbf{y} = \\mathbf{a}^{(L)} \\in \\mathbb{R}^{n_L}\\) de manera similar \\[\n  \\mathbf{z}^{(L)} = W^{(L)} \\mathbf{a}^{(L-1)} + \\mathbf{b}^{(L)}\n  \\] \\[\n  \\mathbf{y} = \\mathbf{a}^{(L)} = \\phi_{\\text{salida}}(\\mathbf{z}^{(L)})\n  \\] donde la función de activación de la capa de salida \\(\\phi_{\\text{salida}}\\) puede ser diferente a la de las capas ocultas (e.g., lineal para regresión, softmax para clasificación multiclase)\n\nLa profundidad \\(L\\) (número de capas computacionales) y los anchos \\(n_1, \\dots, n_L\\) definen la arquitectura de la red. Se cree que la profundidad permite a la red aprender representaciones jerárquicas de los datos, donde las capas iniciales detectan características simples y las capas posteriores las combinan en conceptos más complejos.\n\n\n\n\n\n\n\n\nFigura 3: Arquitectura de una MLP con dos capas ocultas\n\n\n\n\n\n\n\n\n\n\n\nEjemplo: Pase Hacia Adelante Manual (Forward Pass)\n\n\n\nConsideremos una MLP pequeña con \\(L=3\\):\n\nEntrada: \\(\\mathbf{x} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\\)\nCapa Oculta 1: \\(n_1=3\\) neuronas, activación ReLU.\nCapa Oculta 2: \\(n_2=2\\) neuronas, activación ReLU.\nCapa de Salida: \\(n_3=1\\) neurona (salida escalar), activación Identidad.\n\nSupongamos los siguientes valores para los parámetros (matrices por filas):\n\n\\(W^{(1)} = \\begin{pmatrix} 1 & -1 \\\\ 0 & 2 \\\\ 1 & 1 \\end{pmatrix}\\), \\(\\mathbf{b}^{(1)} = \\begin{pmatrix} 0 \\\\ 1 \\\\ -1 \\end{pmatrix}\\)\n\\(W^{(2)} = \\begin{pmatrix} 2 & -1 & 1 \\\\ -1 & 0 & 2 \\end{pmatrix}\\), \\(\\mathbf{b}^{(2)} = \\begin{pmatrix} 0 \\\\ 0.5 \\end{pmatrix}\\)\n\\(W^{(3)} = \\begin{pmatrix} 1 & -2 \\end{pmatrix}\\), \\(\\mathbf{b}^{(3)} = 0.5\\)\n\n\nCapa 1: \\[\n\\mathbf{z}^{(1)} = W^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)} =\n\\begin{pmatrix}\n   1 & -1 \\\\\n   0 & 2 \\\\\n   1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n   1 \\\\ 2\n\\end{pmatrix}\n+\n\\begin{pmatrix} 0 \\\\ 1 \\\\ -1 \\end{pmatrix}\n\\]\n\\[\n=\n\\begin{pmatrix}\n   -1 \\\\\n   4 \\\\\n   3\n\\end{pmatrix}\n+\n\\begin{pmatrix} 0 \\\\ 1 \\\\ -1 \\end{pmatrix}\n=\n\\begin{pmatrix}\n   -1 \\\\\n   5 \\\\\n   2\n\\end{pmatrix}\n\\]\nAplicando ReLU: \\[\n\\mathbf{a}^{(1)} = \\text{ReLU}(\\mathbf{z}^{(1)}) =\n\\begin{pmatrix}\n   0 \\\\\n   5 \\\\\n   2\n\\end{pmatrix}\n\\]\nCapa 2: \\[\n\\mathbf{z}^{(2)} = W^{(2)} \\mathbf{a}^{(1)} + \\mathbf{b}^{(2)} =\n\\begin{pmatrix}\n   2 & -1 & 1 \\\\\n   -1 & 0 & 2\n\\end{pmatrix}\n\\begin{pmatrix}\n   0 \\\\ 5 \\\\ 2\n\\end{pmatrix}\n+\n\\begin{pmatrix}\n   0 \\\\ 0.5\n\\end{pmatrix}\n\\] \\[\n= \\begin{pmatrix} -3 \\\\ 4 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 0.5 \\end{pmatrix} = \\begin{pmatrix} -3 \\\\ 4.5 \\end{pmatrix}\n\\]\nAplicando ReLU: \\[\n\\mathbf{a}^{(2)} = \\text{ReLU}(\\mathbf{z}^{(2)}) = \\begin{pmatrix} 0 \\\\ 4.5 \\end{pmatrix}\n\\]\nCapa 3 (Salida): \\[\n\\mathbf{z}^{(3)} = W^{(3)} \\mathbf{a}^{(2)} + \\mathbf{b}^{(3)} = \\begin{pmatrix} 1 & -2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 4.5 \\end{pmatrix} + 0.5\n\\] \\[\n= -9 + 0.5 = -8.5\n\\]\nComo la activación de salida es identidad: \\[\ny = \\mathbf{a}^{(3)} = \\mathbf{z}^{(3)} = -8.5\n\\]",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "11. Perceptrón, Redes Conexas y Pérdidas"
    ]
  },
  {
    "objectID": "11_perceptron_y_redes_conexas.html#mlps-con-relu-como-funciones-afines-por-tramos-piecewise-affine",
    "href": "11_perceptron_y_redes_conexas.html#mlps-con-relu-como-funciones-afines-por-tramos-piecewise-affine",
    "title": "El perceptrón, redes completamente conexas y pérdidas",
    "section": "MLPs con ReLU como Funciones Afines por Tramos (Piecewise Affine)",
    "text": "MLPs con ReLU como Funciones Afines por Tramos (Piecewise Affine)\nCuando se utiliza la activación ReLU, cada neurona \\(i\\) en la capa \\(\\ell\\) calcula \\(ReLU = \\max(0, \\mathbf{w}_i^{(\\ell)} \\cdot \\mathbf{a}^{(\\ell-1)} + b_i^{(\\ell)})\\). La condición \\(\\mathbf{w}_i^{(\\ell)} \\cdot \\mathbf{a}^{(\\ell-1)} + b_i^{(\\ell)} = 0\\) define un hiperplano en el espacio de las activaciones de la capa anterior \\(\\mathbf{a}^{(\\ell-1)}\\).\nLa composición de múltiples capas de estas operaciones (transformación afín seguida de ReLU) resulta en que la función global \\(f_{\\text{MLP}}: \\mathbb{R}^{n_0} \\to \\mathbb{R}^{n_L}\\) implementada por la red es una función continua y afín por tramos (piecewise affine). Esto significa que el espacio de entrada \\(\\mathbb{R}^{n_0}\\) se divide en un gran número de regiones (poliédricas convexas), y dentro de cada región, la función \\(f_{\\text{MLP}}(\\mathbf{x})\\) se comporta como una función afín \\(\\mathbf{x} \\mapsto A \\mathbf{x} + c\\) (donde la matriz \\(A\\) y el vector \\(c\\) son constantes dentro de esa región específica).\nLa configuración de qué neuronas están activas (\\(\\mathbf{z}_i^{(\\ell)} &gt; 0\\)) o inactivas (\\(\\mathbf{z}_i^{(\\ell)} \\le 0\\)) para una entrada \\(\\mathbf{x}\\) dada define el patrón de activación. Cada patrón de activación distinto corresponde (potencialmente) a una región diferente en el espacio de entrada donde la función es localmente afín.",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "11. Perceptrón, Redes Conexas y Pérdidas"
    ]
  },
  {
    "objectID": "11_perceptron_y_redes_conexas.html#capacidad-de-representación-parámetros-vs.-regiones-lineales",
    "href": "11_perceptron_y_redes_conexas.html#capacidad-de-representación-parámetros-vs.-regiones-lineales",
    "title": "El perceptrón, redes completamente conexas y pérdidas",
    "section": "Capacidad de Representación: Parámetros vs. Regiones Lineales",
    "text": "Capacidad de Representación: Parámetros vs. Regiones Lineales\nLas redes multicapa tienen mucha más capacidad de representación que las redes con una sola capa oculta. Esto se debe a que pueden alcanzar una mayor complejidad con un número menor de parámetros. Además, las redes multicapa pueden aprender representaciones jerárquicas de los datos: Las capas iniciales detectan características simples y las capas posteriores las combinan en conceptos más complejos.\n\nNúmero de Parámetros: El número total de parámetros (pesos y sesgos) en una MLP con \\(L\\) capas es: \\[\n  \\#\\text{Parámetros} = \\sum_{\\ell=1}^L (n_\\ell \\times n_{\\ell-1} + n_\\ell)\n  \\] (Recordando que \\(n_0\\) es la dimensión de entrada).\nNúmero de Regiones Lineales: Para MLPs con activación ReLU, el número máximo de regiones lineales \\(R\\) en las que la función divide el espacio de entrada puede crecer muy rápidamente con la profundidad. Montúfar et al. (2014) demostraron que, bajo ciertas condiciones, el número de regiones puede crecer exponencialmente con la profundidad \\(L\\), aproximadamente como \\(\\Omega\\left(\\left(\\frac{n}{d}\\right)^{(L-1)n_0} n^{n_0}\\right)\\) para una red con capas ocultas de ancho \\(n\\) y entrada de dimensión \\(n_0\\). La fórmula exacta es \\[\n\\left(\\prod_{\\ell=1}^L \\left\\lfloor\\frac{n_\\ell}{n_0}\\right\\rfloor^{(L-1)n_0} \\right) \\sum_{j=0}^{n_0} \\binom{n_L}{j}\n\\]\n\n\n\nCódigo\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\nfrom scipy.special import binom\n\n# Definimos la función para calcular el número máximo de regiones lineales\ndef max_linear_regions(x_values, L, n0=2):\n    regions = []\n    for n in x_values:\n        # Para cada valor de n (ancho de capa), calculamos el producto\n        product_term = 1\n        for l in range(1, L+1):\n            # Asumimos que todas las capas tienen el mismo ancho n\n            product_term *= math.floor(n/n0)**(n0*(L-1))\n        \n        # Calculamos la suma de los coeficientes binomiales\n        sum_term = 0\n        for j in range(n0+1):\n            sum_term += binom(n, j)\n        \n        # El número total de regiones es el producto por la suma\n        regions.append(product_term * sum_term)\n    \n    return regions\n\ndef num_params(x_values, L):\n    params = []\n    for n in x_values:\n        params.append((n**2 + n)*L)\n    return params\n\n# Valores de x (ancho de capa) para evaluar\nx_values = np.arange(2, 101, 1)\n\n# Calculamos el número de regiones para diferentes profundidades L\nregions_L1 = max_linear_regions(x_values, L=1)\nregions_L2 = max_linear_regions(x_values, L=2)\nregions_L3 = max_linear_regions(x_values, L=3)\n\n# Calculamos el número de parámetros para diferentes profundidades L\nparams_L1 = num_params(x_values, L=1)\nparams_L2 = num_params(x_values, L=2)\nparams_L3 = num_params(x_values, L=3)\n\n# Creamos la gráfica\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 6))\n\nax1.plot(x_values, regions_L1, label='L=1', linewidth=2)\nax1.plot(x_values, regions_L2, label='L=2', linewidth=2)\nax1.plot(x_values, regions_L3, label='L=3', linewidth=2)\n\nax1.set_xlabel('Ancho de capa (n)')\nax1.set_ylabel('Número máximo de regiones lineales')\nax1.set_title('Número máximo de regiones lineales vs el ancho de capa')\nax1.legend()\nax1.grid(True)\nax1.set_yscale('log')  # Escala logarítmica para visualizar mejor el crecimiento exponencial\n\nax2.plot(params_L1, regions_L1, label='L=1', linewidth=2)\nax2.plot(params_L2, regions_L2, label='L=2', linewidth=2)\nax2.plot(params_L3, regions_L3, label='L=3', linewidth=2)\n\nax2.set_xlabel('Número de parámetros')\nax2.set_ylabel('Número máximo de regiones lineales')\nax2.set_title('Número de parámetros vs número de regiones lineales')\nax2.legend()\nax2.grid(True)\nax2.set_yscale('log')  # Escala logarítmica para visualizar mejor el crecimiento exponencial\n\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\n\nFigura 4: Crecimiento de la complejidad de una red con el número de parámetros\n\n\n\n\n\nEn la Figura 4 se muestra el crecimiento exponencial del número de regiones lineales con la profundidad de la red. Vemos que el crecimiento es exponencial con la profundidad de la red. Vemos también que a paridad de número de parámetros, una red profunda puede representar funciones con un número muy grande de regiones lineales (alta complejidad).\nEste crecimiento exponencial sugiere que las redes profundas (mayor \\(L\\)) pueden ser exponencialmente más eficientes en términos de representación que las redes superficiales (mayor \\(n_\\ell\\) pero \\(L=1\\) o \\(L=2\\)). Es decir, una red profunda podría representar funciones con un número muy grande de regiones lineales (alta complejidad) usando comparativamente menos parámetros que una red superficial que intente lograr la misma complejidad solo aumentando su ancho. Esta es una de las principales motivaciones teóricas para usar arquitecturas profundas.",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "11. Perceptrón, Redes Conexas y Pérdidas"
    ]
  },
  {
    "objectID": "11_perceptron_y_redes_conexas.html#derivación-general-desde-máxima-verosimilitud",
    "href": "11_perceptron_y_redes_conexas.html#derivación-general-desde-máxima-verosimilitud",
    "title": "El perceptrón, redes completamente conexas y pérdidas",
    "section": "Derivación General desde Máxima Verosimilitud",
    "text": "Derivación General desde Máxima Verosimilitud\nLa idea central de la estimación de máxima verosimilitud es encontrar los parámetros del modelo \\(\\boldsymbol{W}, \\boldsymbol{b}\\) que hacen que los datos observados sean lo más probables posible. Asumimos que tenemos un conjunto de datos de \\(N\\) muestras \\((\\mathbf{x}_i, y_i)\\), que son independientes e identicamente distribuidas (i.i.d.), y una distribución de probabilidad \\(p(y | \\mathbf{x}; \\boldsymbol{W}, \\boldsymbol{b})\\) que define la probabilidad de observar la salida \\(y\\) dada la entrada \\(\\mathbf{x}\\) y los parámetros \\(\\boldsymbol{W}, \\boldsymbol{b}\\).\nDebido a la independencia, la probabilidad conjunta de observar todas las \\(y_i\\) dadas las \\(\\mathbf{x}_i\\) es el producto de las probabilidades individuales: \\[\n\\mathcal{L}(\\boldsymbol{W}, \\boldsymbol{b}) \\equiv p(\\mathbf{y} | \\mathbf{X}; \\boldsymbol{W}, \\boldsymbol{b}) = \\prod_{i=1}^N p(y_i | \\mathbf{x}_i; )\\boldsymbol{W}, \\boldsymbol{b}\n\\] Queremos encontrar los \\(\\boldsymbol{W}, \\boldsymbol{b}\\) que maximizan \\(\\mathcal{L}(\\boldsymbol{W}, \\boldsymbol{b})\\).\nMaximizar \\(\\mathcal{L}(\\boldsymbol{W}, \\boldsymbol{b})\\) es equivalente a maximizar su logaritmo (ya que \\(\\log\\) es una función monotóna creciente). Esto simplifica los cálculos: \\[\n    \\ell(\\boldsymbol{W}, \\boldsymbol{b}) = \\log \\mathcal{L}(\\boldsymbol{W}, \\boldsymbol{b}) = \\log \\prod_{i=1}^N p(y_i | \\mathbf{x}_i; \\boldsymbol{W}, \\boldsymbol{b}) = \\sum_{i=1}^N \\log p(y_i | \\mathbf{x}_i; \\boldsymbol{W}, \\boldsymbol{b})\n    \\]\nLos algoritmos de optimización típicamente minimizan una función. Por lo tanto, definimos la función de pérdida \\(\\mathcal{J}(\\boldsymbol{W}, \\boldsymbol{b})\\) como la Log-Verosimilitud Negativa (NLL): \\[\n    \\mathcal{J}(\\boldsymbol{W}, \\boldsymbol{b}) = -\\ell(\\boldsymbol{W}, \\boldsymbol{b}) = -\\sum_{i=1}^N \\log p(y_i | \\mathbf{x}_i; \\boldsymbol{W}, \\boldsymbol{b})\n    \\] Minimizar \\(\\mathcal{J}(\\boldsymbol{W}, \\boldsymbol{b})\\) es equivalente a maximizar la verosimilitud \\(\\mathcal{L}(\\boldsymbol{W}, \\boldsymbol{b})\\). A menudo, se utiliza la pérdida promedio por muestra: \\(-\\frac{1}{N}\\sum_{i=1}^N \\log p(y_i | \\mathbf{x}_i; \\boldsymbol{W}, \\boldsymbol{b})\\).\nTrabajar con \\(\\ell(\\boldsymbol{W}, \\boldsymbol{b})\\) o \\(\\mathcal{J}(\\boldsymbol{W}, \\boldsymbol{b})\\) en lugar de \\(\\mathcal{L}(\\boldsymbol{W}, \\boldsymbol{b})\\) directamente tiene sus ventajas:\n\nEstabilidad Numérica: El producto de muchas probabilidades (que suelen ser &lt; 1) puede llevar a underflow numérico. La suma de logaritmos es mucho más estable.\nSimplicidad Matemática: Las derivadas del logaritmo suelen ser más manejables, facilitando la optimización basada en gradientes. Por ejemplo, el logaritmo convierte exponentes (como en la Gaussiana) en factores y multiplicaciones en sumas.\n\nAhora aplicaremos esta lógica para derivar algunas funciones de pérdida comunes.",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "11. Perceptrón, Redes Conexas y Pérdidas"
    ]
  },
  {
    "objectID": "11_perceptron_y_redes_conexas.html#entropía-cruzada-para-clasificación-cross-entropy-loss",
    "href": "11_perceptron_y_redes_conexas.html#entropía-cruzada-para-clasificación-cross-entropy-loss",
    "title": "El perceptrón, redes completamente conexas y pérdidas",
    "section": "Entropía Cruzada para Clasificación (Cross-Entropy Loss)",
    "text": "Entropía Cruzada para Clasificación (Cross-Entropy Loss)\nLa entropía cruzada surge naturalmente de la estimación de máxima verosimilitud para problemas de clasificación.\n\nClasificación Binaria:\n\nModelo Probabilístico: Asumimos que la etiqueta \\(y_i \\in \\{0, 1\\}\\) sigue una distribución de Bernoulli, donde la probabilidad de éxito (\\(y_i=1\\)) es la salida del modelo \\(\\hat{y}_i\\), típicamente obtenida aplicando una sigmoide a la salida lineal de la red: \\(\\hat{y}_i = \\sigma(z_i) = p(y_i=1 | \\mathbf{x}_i; \\boldsymbol{W}, \\boldsymbol{b})\\).\n\\(p(y_i | \\mathbf{x}_i; \\boldsymbol{W}, \\boldsymbol{b}) = (\\hat{y}_i)^{y_i} (1 - \\hat{y}_i)^{1-y_i}\\)\nPérdida (NLL por muestra): \\[\n  \\ell_i(\\boldsymbol{W}, \\boldsymbol{b}) = -\\log p(y_i | \\mathbf{x}_i; \\boldsymbol{W}, \\boldsymbol{b}) = -[y_i \\log \\hat{y}_i + (1-y_i) \\log(1 - \\hat{y}_i)]\n  \\] Esta es la pérdida de entropía cruzada binaria para la muestra \\(i\\). La pérdida total es \\(\\mathcal{J}(\\boldsymbol{W}, \\boldsymbol{b}) = \\sum_i \\ell_i(\\boldsymbol{W}, \\boldsymbol{b})\\) (o el promedio).\nClasificación Multiclase:\n\nModelo Probabilístico: Asumimos que la etiqueta \\(y_i\\) es un vector one-hot (e.g., \\([0, 1, 0]\\) si la clase verdadera es la 2 de entre 3 clases) y sigue una distribución Categórica (o Multinoulli). El modelo produce un vector de probabilidades \\(\\hat{\\mathbf{y}}_i = (\\hat{y}_{i,1}, ..., \\hat{y}_{i,C})\\) (típicamente usando Softmax) donde \\(\\hat{y}_{i,c} = p(y_{i,c}=1 | \\mathbf{x}_i; \\boldsymbol{W}, \\boldsymbol{b})\\). Donde \\(C\\) es el número de clases.\n\\(p(y_i | \\mathbf{x}_i; \\boldsymbol{W}, \\boldsymbol{b}) = \\prod_{c=1}^C (\\hat{y}_{i,c})^{y_{i,c}}\\) (donde \\(y_{i,c}\\) es 1 para la clase verdadera, 0 para las demás).\nPérdida (NLL por muestra): \\[\n  \\ell_i(\\boldsymbol{W}, \\boldsymbol{b}) = -\\log p(y_i | \\mathbf{x}_i; \\boldsymbol{W}, \\boldsymbol{b}) = -\\sum_{c=1}^C y_{i,c} \\log \\hat{y}_{i,c}\n  \\] Esta es la pérdida de entropía cruzada categórica para la muestra \\(i\\). La pérdida total es \\(\\mathcal{J}(\\boldsymbol{W}, \\boldsymbol{b}) = \\sum_i \\ell_i(\\boldsymbol{W}, \\boldsymbol{b})\\) (o el promedio).",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "11. Perceptrón, Redes Conexas y Pérdidas"
    ]
  },
  {
    "objectID": "11_perceptron_y_redes_conexas.html#error-cuadrático-medio-mse-para-regresión",
    "href": "11_perceptron_y_redes_conexas.html#error-cuadrático-medio-mse-para-regresión",
    "title": "El perceptrón, redes completamente conexas y pérdidas",
    "section": "Error Cuadrático Medio (MSE) para Regresión",
    "text": "Error Cuadrático Medio (MSE) para Regresión\nEl MSE es la pérdida estándar para regresión y también deriva de la estimación de máxima verosimilitud bajo la suposición de un ruido Gaussiano.\n\nModelo Probabilístico: Asumimos que la etiqueta verdadera \\(y_i\\) es igual a la predicción determinista del modelo \\(\\hat{y}_i = f(\\mathbf{x}_i; \\boldsymbol{W}, \\boldsymbol{b})\\) más un ruido Gaussiano \\(\\varepsilon_i\\) con media cero y varianza constante \\(\\sigma^2\\):\n\n\\(y_i = \\hat{y}_i + \\varepsilon_i\\), donde \\(\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\\).\nEsto implica que \\(y_i\\) sigue una distribución Gaussiana centrada en la predicción del modelo: \\(p(y_i | \\mathbf{x}_i; \\boldsymbol{W}, \\boldsymbol{b}) \\sim \\mathcal{N}(y_i | \\hat{y}_i, \\sigma^2)\\).\n\\(p(y_i | \\mathbf{x}_i; \\boldsymbol{W}, \\boldsymbol{b}) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\hat{y}_i)^2}{2\\sigma^2}\\right)\\)\n\nPérdida (NLL por muestra): \\[\n  \\ell_i(\\boldsymbol{W}, \\boldsymbol{b}) = -\\log p(y_i | \\mathbf{x}_i; \\boldsymbol{W}, \\boldsymbol{b}) = \\frac{(y_i - \\hat{y}_i)^2}{2\\sigma^2} + \\frac{1}{2}\\log(2\\pi\\sigma^2)\n  \\]\nSimplificación a MSE: Para encontrar el \\(\\boldsymbol{W}, \\boldsymbol{b}\\) óptimo, podemos ignorar los términos que no dependen de \\(\\boldsymbol{W}, \\boldsymbol{b}\\). Si asumimos \\(\\sigma^2\\) fija (o la estimamos por separado), minimizar la NLL es equivalente a minimizar: \\[\n\\sum_{i=1}^N (y_i - \\hat{y}_i)^2 \\quad \\text{o} \\quad \\frac{1}{N}\\sum_{i=1}^N (y_i - \\hat{y}_i)^2\n\\] Este último es el Error Cuadrático Medio (Mean Squared Error, MSE).",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "11. Perceptrón, Redes Conexas y Pérdidas"
    ]
  },
  {
    "objectID": "11_perceptron_y_redes_conexas.html#receta-general-para-construir-funciones-de-pérdida-usando-la-estimación-de-máxima-verosimilitud",
    "href": "11_perceptron_y_redes_conexas.html#receta-general-para-construir-funciones-de-pérdida-usando-la-estimación-de-máxima-verosimilitud",
    "title": "El perceptrón, redes completamente conexas y pérdidas",
    "section": "Receta General para Construir Funciones de Pérdida usando la estimación de máxima verosimilitud",
    "text": "Receta General para Construir Funciones de Pérdida usando la estimación de máxima verosimilitud\nPuedes diseñar funciones de pérdida adaptadas a problemas específicos siguiendo estos pasos:\n\nElegir un Modelo Probabilístico: Define \\(p(y | \\mathbf{x}; \\boldsymbol{W}, \\boldsymbol{b})\\) que capture las características de tus datos y el proceso de generación (e.g., tipo de ruido, distribución de la salida).\nEscribir la Verosimilitud: Asumiendo datos i.i.d., la verosimilitud es \\(\\mathcal{L}(\\boldsymbol{W}, \\boldsymbol{b}) = \\prod_i p(y_i | \\mathbf{x}_i; \\boldsymbol{W}, \\boldsymbol{b})\\).\nCalcular la Negativa Log-Verosimilitud (NLL): \\(\\mathcal{J}(\\boldsymbol{W}, \\boldsymbol{b}) = -\\sum_i \\log p(y_i | \\mathbf{x}_i; \\boldsymbol{W}, \\boldsymbol{b})\\). Esta es tu función de pérdida base.\nSimplificar (Opcional): Elimina términos aditivos o factores multiplicativos constantes que no dependan de \\(\\boldsymbol{W}, \\boldsymbol{b}\\), ya que no afectan la ubicación del mínimo.\nAñadir Regularización (Opcional): Para prevenir el sobreajuste o incorporar conocimiento previo, puedes añadir un término de penalización sobre los parámetros \\(\\boldsymbol{W}, \\boldsymbol{b}\\): \\[\n\\mathcal{J}_{\\text{reg}}(\\boldsymbol{W}, \\boldsymbol{b}) = \\mathcal{J}_{\\text{NLL}}(\\boldsymbol{W}, \\boldsymbol{b}) + \\lambda R(\\boldsymbol{W}, \\boldsymbol{b})\n\\] Donde \\(R(\\boldsymbol{W}, \\boldsymbol{b})\\) es el regularizador (e.g., \\(R(\\boldsymbol{W}, \\boldsymbol{b}) = \\|\\boldsymbol{W}\\|^2 + \\|\\boldsymbol{b}\\|^2\\) para regularización L2/Ridge, o \\(R(\\boldsymbol{W}, \\boldsymbol{b}) = \\|\\dots\\|_1\\) para L1/LASSO) y \\(\\lambda &gt; 0\\) es el hiperparámetro de regularización.",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "11. Perceptrón, Redes Conexas y Pérdidas"
    ]
  },
  {
    "objectID": "5_testeo_de_hipotesis_entropia.html",
    "href": "5_testeo_de_hipotesis_entropia.html",
    "title": "Testeo de Hipótesis y Entropía",
    "section": "",
    "text": "Normalmente tenemos una pregunta y queremos usar los datos disponibles para responderla. Ya vimos cómo podemos responder a la pregunta de cuál es el valor de algún parámetro: Usamos un estimador y reportamos un intervalo de confianza.\nHay muchas otras preguntas que podemos responder. Una clase grande de preguntas se pueden formular como hipótesis: Suponemos algo sobre los datos y queremos intentar confirmar o descartar la hipótesis.\nNormalmente sólo está bien definido cómo rechazar una hipótesis. Por esto, en vez de aceptar una hipótesis lo que hacemos es compararla con una hipótesis nula. Por ejemplo queremos saber si detectamos una nueva partícula, lo que hacemos es formular dos hipótesis:\n\nHipótesis nula (\\(H_0\\)): No hay una nueva partícula, lo que vimos es producido por el ruido del detector.\nHipótesis alternativa (\\(H_a\\)): Se detectó una nueva partícula.\n\nLo que hacemos entonces es comparar \\(H_0\\) con \\(H_a\\). Decimos que detectamos la nueva partícula cuando rechazamos la hipótesis nula.\nDe aquí vienen frases tan enredadas como: “No tenemos suficientes datos para rechazar la hipótesis nula”. Esas tres negaciones la hacen una frase muy confusa. Lo que quiere decir esto es que la hipótesis nula es compatible con lo que se observó, pero tal vez con más mediciones logremos rechazarla. En este caso no podemos concluir que vimos algún efecto.\nNormalmente rechazamos una hipótesis si las mediciones caen en una región de baja probabilidad para esa hipótesis. Llamemos \\(R_a\\) esa tal región. Puede ser por ejemplo un rango de valores para la curvatura del universo, o para la masa de una partícula.\n\n\n\n\n\n\nNota 1: Ejemplo\n\n\n\nConsidere que usted lanza una moneda \\(10\\) veces. Quiere saber si la moneda está cargada o es equilibrada.\nLa hipótesis nula es que la moneda no está cargada, es decir que la probabilidad de sacar cara es \\(p=1/2\\).\nLa hipótesis alternativa es que la moneda está cargada, \\(p\\neq 1/2\\).\nPara diferentes resultados obtenemos las siguientes probabildades bajo la hipótesis nula \\[\n\\begin{gather}\nP(10) = P(0) \\approx 0.001\\,,\\quad P(9) = P(1) \\approx 0.010\\,,\\quad P(8) = P(2) \\approx 0.04\\,, \\\\ P(7) = P(3) \\approx 0.117\\,,\\quad P(6) = P(4) \\approx 0.205\\,,\\quad P(5) \\approx 0.246\\,.\n\\end{gather}\n\\]\nSupongamos que obtenemos \\(8\\) caras. La probabilidad de obtener un resultado así de extremo o peor es \\(\\approx 5.1\\%\\). Si fijamos el nivel al cual descartamos la hipótesis nula al \\(5\\%\\) (un nivel muy común por fuera de la física), no podemos aún descartarla. Podemos entonces lanzar más veces la moneda para ver qué ocurre.\n\n\n\n\n\n\nHipótesis simple: Especifica completamente la distribución de probabilidad, sin dejar parámetros libres. Por ejemplo: \\(\\mu = 5\\) para una distribución normal con varianza conocida. Raras veces se puede formular una hipótesis de esta manera, pero es útil para entender cómo testear hipótesis.\nHipótesis compuesta: No especifica completamente la distribución de probabilidad. Por ejemplo: \\(\\mu &lt; 5\\) para una distribución gaussiana con varianza conocida. Otro ejemplo: Los parámetros medidos son compatibles con mi modelo físico.\n\n\n\n\nCuando ponemos una hipótesis a prueba, hay cuatro resultados posibles.\n\nLa hipótesis es verdadera y la aceptamos.\nLa hipótesis es falsa y la rechazamos.\nLa hipótesis es verdadera y la rechazamos. Esto se llama error de tipo I o falso negativo.\nLa hipótesis es falsa y la aceptamos. Esto se llama error de tipo II o falso positivo.\n\nLa probabilidad de cometer errores de tipo I se llama significancia, normalmente denotada \\(\\alpha\\). Al comparar una hipótesis nula con una alternativa, \\(\\alpha\\) es la probabilidad de rechazar la hipótesis nula siendo esta verdadera \\(\\alpha = P[R_a | H_0]\\).\nLa probabilidad de rechazar correctamente una hipótesis falsa se llama la potencia, denotada por \\(\\beta\\). Al comparar una hipótesis nula con una alternativa, \\(\\beta\\) es la probabilidad de rechazar la hipótesis nula, siendo esta falsa \\(\\beta = P[R_a | H_0]\\). Entonces la probabilidad de cometer un error de tipo II es \\(1 - P[R_a | H_0] = 1 - \\beta\\).\nEn un mundo ideal nos gustaría reducir los errores de ambos tipos, es decir \\(\\alpha\\) pequeño y \\(\\beta\\) alto. Pero normalmente reducir los de tipo I aumenta los de tipo II y vice versa. Ilustremos con un par de imágenes:\n\n\n\n\n\n\n\n\nFigura 1: Baja probabilidad de rechazar equivocadamente la hipótesis nula.\n\n\n\n\n\nEn la Figura 1 supongamos que la región de exclusión es a la derecha de la barra negra. Si la hipótesis nula es verdadera, la distribución de probabilidad es la azul. Si la alternativa es verdadera, la distribución de probabilidad es la roja. Vemos que escoger esa región de exclusión nos da una probablidad muy chiquita de excluir la hipótesis nula en caso de que sea verdadera (área bajo la curva azul a la derecha de la línea). Es decir, es poco probable que rechacemos la hipótesis nula por error. Pero por otro lado si la hipótesis alternativa es verdadera, tenemos una probabilidad relativamente alta de aceptar la hipótesis nula (área bajo la curva roja a la izquierda de la línea). Es decir que es bastante probable que por error no rechacemos la hipótesis nula. En este caso vemos que el criterio no es muy potente: No nos permitiría detectar un nuevo fenómeno.\n\n\n\n\n\n\n\n\nFigura 2: Baja probabilidad de no rechazar equivocadamente \\(H_0\\).\n\n\n\n\n\nEn la Figura 2 tenemos el ejemplo contrario. Ahora la probabilidad de no rechazar erróneamente la hipótesis nula es baja. Es decir, podemos “detectar” más fácilmente el nuevo fenómeno, pero si la hipótesis nula es la correcta, corremos más riesgo de rechazarla.",
    "crumbs": [
      "Home",
      "Probabilidad y Estadística",
      "5. Testeo de Hipótesis y Entropía"
    ]
  },
  {
    "objectID": "5_testeo_de_hipotesis_entropia.html#hipótesis-estadísticas",
    "href": "5_testeo_de_hipotesis_entropia.html#hipótesis-estadísticas",
    "title": "Testeo de Hipótesis y Entropía",
    "section": "",
    "text": "Normalmente tenemos una pregunta y queremos usar los datos disponibles para responderla. Ya vimos cómo podemos responder a la pregunta de cuál es el valor de algún parámetro: Usamos un estimador y reportamos un intervalo de confianza.\nHay muchas otras preguntas que podemos responder. Una clase grande de preguntas se pueden formular como hipótesis: Suponemos algo sobre los datos y queremos intentar confirmar o descartar la hipótesis.\nNormalmente sólo está bien definido cómo rechazar una hipótesis. Por esto, en vez de aceptar una hipótesis lo que hacemos es compararla con una hipótesis nula. Por ejemplo queremos saber si detectamos una nueva partícula, lo que hacemos es formular dos hipótesis:\n\nHipótesis nula (\\(H_0\\)): No hay una nueva partícula, lo que vimos es producido por el ruido del detector.\nHipótesis alternativa (\\(H_a\\)): Se detectó una nueva partícula.\n\nLo que hacemos entonces es comparar \\(H_0\\) con \\(H_a\\). Decimos que detectamos la nueva partícula cuando rechazamos la hipótesis nula.\nDe aquí vienen frases tan enredadas como: “No tenemos suficientes datos para rechazar la hipótesis nula”. Esas tres negaciones la hacen una frase muy confusa. Lo que quiere decir esto es que la hipótesis nula es compatible con lo que se observó, pero tal vez con más mediciones logremos rechazarla. En este caso no podemos concluir que vimos algún efecto.\nNormalmente rechazamos una hipótesis si las mediciones caen en una región de baja probabilidad para esa hipótesis. Llamemos \\(R_a\\) esa tal región. Puede ser por ejemplo un rango de valores para la curvatura del universo, o para la masa de una partícula.\n\n\n\n\n\n\nNota 1: Ejemplo\n\n\n\nConsidere que usted lanza una moneda \\(10\\) veces. Quiere saber si la moneda está cargada o es equilibrada.\nLa hipótesis nula es que la moneda no está cargada, es decir que la probabilidad de sacar cara es \\(p=1/2\\).\nLa hipótesis alternativa es que la moneda está cargada, \\(p\\neq 1/2\\).\nPara diferentes resultados obtenemos las siguientes probabildades bajo la hipótesis nula \\[\n\\begin{gather}\nP(10) = P(0) \\approx 0.001\\,,\\quad P(9) = P(1) \\approx 0.010\\,,\\quad P(8) = P(2) \\approx 0.04\\,, \\\\ P(7) = P(3) \\approx 0.117\\,,\\quad P(6) = P(4) \\approx 0.205\\,,\\quad P(5) \\approx 0.246\\,.\n\\end{gather}\n\\]\nSupongamos que obtenemos \\(8\\) caras. La probabilidad de obtener un resultado así de extremo o peor es \\(\\approx 5.1\\%\\). Si fijamos el nivel al cual descartamos la hipótesis nula al \\(5\\%\\) (un nivel muy común por fuera de la física), no podemos aún descartarla. Podemos entonces lanzar más veces la moneda para ver qué ocurre.",
    "crumbs": [
      "Home",
      "Probabilidad y Estadística",
      "5. Testeo de Hipótesis y Entropía"
    ]
  },
  {
    "objectID": "5_testeo_de_hipotesis_entropia.html#tipos-de-hipótesis",
    "href": "5_testeo_de_hipotesis_entropia.html#tipos-de-hipótesis",
    "title": "Testeo de Hipótesis y Entropía",
    "section": "",
    "text": "Hipótesis simple: Especifica completamente la distribución de probabilidad, sin dejar parámetros libres. Por ejemplo: \\(\\mu = 5\\) para una distribución normal con varianza conocida. Raras veces se puede formular una hipótesis de esta manera, pero es útil para entender cómo testear hipótesis.\nHipótesis compuesta: No especifica completamente la distribución de probabilidad. Por ejemplo: \\(\\mu &lt; 5\\) para una distribución gaussiana con varianza conocida. Otro ejemplo: Los parámetros medidos son compatibles con mi modelo físico.",
    "crumbs": [
      "Home",
      "Probabilidad y Estadística",
      "5. Testeo de Hipótesis y Entropía"
    ]
  },
  {
    "objectID": "5_testeo_de_hipotesis_entropia.html#tipos-de-errores",
    "href": "5_testeo_de_hipotesis_entropia.html#tipos-de-errores",
    "title": "Testeo de Hipótesis y Entropía",
    "section": "",
    "text": "Cuando ponemos una hipótesis a prueba, hay cuatro resultados posibles.\n\nLa hipótesis es verdadera y la aceptamos.\nLa hipótesis es falsa y la rechazamos.\nLa hipótesis es verdadera y la rechazamos. Esto se llama error de tipo I o falso negativo.\nLa hipótesis es falsa y la aceptamos. Esto se llama error de tipo II o falso positivo.\n\nLa probabilidad de cometer errores de tipo I se llama significancia, normalmente denotada \\(\\alpha\\). Al comparar una hipótesis nula con una alternativa, \\(\\alpha\\) es la probabilidad de rechazar la hipótesis nula siendo esta verdadera \\(\\alpha = P[R_a | H_0]\\).\nLa probabilidad de rechazar correctamente una hipótesis falsa se llama la potencia, denotada por \\(\\beta\\). Al comparar una hipótesis nula con una alternativa, \\(\\beta\\) es la probabilidad de rechazar la hipótesis nula, siendo esta falsa \\(\\beta = P[R_a | H_0]\\). Entonces la probabilidad de cometer un error de tipo II es \\(1 - P[R_a | H_0] = 1 - \\beta\\).\nEn un mundo ideal nos gustaría reducir los errores de ambos tipos, es decir \\(\\alpha\\) pequeño y \\(\\beta\\) alto. Pero normalmente reducir los de tipo I aumenta los de tipo II y vice versa. Ilustremos con un par de imágenes:\n\n\n\n\n\n\n\n\nFigura 1: Baja probabilidad de rechazar equivocadamente la hipótesis nula.\n\n\n\n\n\nEn la Figura 1 supongamos que la región de exclusión es a la derecha de la barra negra. Si la hipótesis nula es verdadera, la distribución de probabilidad es la azul. Si la alternativa es verdadera, la distribución de probabilidad es la roja. Vemos que escoger esa región de exclusión nos da una probablidad muy chiquita de excluir la hipótesis nula en caso de que sea verdadera (área bajo la curva azul a la derecha de la línea). Es decir, es poco probable que rechacemos la hipótesis nula por error. Pero por otro lado si la hipótesis alternativa es verdadera, tenemos una probabilidad relativamente alta de aceptar la hipótesis nula (área bajo la curva roja a la izquierda de la línea). Es decir que es bastante probable que por error no rechacemos la hipótesis nula. En este caso vemos que el criterio no es muy potente: No nos permitiría detectar un nuevo fenómeno.\n\n\n\n\n\n\n\n\nFigura 2: Baja probabilidad de no rechazar equivocadamente \\(H_0\\).\n\n\n\n\n\nEn la Figura 2 tenemos el ejemplo contrario. Ahora la probabilidad de no rechazar erróneamente la hipótesis nula es baja. Es decir, podemos “detectar” más fácilmente el nuevo fenómeno, pero si la hipótesis nula es la correcta, corremos más riesgo de rechazarla.",
    "crumbs": [
      "Home",
      "Probabilidad y Estadística",
      "5. Testeo de Hipótesis y Entropía"
    ]
  },
  {
    "objectID": "5_testeo_de_hipotesis_entropia.html#dos-hipótesis-simples",
    "href": "5_testeo_de_hipotesis_entropia.html#dos-hipótesis-simples",
    "title": "Testeo de Hipótesis y Entropía",
    "section": "Dos hipótesis simples",
    "text": "Dos hipótesis simples\nSupongamos que la hipótesis nula es que algún conjunto de parámetros que determinan la distribución de probabilidad toma el valor \\(\\boldsymbol{\\theta}_0\\) y la alternativa que toman el valor \\(\\boldsymbol{\\theta}_a\\). En este caso existe un test óptimo. Escogemos la región de exclusión como todos aquellos puntos para los cuales \\[\n\\lambda \\equiv \\frac{L(\\boldsymbol{\\theta}_0)}{L(\\boldsymbol{\\theta}_a)}\n\\] toma un valor menor que algún valor crítico \\(\\lambda\\).\nDada una sensitividad \\(\\alpha\\) fija, este test es el que tiene menor \\(\\beta\\). Es decir, es el test más potente para una dada sensitividad.\n\n\n\n\n\n\nLema de Neyman-Pearson\n\n\n\nLa región de exclusión \\(R_k\\) de todos los puntos tal que \\(\\lambda &lt; k\\) tal que \\(\\alpha = \\int_{R_k} d^nx\\,L(\\theta_0)\\) es la que maximiza \\(\\beta = \\int_{R_k}d^nx\\,L(\\theta_a)\\).\n\n\n\n\n\n\n\n\nDemostración (no entra en la prueba)\n\n\n\n\n\nComo \\(R_k\\) está definida tal que \\(L(\\theta_0)/L(\\theta_a) &lt; k\\), tenemos que \\[\n\\int_{R_k}d^nx\\,L(\\theta_a) &gt; \\frac{1}{k}\\int_{R_k}d^nx\\,L(\\theta_0) = \\alpha\\,.\n\\] Ahora lo comparamos con cualquier otra región \\(R_\\alpha\\) con la mismo \\(\\alpha\\), tenemos \\[\n\\int_{R_k}d^nx\\,L(\\theta_a) &gt; \\frac{1}{k}\\int_{R_\\alpha}d^nx\\,L(\\theta_0)\\,.\n\\tag{1}\\] Por definición esa región tendrá puntos en los cuales \\(\\lambda = L(\\theta_0)/L(\\theta_a) &gt; k\\) mientras que los puntos donde \\(\\lambda &lt; k\\) coinciden con algunos puntos de \\(R_k\\), por lo tanto \\[\n\\frac{1}{k}\\int_{R_\\alpha}d^nx\\,L(\\theta_0) &gt; \\int_{R_\\alpha}d^nx\\,L(\\theta_a)\\,.\n\\tag{2}\\] Combinando las desigualdades 1 y 2 tenemos \\[\n\\int_{R_k}d^nx L(\\theta_a) &gt; \\int_{R_\\alpha}d^nx L(\\theta_a)\\,.\n\\]\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\nLa densidad de una cierta piedra preciosa es \\(2.2\\,\\text{g}/\\text{cm}^3\\) mientras la densidad de cuarzo es \\(2.6\\,\\text{g}/\\text{cm}^3\\). Suponga que usted tiene un instrumento de medición con una resolución de \\(0.2\\,\\text{g}/\\text{cm}^3\\). Calculemos cuándo vale la pena explorar una muestra dada para encontrar la piedra preciosa.\nSi los errores son gaussianos, la razón entre verosimilitudes es \\[\n\\frac{e^{-(x - 2.6)^2/(2\\times0.2^2))}}{e^{-(x - 2.2)^2/(2\\times0.2^2))}} \\propto e^{10 x}\\,.\n\\] Esta es una función creciente de \\(x\\), tal que el test consiste en escoger un valor de corte para \\(x\\). En este caso si estamos dispuestos a explorar erróneamente \\(\\alpha = 5\\%\\) de las muestras de cuarzo podemos poner el corte a \\(\\approx 1.64\\sigma\\) de la media para el cuarzo, es decir a \\(x = 2.53\\,\\text{g}/\\text{cm}^3\\), lo que implicaría descartar \\(\\beta = 50\\%\\) de las muestras genuinas de piedra preciosa. Según el lema, no podemos hacer algo mejor que eso para el \\(\\alpha\\) dado.",
    "crumbs": [
      "Home",
      "Probabilidad y Estadística",
      "5. Testeo de Hipótesis y Entropía"
    ]
  },
  {
    "objectID": "5_testeo_de_hipotesis_entropia.html#hipótesis-compuestas",
    "href": "5_testeo_de_hipotesis_entropia.html#hipótesis-compuestas",
    "title": "Testeo de Hipótesis y Entropía",
    "section": "Hipótesis compuestas",
    "text": "Hipótesis compuestas\nCuando las hipótesis son compuestas no tenemos un test con las propiedades del de Neyman-Pearson. Sin embargo podemos definir algo análogo usando la máxima verosimilitud. Es decir, tomamos \\(\\lambda = L(\\hat{R}_0)/L(\\hat{S})\\) donde \\(L(\\hat{R}_0)\\) es el valor máximo que toma la verosimilitud en la región que respeta la hipótesis nula, y \\(L(\\hat{S})\\) es el valor máximo que toma la verosimilitud en la unión de las regiones de la hipótesis nula y la alternativa.\nEn la práctica hacer esto nos da un test que es razonablemente potente. Además tiene buenas propiedades cuando el número de datos \\(n\\) es grande. En particular, suponga que la hipótesis nula es que los \\(k\\) parámetros de un modelo toman valores \\(\\boldsymbol{\\theta} = \\boldsymbol{\\theta}_0\\) y la hipótesis alternativa es simplemente \\(\\boldsymbol{\\theta} \\neq \\boldsymbol{\\theta}_0\\). Entonces si \\(H_0\\) es verdadera, la variable \\(-2\\ln\\lambda\\) tiene una distribución aproximadamente \\(\\chi^2\\) con \\(k\\) grados de libertad para \\(n\\) grande.\nLa probabilidad de obtener un valor observado de los datos bajo la hipótesis nula igual o más extremo que un cierto nivel \\(p\\) se lo llama el “valor-\\(p\\)” (\\(p\\)-value). Este es igual a \\(\\alpha\\) cuando se toma la región de exclusión como la región en las colas de la distribución de probabilidad.",
    "crumbs": [
      "Home",
      "Probabilidad y Estadística",
      "5. Testeo de Hipótesis y Entropía"
    ]
  },
  {
    "objectID": "5_testeo_de_hipotesis_entropia.html#el-teorema-original",
    "href": "5_testeo_de_hipotesis_entropia.html#el-teorema-original",
    "title": "Testeo de Hipótesis y Entropía",
    "section": "El teorema original",
    "text": "El teorema original\nShannon era un informático (más apropiadamente un científico de la información) que introdujo su concepto de entropía para demostrar un teorema sobre comunicaciones en redes.\nLa pregunta es cuánto podemos comprimir la información producida por un computador para enviarla de manera eficiente a otro computador. Todos hemos usado archivos .zip. La pregunta es cuánto podemos comprimir una serie de bits.\nDefinimos la compresión como un mapa \\(C^n\\) entre las posibles secuencias \\((x_1,...,x_n)\\) y una cadena de bits de longitud \\(nR\\). Entonces la razón de compresión de este mapa es \\(R\\).\nLa descompresión del mensjae es un mapa \\(D^n\\) que toma una cadena de bits de longitud \\(nR\\) y produce \\((x_1,...,x_n)\\).\nUn mecanismo de compresión y descompresión es confiable si \\(D^n(C^n(x)) = x\\) con probabilidad \\(1\\) cuando \\(n\\) tiende a infinito.\nEl teorema dice (asumiendo que el logaritmo en la definición de \\(H\\) es base \\(2\\))\n\n\n\n\n\n\nTeorema del canal sin ruido de Shannon\n\n\n\nSuponga que \\(X_i\\) es una serie de variables i.i.d (independientes e idénticamente distribuidas) sacadas de una distribución discreta de probabilidad con entropía \\(H(X)\\). Sea \\(R &gt; H\\), entonces existe un mecanismo de compresión confiable con razón de compresión \\(R\\). Por el contrario sea \\(R &lt; H\\), entonces no existe un tal mecanismo confiable.\n\n\nLamentablemente no tenemos el tiempo necesario para introducir los conceptos previos necesarios para demostrar este teorema.",
    "crumbs": [
      "Home",
      "Probabilidad y Estadística",
      "5. Testeo de Hipótesis y Entropía"
    ]
  },
  {
    "objectID": "5_testeo_de_hipotesis_entropia.html#un-par-de-consideraciones-intuitivas",
    "href": "5_testeo_de_hipotesis_entropia.html#un-par-de-consideraciones-intuitivas",
    "title": "Testeo de Hipótesis y Entropía",
    "section": "Un par de consideraciones intuitivas",
    "text": "Un par de consideraciones intuitivas\nConsideremos una fuente que sólo produce la letra “c”. Esta fuente no contiene alguna información, no necesitamos comprimir nada.\nAhora supongamos que unos días la fuente produce la palabra “azul” y otros días la fuente produce la palabra “rojo”. En este caso podemos representar la palabra “azul” con 0 y la palabra rojo con “1”. De hecho la entropía es \\(1\\), necesitamos un solo bit.\nEn el otro extremo, supongamos que la fuente produce una cadena completamente aleatoria de “1” y “0” de longitud \\(n\\). No tenemos manera de comprimirla si es completamente aleatoria, la entropía es \\[\nH = -n\\sum_{i=0}^1 \\frac{1}{2}\\log\\frac{1}{2} = n\\,,\n\\] y necesitamos todos los \\(n\\) bits. Esta cadena tiene la máxima cantidad de información, no se puede comprimir.\nAquí vemos que la estadística de la fuente es importante, por eso la información está relacionada con la estadísitica.\nAhora pensemos en una fuente que transmite un mensaje en español. Resulta que los 33 caracteres del español no ocurren con igual probabilidad, las vocales son mucho más probables. Igualmente para las palabras, algunas palabras son más comunes. Idem para las parejas de palabras o frases.\nPara acercarnos a ese caso, ahora pensemos en una cadena de “0” y “1” tal que el “1” aparece con probabilidad \\(0.8\\). Una tal cadena es \\[\n111101110101111\n\\] Parecería que no podemos comprimir esta cadena, después de todo para el cero necesitamos el símbolo “0” y para el uno necesitamos el símbolo “1”. Esto parecería contradecir la intuición que hemos construído de entropía \\[\nH = -n\\left(0.8\\log 0.8 + 0.2 \\log 0.2\\right) \\approx 0.5n\\,.\n\\] Pero en realidad podemos tomar cadenas de caracteres, por ejemplo de cinco caracteres. Las cinco cadenas “11110”, “11101”, “11011”, “10111”, “01111” ocurren con mucha más frecuencia que las otras, tal que podemos usar un par de bits para representarlas en vez de cinco. Jugando de esta manera nos podemos acercar a comprimir esos mensajes por un \\(50\\%\\).",
    "crumbs": [
      "Home",
      "Probabilidad y Estadística",
      "5. Testeo de Hipótesis y Entropía"
    ]
  },
  {
    "objectID": "5_testeo_de_hipotesis_entropia.html#un-ejemplo-de-compresión",
    "href": "5_testeo_de_hipotesis_entropia.html#un-ejemplo-de-compresión",
    "title": "Testeo de Hipótesis y Entropía",
    "section": "Un ejemplo de compresión",
    "text": "Un ejemplo de compresión\nAhora supongamos que tenemos un alfabeto de 8 caracteres que queremos comprimir, llamémoslos \\(\\{1,2,3,4,5,6,7,8\\}\\). De forma ingenua podemos representar cada caracter con 3 bits. ¿Podemos comprimirlo?\nSi los 8 caracteres ocurren con igual probabilidad \\(1/8\\) no podemos hacer mucho \\[\nH = -n\\sum \\frac{1}{8}\\log\\frac{1}{8} = n\\log 8 = 3n\\,.\n\\] Es decir, necesitamos \\(3n\\) bits para representar un mensaje de longitud \\(n\\).\nPero si ocurren con probabilidad diferente, podemos hacer algo mejor. Supongamos que las probabilidades son \\[\n\\begin{multline}\n\\{p_1 = 0.5, p_2 = 0.3, p_3 = 0.1, p_4 = 0.05, \\\\ p_5 = 0.025, p_6 = 0.0125, p_7 = 0.0065, p_8 = 0.006\\}\\,.\n\\end{multline}\n\\] Entonces un esquema debido a Fano consiste en:\n\nOrdenar las probabilidades de forma decreciente (como ya hemos hecho)\nDividir en dos conjuntos que tengan aproximadamente la misma probabilidad. Para nosotros serán \\[\n\\{0.5\\}\\,,\\quad \\{0.3, 0.1, 0.05, 0.025, 0.0125, 0.0065, 0.006\\}\\,.\n\\]\nLos caracteres del primer conjunto se representan con el dígito \\(0\\), los del segundo con el dígito \\(1\\). \\[\n\\begin{multline}\nr(0) = 1\\,,\\quad r(1) = \\{p_2 = 0.3, p_3 = 0.1, p_4 = 0.05, \\\\ p_5 = 0.025, p_6 = 0.0125, p_7 = 0.0065, p_8 = 0.006\\}\\,,\n\\end{multline}\n\\]\nRepetimos hasta terminar \\[\n\\begin{multline}\nr(0) = 1\\,,\\quad r(10) = 2\\,,\\quad r(11) = \\{p_3 = 0.1, p_4 = 0.05, \\\\p_5 = 0.025, p_6 = 0.0125, p_7 = 0.0065, p_8 = 0.006\\}\\,,\n\\end{multline}\n\\] \\[\n\\begin{multline}\nr(0) = 1\\,,\\quad r(10) = 2\\,,\\quad r(110) = \\{p_3 = 0.1, p_4 = 0.05\\}\\,,\\\\ \\quad r(111) = \\{p_5 = 0.025, p_6 = 0.0125, p_7 = 0.0065, p_8 = 0.006\\}\\,,\n\\end{multline}\n\\] \\[\n\\begin{multline}\nr(0) = 1\\,,\\quad r(10) = 2\\,,\\quad r(1100) = 3\\,,\\quad r(1101)= 4\\,,\\\\ \\quad r(1110) = 5\\,,\\quad r(1111) = \\{p_6 = 0.0125, p_7 = 0.0065, p_8 = 0.006\\}\\,,\n\\end{multline}\n\\] \\[\n\\begin{multline}\nr(0) = 1\\,,\\quad r(10) = 2\\,,\\quad r(1100) = 3\\,,\\quad r(1101)= 4\\,,\\\\ \\quad r(1110) = 5\\,,\\quad r(11110) = 6\\,,\\quad r(11111) = \\{p_7 = 0.0065, p_8 = 0.006\\}\\,,\n\\end{multline}\n\\] \\[\n\\begin{multline}\nr(0) = 1\\,,\\quad r(10) = 2\\,,\\quad r(1100) = 3\\,,\\quad r(1101)= 4\\,,\\\\ \\quad r(1110) = 5\\,,\\quad r(11110) = 6\\,,\\quad r(111110) = 7\\,,\\quad r(111111) = 8\\,.\n\\end{multline}\n\\] Es verdad que los símbolos menos probables son representados por más de 3 bits. Pero en promedio un mensaje tendrá longitud\n\n\n0.5*1+0.3*2+0.1*3+0.05*3+0.025*4+0.0125*5+0.0065*6+0.006*6\n\n1.7875000000000003\n\n\nComparemos con el valor de la entropía y veamos que se acerca.\n\n-0.5*np.log(0.5)-0.3*np.log(0.3)-0.1*np.log(0.1)-0.05*np.log(0.05)\\\n    -0.025*np.log(0.025)-0.0125*np.log(0.0125)\\\n    -0.0065*np.log(0.0065)-0.006*np.log(0.006)\n\nnp.float64(1.2982375438631775)\n\n\nEn general, aplicando un esquema como el que vimos la longitud de cada caracter será aproximadamente \\(-\\lceil\\log p\\rceil\\) y como este ocurre con frecuencia \\(p\\), la longitud promedio de un mensaje en bits será cercana a la entropía \\[\n-n\\sum_i p_i\\lceil\\log p_i\\rceil\\,.\n\\]",
    "crumbs": [
      "Home",
      "Probabilidad y Estadística",
      "5. Testeo de Hipótesis y Entropía"
    ]
  },
  {
    "objectID": "15_redes_residuales.html",
    "href": "15_redes_residuales.html",
    "title": "Redes Residuales",
    "section": "",
    "text": "Comentamos en la clase anterior que las diferentes capas de una red neuronal aprenden características de diferentes escalas. Por ejemplo, la primera capa puede aprender características simples como bordes, la segunda capa puede aprender características más complejas como formas, y así sucesivamente. Se esperaba entonces que redes más profundas pudieran aprender funciones más complejas y representaciones más ricas. Se observó este comportamiento cuando AlexNet (8 capas) fue superado por VGG (16-19 capas). Sin embargo, cuando se intentó entrenar redes más profundas, se observó que el error de entrenamiento aumentaba con la profundidad. Un aumento en el error del conjunto de prueba habría indicado un simple sobreajuste, pero el hecho que el error de entrenamiento aumentara con la profundidad indicaba que el modelo más profundo era más difícil de optimizar, no necesariamente que esté sobreajustando.",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "15. Redes Residuales"
    ]
  },
  {
    "objectID": "15_redes_residuales.html#gradientes-fragmentados-shattered-gradients",
    "href": "15_redes_residuales.html#gradientes-fragmentados-shattered-gradients",
    "title": "Redes Residuales",
    "section": "Gradientes fragmentados (shattered gradients)",
    "text": "Gradientes fragmentados (shattered gradients)\nParece que este fenómeno aún no se entiende del todo bien. Una hipótesis es que los gradientes respecto a los parámetros de las primeras capas cambian muy rápidamente. Como el entrenamiento usa una aproximación que asume que los gradientes son suaves, esto induce un error grande.\nPara verlo, seguimos un ejemplo tomado del libro (donde he cambiado la inicialización de Glorot a He).\n\n\nCódigo\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# K is width, D is number of hidden units in each layer\ndef init_params(K, D):\n  # Set seed so we always get the same random numbers\n  np.random.seed(1)\n\n  # Input layer\n  D_i = 1\n  # Output layer\n  D_o = 1\n\n  # He initialization\n  sigma_sq_omega = 2.0/D\n\n  # Make empty lists\n  all_weights = [None] * (K+1)\n  all_biases = [None] * (K+1)\n\n  # Create parameters for input and output layers\n  all_weights[0] = np.random.normal(size=(D, D_i))*np.sqrt(sigma_sq_omega)\n  all_weights[-1] = np.random.normal(size=(D_o, D)) * np.sqrt(sigma_sq_omega)\n  all_biases[0] = np.random.normal(size=(D,1))* np.sqrt(sigma_sq_omega)\n  all_biases[-1]= np.random.normal(size=(D_o,1))* np.sqrt(sigma_sq_omega)\n\n  # Create intermediate layers\n  for layer in range(1,K):\n    all_weights[layer] = np.random.normal(size=(D,D))*np.sqrt(sigma_sq_omega)\n    all_biases[layer] = np.random.normal(size=(D,1))* np.sqrt(sigma_sq_omega)\n\n  return all_weights, all_biases\n\n# Define the Rectified Linear Unit (ReLU) function\ndef ReLU(preactivation):\n  activation = preactivation.clip(0.0)\n  return activation\n\ndef forward_pass(net_input, all_weights, all_biases):\n\n  # Retrieve number of layers\n  K = len(all_weights) -1\n\n  # We'll store the pre-activations at each layer in a list \"all_f\"\n  # and the activations in a second list[all_h].\n  all_f = [None] * (K+1)\n  all_h = [None] * (K+1)\n\n  #For convenience, we'll set\n  # all_h[0] to be the input, and all_f[K] will be the output\n  all_h[0] = net_input\n\n  # Run through the layers, calculating all_f[0...K-1] and all_h[1...K]\n  for layer in range(K):\n      # Update preactivations and activations at this layer according to eqn 7.5\n      all_f[layer] = all_biases[layer] + np.matmul(all_weights[layer], all_h[layer])\n      all_h[layer+1] = ReLU(all_f[layer])\n\n  # Compute the output from the last hidden layer\n  all_f[K] = all_biases[K] + np.matmul(all_weights[K], all_h[K])\n\n  # Retrieve the output\n  net_output = all_f[K]\n\n  return net_output, all_f, all_h\n\n# We'll need the indicator function\ndef indicator_function(x):\n  x_in = np.array(x)\n  x_in[x_in&gt;=0] = 1\n  x_in[x_in&lt;0] = 0\n  return x_in\n\n# Main backward pass routine\ndef calc_input_output_gradient(x_in, all_weights, all_biases):\n\n  # Retrieve number of layers\n  K = len(all_weights) -1\n\n  # Run the forward pass\n  y, all_f, all_h = forward_pass(x_in, all_weights, all_biases)\n\n  # We'll store the derivatives dl_dweights and dl_dbiases in lists as well\n  all_dl_dweights = [None] * (K+1)\n  all_dl_dbiases = [None] * (K+1)\n  # And we'll store the derivatives of the loss with respect to the activation and preactivations in lists\n  all_dl_df = [None] * (K+1)\n  all_dl_dh = [None] * (K+1)\n  # Again for convenience we'll stick with the convention that all_h[0] is the net input and all_f[k] in the net output\n\n  # Compute derivatives of net output with respect to loss\n  all_dl_df[K] = np.ones_like(all_f[K])\n\n  # Now work backwards through the network\n  for layer in range(K,-1,-1):\n    all_dl_dbiases[layer] = np.array(all_dl_df[layer])\n    all_dl_dweights[layer] = np.matmul(all_dl_df[layer], all_h[layer].transpose())\n\n    all_dl_dh[layer] = np.matmul(all_weights[layer].transpose(), all_dl_df[layer])\n\n    if layer &gt; 0:\n      all_dl_df[layer-1] = indicator_function(all_f[layer-1]) * all_dl_dh[layer]\n\n\n  return all_dl_dh[0],y\n\ndef plot_derivatives(K, D, ax):\n    all_weights, all_biases = init_params(K, D)\n    x_in = np.arange(-2, 2, 4.0 / 256.0)\n    x_in = np.resize(x_in, (1, len(x_in)))\n    dydx, y = calc_input_output_gradient(x_in, all_weights, all_biases)\n    ax.plot(np.squeeze(x_in), np.squeeze(dydx), 'b-')\n    ax.set_xlim(-2, 2)\n    ax.set_xlabel(r'Input, $x$')\n    ax.set_ylabel(r'Gradient, $dy/dx$')\n    ax.set_title(f'No layers = {K}')\n\nD = 200\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\nplot_derivatives(1, D, axes[0])\nplot_derivatives(40, D, axes[1])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigura 2: Se muestran los gradientes respecto a las entradas para dos redes perceptrones multicapa con 1 y 24 capas. Los gradientes para la red con 24 capas tienen comportamiento menos suave.\n\n\n\n\n\nEn la Figura 2 vemos que los gradientes para la red con 24 capas tienen más saltos abruptos. Esto hace que sean menos estables durante el entrenamiento.\nEsto ocurre porque los pesos de las primeras capas afectan la salida de forma compleja, tal que el gradiente de la salida respecto a los parámertros de una de las primeras capas es una multiplicación de un gran número de derivadas, todas aproximadas. Por ejemplo, para una red de 4 capas \\[\n\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{f_1}} = \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{f}_3} \\frac{\\partial \\mathbf{f}_3}{\\partial \\mathbf{f}_2} \\frac{\\partial \\mathbf{f}_2}{\\partial \\mathbf{f}_1}\n\\]",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "15. Redes Residuales"
    ]
  },
  {
    "objectID": "15_redes_residuales.html#orden-de-las-operaciones-pre-activación",
    "href": "15_redes_residuales.html#orden-de-las-operaciones-pre-activación",
    "title": "Redes Residuales",
    "section": "Orden de las Operaciones: Pre-activación",
    "text": "Orden de las Operaciones: Pre-activación\nEn las redes residuales, aplicar la activación después de la transformación lineal hace que la salida \\(f(x)\\) sea cero cuando la pre-activación es negativa. Esto hace que el bloque no modifique la entrada.\nPara no perder flexibilidad de esa manera, se suele invertir el orden de las operaciones:\n\nActivación (usualmente ReLU).\nOperación lineal (convoluciones).\n\nEste orden (ReLU antes de la convolución) puede parecer contraintuitivo desde el punto de vista de una red tradicional (que aplicaría primero la transformación lineal y luego la no linealidad), pero en redes residuales puede ayudar a mejorar el flujo del gradiente y evitar la desaparición de información. También se ha explorado el orden inverso (lineal seguido de ReLU), y la elección puede depender de detalles específicos de la arquitectura o del problema.",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "15. Redes Residuales"
    ]
  },
  {
    "objectID": "1_probabilidad.html",
    "href": "1_probabilidad.html",
    "title": "Probabilidad",
    "section": "",
    "text": "Para nosotros en este curso, un dato será una medición de algún sistema. Los datos pueden ser números reales, en cuyo caso los reportamos con una barra de error. Por ejemplo \\[\n9.1300 \\pm 0.0001\\,.\n\\] Note que reportamos el resultado con un número de cifras decimales compatibles con el error.\n¿Pero qué es el error? En realidad al número reportado arriba también le falta información. Necesitamos especificar la confianza a la cual corresponde ese error. Por ejemplo podemos decir que tiene una confianza del \\(95\\%\\). Eso quiere decir que hay una probabilidad del \\(95\\%\\) asociada con ese error. El sentido de esto lo haremos más preciso cuando hablemos de probabilidad.\nTambién son importantes los datos que se representan como números enteros o categorías. Por ejemplo, cada pixel de una imagen son tres números enteros que representan los colores rojo, verde y azul.\nPara su descripción, decimos que los posibles resultados residen en un conjunto \\(S\\). Un subconjunto de \\(S\\) lo llamamos un evento.",
    "crumbs": [
      "Home",
      "Probabilidad y Estadística",
      "1. Probabilidad"
    ]
  },
  {
    "objectID": "1_probabilidad.html#el-dato",
    "href": "1_probabilidad.html#el-dato",
    "title": "Probabilidad",
    "section": "",
    "text": "Para nosotros en este curso, un dato será una medición de algún sistema. Los datos pueden ser números reales, en cuyo caso los reportamos con una barra de error. Por ejemplo \\[\n9.1300 \\pm 0.0001\\,.\n\\] Note que reportamos el resultado con un número de cifras decimales compatibles con el error.\n¿Pero qué es el error? En realidad al número reportado arriba también le falta información. Necesitamos especificar la confianza a la cual corresponde ese error. Por ejemplo podemos decir que tiene una confianza del \\(95\\%\\). Eso quiere decir que hay una probabilidad del \\(95\\%\\) asociada con ese error. El sentido de esto lo haremos más preciso cuando hablemos de probabilidad.\nTambién son importantes los datos que se representan como números enteros o categorías. Por ejemplo, cada pixel de una imagen son tres números enteros que representan los colores rojo, verde y azul.\nPara su descripción, decimos que los posibles resultados residen en un conjunto \\(S\\). Un subconjunto de \\(S\\) lo llamamos un evento.",
    "crumbs": [
      "Home",
      "Probabilidad y Estadística",
      "1. Probabilidad"
    ]
  },
  {
    "objectID": "1_probabilidad.html#visualización-de-los-datos",
    "href": "1_probabilidad.html#visualización-de-los-datos",
    "title": "Probabilidad",
    "section": "Visualización de los datos",
    "text": "Visualización de los datos\nCuando la medición nos da un solo número real, una manera útil de graficarla es mediante un histograma. Es decir, dividimos el intervalo de valores de la variable en un cierto número de subintervalos. El histograma nos dice cuántas veces obtuvimos una medición en el intervalo\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nmediciones = np.random.normal(loc=5, scale=1, size=1000)\n\nplt.hist(mediciones)\nplt.show()\n\n\n\n\n\n\n\n\nEn este gráfico cada barra es el número de datos que cae en el subintervalo. El gráfico nos da una idea de cómo se distribuyen los datos.\nVeamos qué pasa si variamos la cantidad de subintervalos\n\n# Número de subintervalos para el histograma \nbin_counts = [2, 5, 20, 50]\n\n# Crear subfiguras\nfig, axes = plt.subplots(2, 2, figsize=(6, 6))\n\n# Graficar cada histograma \nfor ax, bins in zip(axes.flatten(), bin_counts):\n    ax.hist(mediciones, bins=bins)\n    ax.set_title(f'Histograma con {bins} subintervalos')\n    ax.set_xlabel('$x$')\n    ax.set_ylabel('Frequencia')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nSi aumentamos la cantidad de subintervalos usados, el gráfico se hace ruidoso, varía mucho el valor de cada intervalo. Si tenemos pocos subintervalos, es difícil intuir la distribución.\nEn varias dimensiones, es común describir los datos mediante gráficos de dispersión (scatterplots). Estos nos pueden decir dónde se concentran los resultados y si hay correlación entre variables.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Gaussiana 2d con correlacion\nx1_pure = np.random.normal(loc=0, scale=1, size=1000)\nx2_pure = np.random.normal(loc=0, scale=1, size=1000)\nx3 = np.random.normal(loc=0, scale=1, size=1000)\nx1 = 0.3 * x1_pure + 0.7 * x2_pure\nx2 = 0.5 * x1_pure + 0.5 * x2_pure\n\nfig, axes = plt.subplots(2, 2, figsize=(6, 3))\naxes[0,0].scatter(x1, x2)\naxes[0,0].set_xlabel('$x_1$')\naxes[0,0].set_ylabel('$x_2$')\naxes[0,1].scatter(x1, x3)\naxes[0,1].set_xlabel('$x_1$')\naxes[0,1].set_ylabel('$x_3$')\naxes[1,0].scatter(x2, x3)\naxes[1,0].set_xlabel('$x_2$')\naxes[1,0].set_ylabel('$x_3$')\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Home",
      "Probabilidad y Estadística",
      "1. Probabilidad"
    ]
  },
  {
    "objectID": "9_arboles_bosques_boosting.html",
    "href": "9_arboles_bosques_boosting.html",
    "title": "Árboles, Bosques, Boosting",
    "section": "",
    "text": "Ahora estamos listos para estudiar algunos de los algoritmos más sofisticados. Empezaremos por un par que son sorprendentemente podersosos cuando la cantidad de datos es limitada y estos tienen una estructura bien definida (por ejemplo tenemos una tabla de datos). Estos son los bosques aleatorios y el boosting.\nPero antes, debemos empezar por un método bastante sencillo (y muy interpretable).\nTrabajaremos con el siguiente ejemplo artificial\nCódigo\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Fijar la semilla de aleatoriedad para reproducibilidad\nnp.random.seed(42)\n\n# ---- Parámetros ----\nN = 500         # Número de puntos de datos\nd = 6           # Dimensionalidad del espacio de entrada\n\n# Parámetros de la señal:\nsigma_senal = 0.1  # Controla la agudeza del pico alrededor de radio = 2\n\n# Parámetros del ruido:\nsigma_ruido = 0.05  # Amplitud del ruido (ruido blanco)\n\n# ---- Generar el conjunto de datos ----\n\n# 1. Generar N puntos distribuidos uniformemente en el hiper-cubo [-1.5, 1.5]^6.\ndatos = np.random.uniform(-1.5, 1.5, size=(N, d))\n\n# 2. Calcular la norma euclidiana (radio) para cada punto.\nradio = np.linalg.norm(datos, axis=1)\n\n# 3. Calcular la señal base como una función gaussiana del radio.\n#    Esta función tiene un máximo (valor 1) cuando radio = 2.\nsenal = np.exp(-((radio - 2)**2) / (2 * sigma_senal**2))\n\n# 4. Generar ruido gaussiano no correlacionado (ruido blanco).\nruido = np.random.normal(loc=0, scale=sigma_ruido, size=N)\n\n# 5. Definir la variable objetivo (target) sumando la señal y el ruido.\nobjetivo = senal + ruido \n\n# ---- Visualización ----\n# Puesto que la función subyacente depende solo del radio, se grafica la variable objetivo y la señal verdadera en función del radio.\nplt.figure(figsize=(6, 5))\nplt.scatter(radio, objetivo, alpha=0.5, label='Objetivo ruidoso (y)')\nplt.scatter(radio, senal, alpha=0.5, label='Señal verdadera (senal)', color='red')\nplt.xlabel(\"Radio (||x||)\")\nplt.ylabel(\"Valor de la variable objetivo\")\nplt.title(\"Variable Objetivo en Función del Radio en 6D\")\nplt.legend()\nplt.show()\n\n# ---- Salida Resumida ----\nprint(\"Dimensiones del conjunto de datos:\")\nprint(\"Forma de datos:\", datos.shape)\nprint(\"Forma de objetivo:\", objetivo.shape)\n\n\n\n\n\n\n\n\n\nDimensiones del conjunto de datos:\nForma de datos: (500, 6)\nForma de objetivo: (500,)\nGeneramos datos en una hiperesfera de seis dimensiones. Los datos verdaderos dependen sólo del radio de la hiperesfera, pero tenemos los valores de cada una de las seis variables. Este es un caso muy no-lineal, por lo que no es fácil de aproximar con un modelo lineal como la mayoría de los vistos hasta ahora.",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "9. Árboles, Bosques y Boosting"
    ]
  },
  {
    "objectID": "9_arboles_bosques_boosting.html#árboles-de-clasificación",
    "href": "9_arboles_bosques_boosting.html#árboles-de-clasificación",
    "title": "Árboles, Bosques, Boosting",
    "section": "Árboles de Clasificación",
    "text": "Árboles de Clasificación\nLos árboles también se pueden usar para clasificación. En este caso, en lugar de minimizar el error cuadrático medio, se busca minimizar una medida de impureza como la entropía:\n\\[\nH(p) = -\\sum_{i=1}^N \\sum_{k=1}^K p_{ik} \\log_2 p_{ik}\n\\]\ndonde \\(p_{ik}\\) es la proporción de observaciones de la clase \\(k\\) en el nodo \\(i\\). La entropía mide la incertidumbre en la clasificación: es máxima cuando todas las clases son igualmente probables y es cero cuando la probabilidad de una clase es 1 y las otras son cero para cada observación. Es decir, es cero cuando cada clase es “pura” en el sentido de que todos los puntos en el nodo pertenecen a la misma clase.\nAl dividir un nodo, se busca la característica y el punto de corte que maximicen la reducción en la entropía. Esta reducción se conoce como ganancia de información:\n\\[\nIG = H(p) - \\left( \\frac{N_{\\text{izq}}}{N} H(p_{\\text{izq}}) + \\frac{N_{\\text{der}}}{N} H(p_{\\text{der}}) \\right)\n\\]\ndonde \\(N\\) es el número total de observaciones en el nodo, y \\(N_{\\text{izq}}\\) y \\(N_{\\text{der}}\\) son los números de observaciones en los nodos hijos izquierdo y derecho respectivamente.",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "9. Árboles, Bosques y Boosting"
    ]
  },
  {
    "objectID": "9_arboles_bosques_boosting.html#bagging-y-bosques-aleatorios",
    "href": "9_arboles_bosques_boosting.html#bagging-y-bosques-aleatorios",
    "title": "Árboles, Bosques, Boosting",
    "section": "Bagging y Bosques Aleatorios",
    "text": "Bagging y Bosques Aleatorios\nLos árboles de decisión tienen una alta varianza. Es decir, un árbol de decisión entrenado con un conjunto de datos puede dar resultados muy diferentes si se entrena con otro conjunto de datos. Una forma de reducir la varianza en general es promediar sobre muchas realizaciones entrenadas sobre múltiples conjuntos de datos. Sin embargo, no tenemos múltiples conjuntos de datos. Por esto podemos usar muestreo con reemplazo para obtener múltiples conjuntos de datos. Es decir, procedemos de la siguiente manera:\n\nTomamos un subconjunto aleatorio con reemplazo de los datos de tamaño \\(N\\).\nEntrenamos un árbol de decisión con ese subconjunto.\nRepetimos el proceso \\(B\\) veces.\nObtenemos \\(B\\) árboles de decisión.\nLa predicción final es el promedio de las predicciones de los \\(B\\) árboles.\n\nA esto se le llama bagging (Bootstrap Aggregating).\n\n\nCódigo\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Crear el estimador base (árbol de decisión)\narbol_base = DecisionTreeRegressor(max_depth=3, random_state=42)\n\n# Crear el modelo de bagging\nbagging = BaggingRegressor(\n    n_estimators=100,  # Número de árboles\n    max_samples=0.8,   # Fracción de datos a usar en cada árbol\n    random_state=42\n)\n\n# Entrenar el modelo\nbagging.fit(datos, objetivo)\n\n# Realizar predicciones\npredicciones_bagging = bagging.predict(datos)\n\n# Evaluar el rendimiento\nmse_bagging = mean_squared_error(objetivo, predicciones_bagging)\nprint(\"MSE del bagging:\", mse_bagging)\n\n# Visualizar las predicciones\nplt.figure(figsize=(6, 5))\nplt.scatter(radio, objetivo, alpha=0.5, label='Datos observados')\nplt.scatter(radio, predicciones_bagging, alpha=0.5, label='Predicciones del bagging', color='green')\nplt.xlabel(\"Radio (||x||)\")\nplt.ylabel(\"Valor de la variable objetivo\")\nplt.title(\"Predicciones de Bagging vs. Radio en 6D\")\nplt.legend()\nplt.show()\n\n\nMSE del bagging: 0.020796358419776385\n\n\n\n\n\n\n\n\n\nAunque esto reduce la varianza, los árboles de decisión tienen todavía una alta correlación entre ellos. Es decir, si un árbol predice que un punto es de una clase, los otros árboles tenderán a predecir lo mismo. Para reducir esta correlación, podemos introducir una fuente adicional de aleatoriedad. La idea es permitir que cada árbol pueda usar un subconjunto de características para hacer las divisiones.\n\nBosques Aleatorios\nLos bosques aleatorios son una modificación del bagging que introduce una fuente adicional de aleatoriedad. La idea principal es:\n\nEntrenar múltiples árboles de decisión\nCada árbol se entrena con un subconjunto aleatorio de los datos (muestreo con reemplazo)\nEn cada nodo, solo se considera un SUBCONJUNTO ALEATORIO de las características (típicamente \\(\\sqrt{p}\\) características en clasificación o \\(p/3\\) en regresión, donde \\(p\\) es el número total de características)\nLa predicción final es el promedio (regresión) o la mayoría de votos (clasificación) de todos los árboles\n\nLa diferencia clave con el bagging es la restricción en las características disponibles en cada nodo. Esta restricción reduce la correlación entre los árboles del ensamble, lo que típicamente resulta en un mejor rendimiento que el bagging simple. En bagging, si hay características muy predictivas, todos los árboles tenderán a usar esas mismas características, lo que aumenta la correlación entre ellos. Al restringir aleatoriamente las características disponibles en cada nodo, los bosques aleatorios evitan este problema y producen árboles más diversos y menos correlacionados.\n\n\nCódigo\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Crear y entrenar el bosque aleatorio\nbosque = RandomForestRegressor(n_estimators=100, max_features='sqrt', random_state=42)\nbosque.fit(datos, objetivo)\n\n# Realizar predicciones\npredicciones_bosque = bosque.predict(datos)\n\n# Evaluar el rendimiento\nmse_bosque = mean_squared_error(objetivo, predicciones_bosque)\nprint(\"MSE del bosque aleatorio:\", mse_bosque)\n\n# Visualizar las predicciones\nplt.figure(figsize=(6, 5))\nplt.scatter(radio, objetivo, alpha=0.5, label='Datos observados')\nplt.scatter(radio, predicciones_bosque, alpha=0.5, label='Predicciones del bosque', color='green')\nplt.xlabel(\"Radio (||x||)\")\nplt.ylabel(\"Valor de la variable objetivo\")\nplt.title(\"Predicciones de Bosque Aleatorio vs. Radio en 6D\")\nplt.legend()\nplt.show()\n\n\nMSE del bosque aleatorio: 0.01404837750553461",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "9. Árboles, Bosques y Boosting"
    ]
  },
  {
    "objectID": "9_arboles_bosques_boosting.html#boosting",
    "href": "9_arboles_bosques_boosting.html#boosting",
    "title": "Árboles, Bosques, Boosting",
    "section": "Boosting",
    "text": "Boosting\nEl boosting es otra técnica de ensamblaje que construye modelos secuencialmente, donde cada nuevo modelo intenta corregir los errores del anterior. El algoritmo más popular es Gradient Boosting:\n\nInicializar con una predicción constante (por ejemplo, el promedio de los valores objetivo)\nPara cada iteración:\n\nCalcular los residuos (diferencia entre predicción actual y valores reales)\nEntrenar un nuevo árbol para predecir los residuos\nActualizar las predicciones sumando una fracción de la predicción del nuevo árbol\n\nLa predicción final es la suma de todas las predicciones de los árboles\n\n\n\nCódigo\nimport xgboost as xgb\n\n# Crear y entrenar el modelo XGBoost\nxgb_model = xgb.XGBRegressor(\n    n_estimators=2000,        \n    learning_rate=0.005,      \n    max_depth=6,              \n    random_state=42,\n    objective='reg:squarederror'\n)\nxgb_model.fit(datos, objetivo)\n\n# Realizar predicciones\npredicciones_xgb = xgb_model.predict(datos)\n\n# Evaluar el rendimiento\nmse_xgb = mean_squared_error(objetivo, predicciones_xgb)\nprint(\"MSE de boosted trees:\", mse_xgb)\n\n# Visualizar las predicciones\nplt.figure(figsize=(6, 5))\nplt.scatter(radio, objetivo, alpha=0.5, label='Datos observados')\nplt.scatter(radio, predicciones_xgb, alpha=0.5, label='Predicciones de XGBoost', color='green')\nplt.xlabel(\"Radio (||x||)\")\nplt.ylabel(\"Valor de la variable objetivo\")\nplt.title(\"Predicciones de XGBoost vs. Radio en 6D\")\nplt.legend()\nplt.show()\n\n\nMSE de boosted trees: 0.007198678032281405\n\n\n\n\n\n\n\n\n\nEn este caso yo escogí unos hiperparámetros que dan un error pequeño sobre el conjunto de entrenamiento. En general, esto no es lo que se hace ya que el error sobre el conjunto de entrenamiento no es una buena estimación del error sobre datos nuevos. Lo que se hace es usar validación cruzada para escoger los hiperparámetros.",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "9. Árboles, Bosques y Boosting"
    ]
  },
  {
    "objectID": "9_arboles_bosques_boosting.html#árboles-de-decisión-1",
    "href": "9_arboles_bosques_boosting.html#árboles-de-decisión-1",
    "title": "Árboles, Bosques, Boosting",
    "section": "Árboles de Decisión",
    "text": "Árboles de Decisión\n\nSobreajuste: Muy propensos a sobreajuste, especialmente si no se controla la profundidad\nCosto computacional: Muy eficiente en entrenamiento y predicción\nInterpretabilidad: Excelente - se pueden visualizar y entender fácilmente\nHiperparámetros: Pocos y fáciles de interpretar (profundidad máxima, número mínimo de muestras por hoja)\nRendimiento: Regresores débiles con alta varianza - predicciones escalonadas y poco suaves\nUso recomendado: Cuando la interpretabilidad es crucial y el conjunto de datos es pequeño. No recomendado como regresor único en problemas donde se necesita precisión.",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "9. Árboles, Bosques y Boosting"
    ]
  },
  {
    "objectID": "9_arboles_bosques_boosting.html#bagging",
    "href": "9_arboles_bosques_boosting.html#bagging",
    "title": "Árboles, Bosques, Boosting",
    "section": "Bagging",
    "text": "Bagging\n\nSobreajuste: Menos propenso que árboles individuales, pero puede sobreajustar si los árboles base son muy complejos\nCosto computacional: Lineal con el número de árboles, paralelizable\nInterpretabilidad: Baja - el ensamble es difícil de interpretar\nHiperparámetros: Número de árboles, tamaño del subconjunto de datos\nUso recomendado: Cuando se necesita reducir la varianza de árboles individuales",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "9. Árboles, Bosques y Boosting"
    ]
  },
  {
    "objectID": "9_arboles_bosques_boosting.html#bosques-aleatorios-1",
    "href": "9_arboles_bosques_boosting.html#bosques-aleatorios-1",
    "title": "Árboles, Bosques, Boosting",
    "section": "Bosques Aleatorios",
    "text": "Bosques Aleatorios\n\nSobreajuste: Menos propenso que bagging debido a la aleatorización adicional\nCosto computacional: Similar a bagging, pero ligeramente más costoso por la selección aleatoria de características\nInterpretabilidad: Baja, pero permite medir la importancia de características\nHiperparámetros: Número de árboles, número de características a considerar en cada división\nUso recomendado: Problemas generales de regresión y clasificación donde la interpretabilidad no es crucial",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "9. Árboles, Bosques y Boosting"
    ]
  },
  {
    "objectID": "9_arboles_bosques_boosting.html#boosting-1",
    "href": "9_arboles_bosques_boosting.html#boosting-1",
    "title": "Árboles, Bosques, Boosting",
    "section": "Boosting",
    "text": "Boosting\n\nSobreajuste: Muy propenso si no se controla adecuadamente con regularización\nCosto computacional: Alto, especialmente con muchos árboles y datos grandes\nInterpretabilidad: Baja, pero permite medir la importancia de características\nHiperparámetros: Múltiples e importantes (tasa de aprendizaje, número de árboles, profundidad, regularización)\nUso recomendado: Cuando se necesita el mejor rendimiento posible y se tienen recursos computacionales suficientes",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "9. Árboles, Bosques y Boosting"
    ]
  },
  {
    "objectID": "19_Operador_Neural.html",
    "href": "19_Operador_Neural.html",
    "title": "Operador Neuronal",
    "section": "",
    "text": "Hasta ahora hemos estudiado redes neuronales que toman un vector de entrada \\(\\boldsymbol{x}\\) y producen un vector de salida \\(\\boldsymbol{y}\\), es decir, \\(f: \\mathbb{R}^n \\to \\mathbb{R}^m\\). En contraste, los operadores neuronales son una clase de modelos de aprendizaje profundo diseñados para aprender mapeos entre espacios de funciones, en lugar de solo vectores de dimensión finita. Es decir, los operadores neuronales aprenden un operador \\(\\mathcal{G}\\) que toma una función de entrada y produce una función de salida. Formalmente, uno puede pensar en \\(\\mathcal{G}: X \\to Y\\), donde \\(X\\) y \\(Y\\) son espacios de dimensión infinita (por ejemplo, espacios de Banach) de funciones.\nArquitectónicamente, se asemejan a las redes que hemos visto, pero con capas definidas por transformadas integrales en vez de transformaciones lineales elemento por elemento. Una capa de operador neuronal puede escribirse como:\n\\[\nv_{j+1}(x) = \\sigma\\!\\Big( W\\, v_j(x) \\;+\\; \\int_D K_\\Theta(x,y)\\, v_j(y)\\,dy \\Big),\n\\]\ndonde \\(v_j(x)\\) es una función intermedia (comenzando con \\(v_0\\) como la función de entrada), \\(W\\) es una transformación lineal aprendida, \\(K_\\Theta(x,y)\\) es un kernel aprendido para un operador integral, y \\(\\sigma\\) es una activación no lineal (aplicada elemento por elemento). Al componer varias de estas capas, los operadores neuronales pueden representar operadores no lineales muy generales. De hecho, existen teoremas de aproximación universal que aseguran que los operadores neuronales pueden aproximar cualquier operador continuo (en conjuntos compactos) con precisión arbitraria. Esto extiende el clásico teorema de aproximación universal de redes neuronales de dimensión finita al caso de mapeos de operadores de dimensión infinita.\nUna característica clave de los operadores neuronales es que, con la formulación adecuada, el mismo modelo aprendido puede aplicarse a diferentes discretizaciones o puntos de evaluación de las funciones de entrada/salida. A diferencia de una red neuronal estándar que está atada a un tamaño de entrada fijo, los operadores neuronales pueden recibir funciones muestreadas en cualquier malla (o incluso como coeficientes en una base) y producir salidas en cualquier conjunto de puntos deseado. En cierto sentido, así como ocurría en las PINNs, son ya operadores interpolados.\nSe han desarrollado varias arquitecturas especializadas de operadores neuronales como DeepONet, FNO, etc.\nLa motivación principal de los operadores neuronales es crear sustitutos rápidos y generalizables para problemas de física computacional. Muchos sistemas físicos se modelan mediante ecuaciones diferenciales parciales u otras ecuaciones donde un conjunto de funciones de entrada (por ejemplo, condiciones iniciales, de frontera o coeficientes que varían espacialmente) se mapea a una función de salida (la solución). Los métodos estándar deben recomputar la solución desde cero para cada nueva entrada (por ejemplo para cada nueva condición inicial) y pueden ser extremadamente costosos para sistemas grandes. Los operadores neuronales ofrecen un enfoque de aprender una vez, reutilizar muchas veces: Entrenamos un modelo con muchas parejas de entrada-salida (usando datos de un solucionador de alta fidelidad o experimentos) y luego predecimos nuevas soluciones en una sola pasada hacia adelante.\nLos operadores neuronales se han aplicado en áreas como modelado de flujo turbulento, modelado climático/meteorológico, mecánica estructural y flujo en medios porosos. En estos dominios, obtener soluciones en tiempo real o muchas consultas es crítico (para control, optimización, cuantificación de incertidumbre, etc.), y los operadores neuronales sirven como sustitutos eficientes.\nEn general, a los métodos usados para aproximar el resultado de una simulación se los llama emuladores. Los emuladores se usan extensivamente en cosmología para explorar espacioes de parámetros de alta dimensión (tirando muchos puntos) sin necesidad de correr una simulación nueva cada vez.",
    "crumbs": [
      "Home",
      "Algunas aplicaciones",
      "19. Operadores Neuronales"
    ]
  },
  {
    "objectID": "19_Operador_Neural.html#generación-de-datos",
    "href": "19_Operador_Neural.html#generación-de-datos",
    "title": "Operador Neuronal",
    "section": "1. Generación de Datos",
    "text": "1. Generación de Datos\nPrimero, establecemos los parámetros del problema y creamos un conjunto de datos. Elegimos una cuadrícula espacial de tamaño \\(N=100\\) (puntos en \\([0,1]\\)), consideramos \\(c=1\\) y un tiempo final \\(T=0.25\\). Generamos funciones de desplazamiento inicial aleatorio \\(f(x)\\) superponiendo algunos modos de Fourier aleatorios para obtener funciones suaves y periódicas. Luego calculamos el correspondiente \\(u(T,x)\\) usando la solución analítica. Esto nos da pares \\((f, u_T)\\) para entrenamiento.\n\nimport numpy as np\n\n# Parametros\nN = 100                 # numero de puntos espaciales\ndominio = np.linspace(0, 1, N, endpoint=False)\nc = 1.0\nT = 0.25                # tiempo final\ndesplazamiento = int(c * T * N)  # desplazamiento en puntos de la cuadrícula\n\n# Funcion para generar un desplazamiento inicial aleatorio f(x)\ndef funcion_inicial_aleatoria(x, num_modos=5):\n    f = np.zeros_like(x)\n    # superponer modos de Fourier aleatorios\n    for k in range(1, num_modos+1):\n        A = np.random.uniform(-1, 1)        # amplitud aleatoria\n        phi = np.random.uniform(0, 2*np.pi) # fase aleatoria\n        f += A * np.cos(2*np.pi * k * x + phi)\n    return f\n\n# Crear conjuntos de entrenamiento y prueba\nn_entrenamiento = 1000\nn_prueba  = 200\nentrenamiento_x = np.zeros((n_entrenamiento, N))\nentrenamiento_y = np.zeros((n_entrenamiento, N))\nfor i in range(n_entrenamiento):\n    f = funcion_inicial_aleatoria(dominio)\n    # Solucion en T: u(T,x) = 0.5[f(x - cT) + f(x + cT)]\n    u_T = 0.5 * (np.roll(f, desplazamiento) + np.roll(f, -desplazamiento))\n    entrenamiento_x[i, :] = f\n    entrenamiento_y[i, :] = u_T\n\n# De forma similar para el conjunto de prueba\nprueba_x = np.zeros((n_prueba, N))\nprueba_y = np.zeros((n_prueba, N))\nfor j in range(n_prueba):\n    f = funcion_inicial_aleatoria(dominio)\n    u_T = 0.5 * (np.roll(f, desplazamiento) + np.roll(f, -desplazamiento))\n    prueba_x[j, :] = f\n    prueba_y[j, :] = u_T\n\n# Convertir los datos a tensores de PyTorch\nimport torch\nentrenamiento_x_t = torch.tensor(entrenamiento_x, dtype=torch.float32)\nentrenamiento_y_t = torch.tensor(entrenamiento_y, dtype=torch.float32)\nprueba_x_t  = torch.tensor(prueba_x, dtype=torch.float32)\nprueba_y_t  = torch.tensor(prueba_y, dtype=torch.float32)\n\nEn el código anterior, np.roll(f, desplazamiento) desplaza el arreglo f una cantidad de puntos (con envoltura periódica), evaluando efectivamente \\(f(x - T)\\) o \\(f(x + T)\\) en la cuadrícula discreta. Al promediar obtenemos la solución en el tiempo \\(T\\). Finalmente, convertimos los arrays de NumPy a tensores de PyTorch para entrenamiento.",
    "crumbs": [
      "Home",
      "Algunas aplicaciones",
      "19. Operadores Neuronales"
    ]
  },
  {
    "objectID": "19_Operador_Neural.html#definición-del-modelo-de-operador-neuronal",
    "href": "19_Operador_Neural.html#definición-del-modelo-de-operador-neuronal",
    "title": "Operador Neuronal",
    "section": "2. Definición del Modelo de Operador Neuronal",
    "text": "2. Definición del Modelo de Operador Neuronal\nUsaremos una red neuronal totalmente conectada. Esta red toma un vector de longitud \\(N\\) (la condición inicial discretizada \\(f\\)) y produce un vector de longitud \\(N\\) (la solución predicha \\(u_T\\)). Aunque es esencialmente una red estándar (sin imponer estructura especial de operador), aún puede aprender el mapeo de \\(f\\) a \\(u_T\\) dada suficiente capacidad y datos. Modelos más sofisticados podrían usar convoluciones o compartir pesos para aprovechar simetrías, pero aquí usamos este enfoque básico por claridad.\n\nimport torch.nn as nn\n\nclass OperadorNeuronal1D(nn.Module):\n    def __init__(self, N):\n        super().__init__()\n        self.red = nn.Sequential(\n            nn.Linear(N, 128), nn.ReLU(),\n            nn.Linear(128, 128), nn.ReLU(),\n            nn.Linear(128, N)\n        )\n    def forward(self, x):\n        return self.red(x)\n\n# Instanciar el modelo\nmodelo = OperadorNeuronal1D(N)\n\nEsto define un perceptrón de 3 capas (dos ocultas de tamaño 128 con activaciones ReLU). No hay capas convolucionales ni de Fourier; la entrada se trata como vector plano.",
    "crumbs": [
      "Home",
      "Algunas aplicaciones",
      "19. Operadores Neuronales"
    ]
  },
  {
    "objectID": "19_Operador_Neural.html#entrenamiento-del-modelo",
    "href": "19_Operador_Neural.html#entrenamiento-del-modelo",
    "title": "Operador Neuronal",
    "section": "3. Entrenamiento del Modelo",
    "text": "3. Entrenamiento del Modelo\nEntrenamos el operador neuronal usando el error cuadrático medio (MSE) entre la salida de la red y la solución verdadera \\(u_T(x)\\), mediante descenso de gradiente (optimizador Adam). En cada época hacemos una pasada completa sobre los datos (por simplicidad, no usamos minibatches aquí, pero podría hacerse).\n\nimport torch.optim as optim\n\n# Funcion de perdida y optimizador\ncriterio = nn.MSELoss()\noptimizador = optim.Adam(modelo.parameters(), lr=1e-3)\n\n# Ciclo de entrenamiento\nnum_epocas = 500\nfor epoca in range(num_epocas):\n    # Paso hacia adelante con todos los datos de entrenamiento\n    prediccion = modelo(entrenamiento_x_t)            # forma: (n_entrenamiento, N)\n    perdida = criterio(prediccion, entrenamiento_y_t)\n    # Retropropagacion y actualizacion de pesos\n    optimizador.zero_grad()\n    perdida.backward()\n    optimizador.step()\n    # Imprimir progreso ocasionalmente\n    if epoca % 100 == 0:\n        print(f\"Epoca {epoca}: perdida de entrenamiento = {perdida.item():.6f}\")\n\nEpoca 0: perdida de entrenamiento = 0.335672\nEpoca 100: perdida de entrenamiento = 0.001228\nEpoca 200: perdida de entrenamiento = 0.000407\nEpoca 300: perdida de entrenamiento = 0.000203\nEpoca 400: perdida de entrenamiento = 0.000127\n\n\nCorremos, por ejemplo, 500 épocas. Cada 100 épocas imprimimos la pérdida para monitorear el avance.",
    "crumbs": [
      "Home",
      "Algunas aplicaciones",
      "19. Operadores Neuronales"
    ]
  },
  {
    "objectID": "19_Operador_Neural.html#prueba-del-operador-aprendido",
    "href": "19_Operador_Neural.html#prueba-del-operador-aprendido",
    "title": "Operador Neuronal",
    "section": "4. Prueba del Operador Aprendido",
    "text": "4. Prueba del Operador Aprendido\nTras entrenar, evaluamos el modelo sobre condiciones iniciales de prueba no vistas. Calculamos el error relativo para ver cuán bien se aprendió el operador y comparamos visualmente para un ejemplo.\n\n# Evaluar en datos de prueba\nmodelo.eval()  # modo evaluacion\nwith torch.no_grad():\n    # elegir un indice del conjunto de prueba\n    k = 0\n    f0 = prueba_x_t[k]         # condicion inicial\n    solucion_verdadera = prueba_y_t[k]    # solucion exacta en T\n    prediccion_u = modelo(f0.unsqueeze(0)).squeeze(0)  # agregar dimension batch\n    # Calcular error relativo L2\n    error = torch.norm(prediccion_u - solucion_verdadera) / torch.norm(solucion_verdadera)\n    print(f\"Error relativo L2 en muestra de prueba {k}: {error.item():.4f}\")\n\nError relativo L2 en muestra de prueba 0: 0.0513\n\n\nEn este ejemplo 1D, el problema era fácil – el operador era un promedio de desplazamientos lineales, y la red lo aprende rápidamente. En escenarios más complejos (EDPs no lineales, más dimensiones, etc.), se requerirían arquitecturas más avanzadas y conjuntos de datos más grandes, pero el flujo de trabajo sería análogo: simular muchas instancias del sistema, entrenar una red para mapear entradas a salidas, y luego usar el modelo entrenado para predicciones rápidas.",
    "crumbs": [
      "Home",
      "Algunas aplicaciones",
      "19. Operadores Neuronales"
    ]
  },
  {
    "objectID": "4_minimos_cuadrados_intervalos.html",
    "href": "4_minimos_cuadrados_intervalos.html",
    "title": "Mínimos Cuadrados y Estimación de Intervalos",
    "section": "",
    "text": "Vamos a estudiar un método más de obtener estimadores. Es tal vez el método más usado por su simplicidad y nos dará pie para motivar varios de los conceptos usados en aprendizaje automático. Este es el método de mínimos cuadrados.\nLuego empezaremos a estudiar cómo sacar conclusiones a partir de los datos. Nuestro primer problema es cómo interpretar los resultados de una medición.",
    "crumbs": [
      "Home",
      "Probabilidad y Estadística",
      "4. Mínimos Cuadrados e Intervalos"
    ]
  },
  {
    "objectID": "4_minimos_cuadrados_intervalos.html#frecuentista-el-intervalo-de-confianza.",
    "href": "4_minimos_cuadrados_intervalos.html#frecuentista-el-intervalo-de-confianza.",
    "title": "Mínimos Cuadrados y Estimación de Intervalos",
    "section": "Frecuentista: El intervalo de confianza.",
    "text": "Frecuentista: El intervalo de confianza.\nPara cada valor del parámetro \\(\\theta\\) tenemos una probabilidad de obtener un valor \\(\\hat{\\theta}\\) al aplicar un estimador \\(P(\\hat{\\theta}|\\theta)\\). De esta forma, para cada valor verdadero de \\(\\theta\\) podemos calcular un intervalo \\([\\hat{\\theta}_-, \\hat{\\theta}_+]\\) tal que la probabilidad de obtener el valor medido en ese intervalo es \\(C\\). Podemos graficar las curvas \\(\\hat{\\theta}_-\\) y \\(\\hat{\\theta}_+\\) como funciones de \\(\\theta\\).\n\n\n\n\n\n\n\n\n\nPero ¡No conocemos el valor de \\(\\theta\\)! Sólo conocemos el valor medido \\(\\hat{\\theta}\\). Entonces lo que podemos hacer es buscar la intersección de la línea vertical \\(\\hat{\\theta}\\) con \\(\\hat{\\theta}_+\\) y \\(\\hat{\\theta}_-\\). Esto nos dará los valores \\(\\theta_+\\) y \\(\\theta_-\\) que podemos reportar como los extremos de nuestro intervalo de confianza para el parámetro.\n\n\n\n\n\n\n\n\n\nLa interpretación es que si repetimos el experimento muchas veces, una fracción \\(C\\) de esas repeticiones contendrá el verdadero valor \\(\\theta\\) dentro del intervalo.\nPara una gaussiana, las curvas son líneas rectas, tal que el intervalo a \\(n\\sigma\\) tiene una probabilidad \\(N((\\hat{\\theta} - \\theta)/(n\\sigma))\\) de contener el verdadero valor. En estos enunciados \\(\\sigma\\) se refiere a \\(\\langle(\\hat{\\theta} - \\theta)^2\\rangle\\).",
    "crumbs": [
      "Home",
      "Probabilidad y Estadística",
      "4. Mínimos Cuadrados e Intervalos"
    ]
  },
  {
    "objectID": "4_minimos_cuadrados_intervalos.html#bayesiana-el-invervalo-de-credibilidad",
    "href": "4_minimos_cuadrados_intervalos.html#bayesiana-el-invervalo-de-credibilidad",
    "title": "Mínimos Cuadrados y Estimación de Intervalos",
    "section": "Bayesiana: El invervalo de credibilidad",
    "text": "Bayesiana: El invervalo de credibilidad\nLa interpretación frecuentista tiene la desventaja de no usar información previa. Según la interpretación Bayesiana sí podemos decir frases como “la probabilidad de que la masa del electrón esté en un intervalo es tal”. La definición bayesiana considera la probabilidad como algo subjetivo dada la información disponible, entonces esa frase no se refiere a la probabilidad de que el electrón tenga esa masa si no a cuál es nuestra certidumbre, o “cuánto estamos dispuestos a apostar”.\nPara calcular intervalos de credibilidad bayesianos, calculamos la probabilidad posterior \\[\nf_{post}(\\theta|\\hat{\\theta}) = \\frac{L(\\hat{\\theta}|\\theta)f_{prev}(\\theta)}{\\int d\\theta'\\,L(\\hat{\\theta}|\\theta)f_{prev}(\\theta)}\\,.\n\\] El intervalo se calcula usando este posterior.\nEl problema de la interpretación bayesiana es que no nos dice cómo calcular el prior \\(f_{prev}\\). Este viene de nuestro conocimiento anterior sobre la cantidad a medir. Pero es algo muy subjetivo. Por ejemplo, si no sabemos nada podríamos asignarle una probabilidad uniforme. Pero si \\(x\\) tiene una distribución uniforme, \\(\\ln x\\) tiene una distribución con un exponencial. Entonces si no sabemos nada sobre \\(x\\) pareciera que sabemos algo sobre \\(\\ln x\\) ya que no le asignamos la misma probabilidad a todos sus valores, y vice versa. ¿Cuál escoger? Hay algunos criterios que tienen que ver por ejemplo con la entropía, pero el consenso de la comunidad es que ninguno es del todo satisfactorio.",
    "crumbs": [
      "Home",
      "Probabilidad y Estadística",
      "4. Mínimos Cuadrados e Intervalos"
    ]
  },
  {
    "objectID": "12_gradient_descent_and_autodiff.html",
    "href": "12_gradient_descent_and_autodiff.html",
    "title": "Descenso de Gradiente y Auto-diferenciación",
    "section": "",
    "text": "Ahora queremos estudiar cómo encontrar los parámetros óptimos de un modelo para minimizar una función de pérdida. Existen muchos algoritmos de optimización. Uno de los que converge en menos pasos es el método de Newton, pero su costo computacional crece rápidamente con el número de parámetros. Las redes neuronales pueden involucrar billones de parámetros, lo que hace que el método de Newton y otros similares sean imposibles de aplicar. Por lo tanto se usa el Descenso de Gradiente (Gradient Descent, GD) al entrenar redes neuronales.\nEl descenso de gradiente consiste en tomar el gradiente de la función de pérdida en el espacio de parámetros. La idea es ir en la dirección opuesta al gradiente, ya que el gradiente apunta en la dirección de máximo crecimiento de la función (y su opuesto en la dirección de máximo descenso). Cuando llegamos a un mínimo de la función, el gradiente será cero.\nEn esta sección veremos primero cómo aplicar el descenso de gradiente para un perceptrón multicapa asumiendo que el gradiente de la función de pérdida con respecto a los parámetros de la red es conocido. Luego veremos cómo calcular el gradiente de la función de pérdida con respecto a los parámetros de la red de manera eficiente. Finalmente discutiremos algunos detalles de cómo escoger el punto inicial en el espacio de parámetros para inciar el descenso de gradiente.",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "12. Descenso de Gradiente y Auto-diferenciación"
    ]
  },
  {
    "objectID": "12_gradient_descent_and_autodiff.html#ejemplo-de-regresión-lineal-simple-con-descenso-de-gradiente",
    "href": "12_gradient_descent_and_autodiff.html#ejemplo-de-regresión-lineal-simple-con-descenso-de-gradiente",
    "title": "Descenso de Gradiente y Auto-diferenciación",
    "section": "Ejemplo de Regresión Lineal Simple con Descenso de Gradiente",
    "text": "Ejemplo de Regresión Lineal Simple con Descenso de Gradiente\nAntes de aplicar el descenso de gradiente a una red neuronal, empezamos por hacerlo en un caso sencillo que es fácil de visualizar: la regresión lineal simple. En este caso queremos ajustar la pendiente y el intercepto de una recta \\(y = m x + b\\) a un conjunto de datos \\((x_i, y_i)\\). La función de pérdida (MSE) a minimizar es: \\[\n\\mathcal{J}(m,b) = \\frac{1}{N}\\sum_{i=1}^N (y_i - (m x_i + b))^2.\n\\] donde \\(N\\) es el número total de puntos de datos. Notemos que la función de pérdida es una función de dos variables, \\(m\\) y \\(b\\). Por eso la podemos graficar fácilmente.\n\n\nCódigo\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Generar muestra aleatoria\nnp.random.seed(42)\nN = 100\nx = np.random.uniform(-10, 10, N)\nm_verdadero, b_verdadero = 2, -1\ny = m_verdadero * x + b_verdadero + np.random.normal(0, 2, N)\n\n# Definir función de pérdida\ndef perdida_ecm(m, b, x, y):\n    return np.mean((y - (m * x + b))**2)\n\n# Crear malla para gráfico 3D y heatmap\nrango_m = np.linspace(-3, 7, 100)\nrango_b = np.linspace(-24, 24, 100)\nM, B = np.meshgrid(rango_m, rango_b)\nZ = np.array([perdida_ecm(m, b, x, y) for m, b in zip(M.ravel(), B.ravel())]).reshape(M.shape)\n\n# Configurar el layout de los subplots\nfig = plt.figure(figsize=(6, 12))\n\n# Graficar superficie 3D\nax1 = fig.add_subplot(211, projection='3d')\nsuperficie = ax1.plot_surface(M, B, Z, cmap='viridis')\nax1.set_xlabel('m (pendiente)')\nax1.set_ylabel('b (intercepto)')\nax1.set_zlabel('Pérdida ECM')\nax1.set_title('Función de Pérdida 3D')\n\n# Graficar heatmap 2D\nax2 = fig.add_subplot(212)\nheatmap = ax2.imshow(Z, extent=[rango_m.min(), rango_m.max(), rango_b.min(), rango_b.max()], \n                     origin='lower', cmap='viridis', aspect='auto')\ncontours = ax2.contour(M, B, Z, colors='black', levels=10)\nax2.set_xlabel('m (pendiente)')\nax2.set_ylabel('b (intercepto)')\nax2.set_title('Heatmap de la Función de Pérdida')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigura 1: Función de pérdida para regresión lineal\n\n\n\n\n\nEn la Figura 1 se muestra la función de pérdida para regresión lineal. Vemos que esta es mínima en el punto que corresponde a los valores verdaderos de \\(m\\) y \\(b\\) y que es convexa, es decir, que cualquier línea recta que conecte dos puntos está por encima de la superficie.\nUtilizamos el Descenso de Gradiente (Gradient Descent, GD) para encontrar los valores óptimos de \\(m\\) y \\(b\\). La actualización en cada iteración se realiza siguiendo la dirección opuesta al gradiente de la función de pérdida:\n\\[\n\\begin{aligned}\nm &\\leftarrow m - \\eta \\,\\frac{\\partial \\mathcal{L}}{\\partial m},\\\\\nb &\\leftarrow b - \\eta \\,\\frac{\\partial \\mathcal{L}}{\\partial b},\n\\end{aligned}\n\\]\nAquí, \\(\\eta\\) es la tasa de aprendizaje (learning rate), un hiperparámetro que controla el tamaño de los pasos que damos en cada actualización. Las derivadas parciales (el gradiente) indican la dirección de máximo ascenso de la pérdida, y su inverso es la dirección de máximo descenso. Al restar el gradiente multiplicado por \\(\\eta\\), nos movemos un poco hacia el mínimo. En este caso podemos calcular fácilmente el gradiente de la función de pérdida con respecto a \\(m\\) y \\(b\\): \\[\n\\frac{\\partial \\mathcal{J}}{\\partial m}\n= -\\frac{2}{N}\\sum_{i=1}^N (y_i - (m x_i + b))\\,x_i,\n\\] \\[\n\\frac{\\partial \\mathcal{J}}{\\partial b}\n= -\\frac{2}{N}\\sum_{i=1}^N (y_i - (m x_i + b)).\n\\]\n\n\nCódigo\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\n\n# Definir función de pérdida y sus derivadas\ndef perdida_ecm(m, b, x, y):\n    return np.mean((y - (m * x + b))**2)\n\ndef derivada_m(m, b, x, y):\n    return -2 * np.mean((y - (m * x + b)) * x)\n\ndef derivada_b(m, b, x, y):\n    return -2 * np.mean(y - (m * x + b))\n\n# Crear malla para gráfico 3D y heatmap\nrango_m = np.linspace(0, 4, 100)\nrango_b = np.linspace(-5.5, 2, 100)\nM, B = np.meshgrid(rango_m, rango_b)\nZ = np.array([perdida_ecm(m, b, x, y) for m, b in zip(M.ravel(), B.ravel())]).reshape(M.shape)\n\n# Configurar figura\nfig, ax = plt.subplots(figsize=(10, 6))\nheatmap = ax.imshow(Z, extent=[rango_m.min(), rango_m.max(), rango_b.min(), rango_b.max()], \n                    origin='lower', cmap='viridis', aspect='auto')\ncontours = ax.contour(M, B, Z, colors='black', levels=10)\nax.set_xlabel('m (pendiente)')\nax.set_ylabel('b (intercepto)')\nax.set_title('Descenso de Gradiente')\n\n# Ejecutar el descenso de gradiente\nm_inicio, b_inicio = 3, -2.5  # Punto inicial\neta = 0.015  # Tasa de aprendizaje\npasos = 15  # Número de pasos a mostrar\n\n# Calcular la trayectoria completa de antemano\ntrayectoria_m = [m_inicio]\ntrayectoria_b = [b_inicio]\n\nm_actual, b_actual = m_inicio, b_inicio\nfor _ in range(pasos):\n    grad_m = derivada_m(m_actual, b_actual, x, y)\n    grad_b = derivada_b(m_actual, b_actual, x, y)\n    \n    # Actualizar parámetros\n    m_actual -= eta * grad_m\n    b_actual -= eta * grad_b\n    \n    # Guardar trayectoria\n    trayectoria_m.append(m_actual)\n    trayectoria_b.append(b_actual)\n\n# Inicializar elementos de la animación\nline, = ax.plot([], [], 'ro-', linewidth=2, markersize=8)\npoints = ax.scatter([], [], color='red', s=50)\n\n# Función de inicialización\ndef init():\n    line.set_data([], [])\n    points.set_offsets(np.empty((0, 2)))\n    return line, points\n\n# Función de actualización para cada frame\ndef update(frame):\n    line.set_data(trayectoria_m[:frame+1], trayectoria_b[:frame+1])\n    points.set_offsets(np.column_stack((trayectoria_m[:frame+1], trayectoria_b[:frame+1])))\n    return line, points\n\n# Crear animación\nani = animation.FuncAnimation(fig, update, frames=len(trayectoria_m),\n                              init_func=init, blit=True, interval=300)\n\nHTML(ani.to_jshtml())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n(a) Descenso de gradiente para regresión lineal\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigura 2\n\n\n\n\nEn la Figura 2 se muestra el descenso de gradiente para regresión lineal. Graficamos unos pocos pasos del descenso de gradiente. Ya podemos apreciar un posible problema: El descenso avanza rápidamente en la dirección de la pendiente, pero luego se detiene y avanza lentamente en la dirección del intercepto. Esto es porque el gradiente es más pronunciado en la dirección de la pendiente que en la dirección del intercepto. Veremos más adelante cómo intentar mejorar esto.",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "12. Descenso de Gradiente y Auto-diferenciación"
    ]
  },
  {
    "objectID": "12_gradient_descent_and_autodiff.html#ejemplo-2d-de-modelo-gabor-paisajes-con-mínimos-locales",
    "href": "12_gradient_descent_and_autodiff.html#ejemplo-2d-de-modelo-gabor-paisajes-con-mínimos-locales",
    "title": "Descenso de Gradiente y Auto-diferenciación",
    "section": "Ejemplo 2D de modelo Gabor: paisajes con mínimos locales",
    "text": "Ejemplo 2D de modelo Gabor: paisajes con mínimos locales\nEl ejemplo de arriba fue para una función de pérdida convexa. En general las funciones de pérdida de las redes neuronales no tienen esta propiedad. Por lo tanto, el “paisaje” de la pérdida puede ser complejo, presentando múltiples mínimos locales y puntos de silla (saddle points). Esto dificulta la optimización, ya que el Descenso de Gradiente puede quedar atrapado en un mínimo local que no es el mínimo global. Usaremos una función sencilla con esas características llamada función de Gabor para ilustrar el problema.\nUsamos una función Gabor 1D como modelo, donde ajustaremos dos de sus parámetros. Consideramos \\(\\sigma\\) y \\(\\omega\\) como parámetros fijos conocidos. Optimizaremos la amplitud \\(A\\) y la fase \\(\\phi\\): \\[\nf(x; \\phi_0,\\phi_1) = e^{-\\frac{(\\phi_0 + \\phi_1 x)^2}{32}}\\cos(\\phi_0 + \\phi_1 x + \\phi).\n\\] Ajustamos \\((\\phi_0,\\phi_1)\\) a datos \\((x_i, y_i)\\) generados usando valores verdaderos \\((\\phi_0^*,\\phi_1^*)\\), en este ejemplo.\nMinimizamos el Error Cuadrático Medio entre las predicciones del modelo Gabor y los datos \\(y_i\\). Siendo \\(N\\) el número de puntos \\(x_i\\): \\[\n\\mathcal{L}(\\phi_0,\\phi_1) = \\frac{1}{N}\\sum_{i=1}^N (y_i - f(x_i; \\phi_0,\\phi_1))^2.\n\\] La naturaleza oscilatoria del término \\(\\cos(\\phi_0 + \\phi_1 x + \\phi)\\), multiplicada por la amplitud \\(A\\), crea un paisaje de pérdida no convexo con múltiples valles (mínimos locales) y puntos de silla.\nPara entender el paisaje de la pérdida, generamos un gráfico de contorno de \\(\\mathcal{L}(\\phi_0,\\phi_1)\\) evaluando la pérdida en una rejilla de valores de \\(\\phi_0\\) y \\(\\phi_1\\). Este gráfico muestra las regiones de baja pérdida (valles) y las zonas de puntos de silla. Adicionalmente, se pueden trazar las trayectorias seguidas por el algoritmo de Descenso de Gradiente partiendo desde diferentes puntos iniciales \\((\\phi_{0,0}, \\phi_{1,0})\\).\n\n\nCódigo\n# Definir función de Gabor y generar datos\ndef gabor(x, phi_0, phi_1, sigma=5.0):\n    return np.exp(-(phi_0 + phi_1*x)**2/(2*sigma**2)) * np.sin(phi_0 + phi_1*x)\n\ndef d_gabor_dx(x, sigma=5.0):\n    return - x * np.exp(-x**2/(2*sigma**2))/sigma**2 * np.sin(x) + \\\n        np.exp(-x**2/(2*sigma**2)) * np.cos(x)\n\n# Generar datos sintéticos\nnp.random.seed(42)\nN = 100\nx = np.linspace(-3, 3, N)\nphi_0_verdadero, phi_1_verdadero = 2.0, 1.0\nsigma = 5.0  # Parámetros fijos\ny = gabor(x, phi_0_verdadero, phi_1_verdadero, sigma) + np.random.normal(0, 1, N)\n\n# Definir función de pérdida y sus derivadas\ndef perdida_ecm(phi_0, phi_1, x, y, sigma=5.0):\n    y_pred = gabor(x, phi_0, phi_1, sigma)\n    return np.mean((y - y_pred)**2)\n\ndef derivada_phi_0(phi_0, phi_1, x, y, sigma=5.0):\n    y_pred = gabor(x, phi_0, phi_1, sigma)\n    return -2*np.mean((y - y_pred) * d_gabor_dx(phi_0 + phi_1*x, sigma))\n\ndef derivada_phi_1(phi_0, phi_1, x, y, sigma=5.0):\n    y_pred = gabor(x, phi_0, phi_1, sigma)\n    return -2*np.mean((y - y_pred) * d_gabor_dx(phi_0 + phi_1*x, sigma)*x)\n\n# Crear malla para gráfico de contorno\nrango_phi_0 = np.linspace(-1, 4, 100)\nrango_phi_1 = np.linspace(-2, 4, 100)\nPhi_0_grid, Phi_1_grid = np.meshgrid(rango_phi_0, rango_phi_1)\nZ = np.array([perdida_ecm(p_0, p_1, x, y) for p_0, p_1 in zip(Phi_0_grid.ravel(), Phi_1_grid.ravel())]).reshape(Phi_0_grid.shape)\n\n# Configurar figura\nfig, ax = plt.subplots(figsize=(10, 6))\nheatmap = ax.imshow(Z, extent=[rango_phi_0.min(), rango_phi_0.max(), rango_phi_1.min(), rango_phi_1.max()], \n                    origin='lower', cmap='viridis', aspect='auto')\ncontours = ax.contour(Phi_0_grid, Phi_1_grid, Z, colors='black', levels=10)\nax.set_xlabel(r'$\\phi_0$')\nax.set_ylabel(r'$\\phi_1$')\nax.set_title('Descenso de Gradiente en Modelo Gabor')\n\n# Definir puntos iniciales para diferentes trayectorias\npuntos_iniciales = [\n    (2.3, 1.0),    # (φ_0, φ_1) - Punto 1\n    (1.0, 3.0),    # Punto 2\n    (3.0, 0.0),    # Punto 3\n    (1.0, -1.0),   # Punto 4\n    (2.5, -1.5)    # Punto 5\n]\n\ncolores = ['r', 'b', 'g', 'y', 'c']\neta = 0.1  # Tasa de aprendizaje\npasos = 30  # Número de pasos a mostrar\n\n# Calcular todas las trayectorias de antemano\ntodas_trayectorias = []\n\nfor phi_0_inicio, phi_1_inicio in puntos_iniciales:\n    trayectoria_phi_0 = [phi_0_inicio]\n    trayectoria_phi_1 = [phi_1_inicio]\n    \n    phi_0_actual, phi_1_actual = phi_0_inicio, phi_1_inicio\n    for _ in range(pasos):\n        grad_phi_0 = derivada_phi_0(phi_0_actual, phi_1_actual, x, y)\n        grad_phi_1 = derivada_phi_1(phi_0_actual, phi_1_actual, x, y)\n        \n        # Actualizar parámetros\n        phi_0_actual -= eta * grad_phi_0\n        phi_1_actual -= eta * grad_phi_1\n        \n        # Guardar trayectoria\n        trayectoria_phi_0.append(phi_0_actual)\n        trayectoria_phi_1.append(phi_1_actual)\n    \n    todas_trayectorias.append((trayectoria_phi_0, trayectoria_phi_1))\n\n# Inicializar elementos de la animación\nlines = []\npoints_collection = []\n\nfor i, color in enumerate(colores):\n    line, = ax.plot([], [], f'{color}-', linewidth=2, markersize=8)\n    points = ax.scatter([], [], color=color, s=50)\n    lines.append(line)\n    points_collection.append(points)\n\n# Función de inicialización\ndef init():\n    for line in lines:\n        line.set_data([], [])\n    for points in points_collection:\n        points.set_offsets(np.empty((0, 2)))\n    return lines + points_collection\n\n# Función de actualización para cada frame\ndef update(frame):\n    for i, ((trayectoria_phi_0, trayectoria_phi_1), line, points) in enumerate(zip(todas_trayectorias, lines, points_collection)):\n        line.set_data(trayectoria_phi_0[:frame+1], trayectoria_phi_1[:frame+1])\n        points.set_offsets(np.column_stack((trayectoria_phi_0[:frame+1], trayectoria_phi_1[:frame+1])))\n    \n    return lines + points_collection\n\n# Crear animación\nani = animation.FuncAnimation(fig, update, frames=pasos+1,\n                              init_func=init, blit=True, interval=300)\n\nHTML(ani.to_jshtml())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n(a) Descenso de gradiente para modelo Gabor\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigura 3\n\n\n\n\nEn la Figura 3 se muestra el paisaje de la pérdida para el modelo Gabor, y las trayectorias de convergencia para diferentes puntos iniciales. Vemos que diferentes puntos van hacia diferentes mínimos locales, lo que dificulta la convergencia al mínimo global.",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "12. Descenso de Gradiente y Auto-diferenciación"
    ]
  },
  {
    "objectID": "12_gradient_descent_and_autodiff.html#descenso-de-gradiente-estocástico-sgd-y-por-mini-lotes",
    "href": "12_gradient_descent_and_autodiff.html#descenso-de-gradiente-estocástico-sgd-y-por-mini-lotes",
    "title": "Descenso de Gradiente y Auto-diferenciación",
    "section": "Descenso de Gradiente Estocástico (SGD) y por mini-lotes",
    "text": "Descenso de Gradiente Estocástico (SGD) y por mini-lotes\nCuando el conjunto de datos es muy grande, calcular el gradiente sobre todos los \\(N\\) ejemplos en cada paso puede ser computacionalmente muy costoso. Por ello, se utilizan variantes del GD que usan un subconjunto de los datos para aproximar el gradiente a cada paso. Esta será sólo una aproximación, y nos da una estima ruidosa del gradiente de la pérdida ideal. Pero esto tiene una gran ventaja: Ese ruido puede empujar el algoritmo a saltar de mínimos locales poco profundos. Veamos dos variaciones de esa idea:\n\nDescenso de Gradiente Estocástico (SGD) Realiza una actualización de parámetros por cada ejemplo individual \\((x_i, y_i)\\). En cada paso, el gradiente se estima usando solo ese único ejemplo: \\(\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} \\mathcal{L}_{\\text{indiv}}(\\theta; x_i, y_i)\\), donde \\(\\mathcal{L}_{\\text{indiv}}\\) es la pérdida calculada para ese punto. Luego repite para todos los puntos \\((x_i, y_i)\\) y vuelve a empezar.\n\nVentajas: Actualizaciones muy rápidas y bajo coste computacional por actualización. La alta varianza en la estimación del gradiente puede ayudar a escapar de mínimos locales “malos”.\nDesventajas: Al usar un único dato para calcular el gradiente, la trayectoria de convergencia es muy ruidosa (alta varianza). Puede que nunca converja exactamente al mínimo, sino que oscile alrededor de este.\n\nDescenso de Gradiente por Mini-lotes: Es un compromiso entre los dos anteriores y el método más común en Deep Learning. El gradiente se calcula sobre un pequeño subconjunto aleatorio de datos llamado mini-batch (lote), de tamaño \\(B\\) (e.g., \\(B=32, 64, 128\\)). Luego repite para todos los lotes y vuelve a empezar.\n\nVentajas: Reduce significativamente la varianza del gradiente comparado con SGD, llevando a una convergencia más estable. Mantiene una alta eficiencia computacional aprovechando la paralelización de hardware (GPUs).\nDesventajas: Introduce un nuevo hiperparámetro (tamaño del batch, \\(B\\)).\n\n\nEn la práctica, el descenso de gradiente por mini-lotes es el método más común y eficiente. Es el usado para entrenar las redes neuronales famosas que han impulsado el interés en el aprendizaje automático.\nA continuación el pseudocódigo para el descenso de gradiente por mini-lotes:\n# Inicializar parámetros\ntheta = theta_0\n\n# Bucle de entrenamiento\nfor epoch in range(num_epochs):\n    # Mezclar datos\n    np.random.shuffle(data)\n    \n    # Bucle sobre lotes\n    for batch in data:\n        # Calcular gradiente\n        grad = compute_gradient(batch)\n        \n        # Actualizar parámetros\n        theta = theta - eta * grad",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "12. Descenso de Gradiente y Auto-diferenciación"
    ]
  },
  {
    "objectID": "12_gradient_descent_and_autodiff.html#momentum",
    "href": "12_gradient_descent_and_autodiff.html#momentum",
    "title": "Descenso de Gradiente y Auto-diferenciación",
    "section": "Momentum",
    "text": "Momentum\nOtra estrategia para atacar el problema de los mínimos locales y la oscilación alrededor de mínimos, es cambiar el proceso de aprendizaje. Es decir, modificamos la forma como se actualizan los pesos a cada paso.\nEl método de Momentum introduce un término de “velocidad” \\(v\\) que acumula una media móvil exponencial de los gradientes pasados. Ayuda a acelerar el descenso en direcciones consistentes y amortigua las oscilaciones en direcciones donde el gradiente cambia rápidamente (típico en valles estrechos y alargados).\nLa actualización se modifica como sigue (donde \\(\\theta\\) representa los parámetros del modelo, como \\(m\\) y \\(b\\)):\n\\[\n\\begin{aligned}\nv_t &\\leftarrow \\beta\\,v_{t-1} + (1-\\beta)\\,\\nabla\\mathcal{L}(\\theta_{t-1}),\\\\\n\\theta_t &\\leftarrow \\theta_{t-1} - \\eta\\,v_t.\n\\end{aligned}\n\\]\n\n\\(v_t\\) es el vector de velocidad en el paso \\(t\\).\n\\(\\beta\\) es el coeficiente de momentum (usualmente cercano a 0.9). Controla cuánto del momentum anterior se conserva. Un \\(\\beta\\) alto significa que los gradientes pasados tienen más influencia.\n\\(\\nabla\\mathcal{L}(\\theta_{t-1})\\) es el gradiente calculado en el paso actual (puede ser batch, mini-batch o stochastic).\n\nA cantidades que se acumulan de la manera como lo hace \\(v_t\\) se les llama media móvil exponencial.\nEsto es burdamente similar a una bola pesada rodando por una pendiente. Acumula momento, lo que le permite pasar por pequeños baches y acelerar en las bajadas consistentes. A pesar de llamarse momentum, no es exactamente el momentum definido en la física.\nA continuación el pseudocódigo para el descenso de gradiente con momentum en el problema de regresión lineal:\n# Inicializar velocidades\nv_m, v_b = 0.0, 0.0\nbeta = 0.9 # Coeficiente de momentum\neta = 0.01 # Tasa de aprendizaje\n\n# Bucle de entrenamiento (puede ser sobre épocas y mini-batches)\nfor iteration in range(num_iterations):\n    # Calcular gradientes (usando batch, mini-batch, o SGD)\n    # grad_m = ∂L/∂m, grad_b = ∂L/∂b\n    grad_m, grad_b = compute_gradient(m, b, data_batch)\n\n    # Actualizar velocidades\n    v_m = beta * v_m + (1 - beta) * grad_m\n    v_b = beta * v_b + (1 - beta) * grad_b\n\n    # Actualizar parámetros usando las velocidades\n    m -= eta * v_m\n    b -= eta * v_b",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "12. Descenso de Gradiente y Auto-diferenciación"
    ]
  },
  {
    "objectID": "12_gradient_descent_and_autodiff.html#adam-adaptive-moment-estimation",
    "href": "12_gradient_descent_and_autodiff.html#adam-adaptive-moment-estimation",
    "title": "Descenso de Gradiente y Auto-diferenciación",
    "section": "Adam (Adaptive Moment Estimation)",
    "text": "Adam (Adaptive Moment Estimation)\nCuando hicimos el ejemplo de regresión lineal, vimos que la convergencia hacia el mínimo en una dimensión era mucho más rápida que en la otra. Adam es un optimizador que intenta resolver este problema ajustando automáticamente la tasa de aprendizaje para cada parámetro individualmente.\nAdam es un optimizador muy popular que combina dos ideas principales:\n\nMomentum: Utiliza una media móvil exponencial del gradiente (primer momento, \\(m_t\\)), similar al método de Momentum.\nAdaptación de Tasa de Aprendizaje: Utiliza una media móvil exponencial del cuadrado del gradiente (segundo momento, \\(v_t\\)). Esto reduce la diferencia entre las tasas de aprendizaje más rápidas y las más lentas.\n\nAdemás, aplica una corrección del sesgo (bias correction) a ambas estimaciones de momentos para contrarrestar su tendencia a estar sesgadas hacia cero durante las primeras iteraciones.\nLas ecuaciones de actualización son: \\[\n\\begin{aligned}\nm_t &= \\beta_1 m_{t-1} + (1-\\beta_1)\\,g_t & \\text{(Estimación del 1er momento)} \\\\\nv_t &= \\beta_2 v_{t-1} + (1-\\beta_2)\\,g_t^2 & \\text{(Estimación del 2do momento)} \\\\\n\\hat m_t &= \\frac{m_t}{1-\\beta_1^t} & \\text{(Corrección del sesgo 1er momento)} \\\\\n\\hat v_t &= \\frac{v_t}{1-\\beta_2^t} & \\text{(Corrección del sesgo 2do momento)} \\\\\n\\theta_t &\\leftarrow \\theta_{t-1} - \\eta\\,\\frac{\\hat m_t}{\\sqrt{\\hat v_t}+\\epsilon} & \\text{(Actualización del parámetro } \\theta \\text{)}\n\\end{aligned}\n\\]\nDonde: * \\(t\\) es el número de iteración (paso de actualización). * \\(g_t = \\nabla_{\\theta} \\mathcal{L}(\\theta_{t-1})\\) es el gradiente en el paso \\(t\\). * \\(\\beta_1\\) es el coeficiente de decaimiento para el primer momento (usualmente ~0.9). * \\(\\beta_2\\) es el coeficiente de decaimiento para el segundo momento (usualmente ~0.999). * \\(\\eta\\) es la tasa de aprendizaje global (e.g., 1e-3). * \\(\\epsilon\\) es una constante pequeña (usualmente ~1e-8) para evitar la división por cero y mejorar la estabilidad numérica.\nA continuación se muestra un pseudocódigo para el descenso de gradiente con Adam en el problema de regresión lineal.\n# Inicializar momentos\nm_m, m_b = 0.0, 0.0  # Primer momento (media móvil del gradiente)\nv_m, v_b = 0.0, 0.0  # Segundo momento (media móvil del cuadrado del gradiente)\nbeta1 = 0.9          # Coeficiente de decaimiento para el primer momento\nbeta2 = 0.999        # Coeficiente de decaimiento para el segundo momento\nepsilon = 1e-8       # Constante para evitar división por cero\neta = 0.01           # Tasa de aprendizaje\nt = 0                # Contador de iteraciones\n\n# Bucle de entrenamiento\nfor iteration in range(num_iterations):\n    t += 1\n    \n    # Calcular gradientes (usando batch, mini-batch, o SGD)\n    grad_m, grad_b = compute_gradient(m, b, data_batch)\n    \n    # Actualizar primer momento (media móvil del gradiente)\n    m_m = beta1 * m_m + (1 - beta1) * grad_m\n    m_b = beta1 * m_b + (1 - beta1) * grad_b\n    \n    # Actualizar segundo momento (media móvil del cuadrado del gradiente)\n    v_m = beta2 * v_m + (1 - beta2) * (grad_m**2)\n    v_b = beta2 * v_b + (1 - beta2) * (grad_b**2)\n    \n    # Corregir el sesgo\n    m_m_corrected = m_m / (1 - beta1**t)\n    m_b_corrected = m_b / (1 - beta1**t)\n    v_m_corrected = v_m / (1 - beta2**t)\n    v_b_corrected = v_b / (1 - beta2**t)\n    \n    # Actualizar parámetros\n    m -= eta * m_m_corrected / (np.sqrt(v_m_corrected) + epsilon)\n    b -= eta * m_b_corrected / (np.sqrt(v_b_corrected) + epsilon)",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "12. Descenso de Gradiente y Auto-diferenciación"
    ]
  },
  {
    "objectID": "12_gradient_descent_and_autodiff.html#algoritmo-de-backpropagation-modo-reverso-de-ad",
    "href": "12_gradient_descent_and_autodiff.html#algoritmo-de-backpropagation-modo-reverso-de-ad",
    "title": "Descenso de Gradiente y Auto-diferenciación",
    "section": "Algoritmo de Backpropagation (Modo Reverso de AD)",
    "text": "Algoritmo de Backpropagation (Modo Reverso de AD)\nEn redes neuronales y otros modelos con muchos parámetros, la diferenciación automática se implementa de forma eficiente mediante el modo reverso (reverse mode), cuyo algoritmo más conocido es backpropagation. Existe también el modo directo (forward mode), que es menos eficiente pero puede ser útil en ciertas situaciones, y no lo veremos aquí por falta de tiempo.\nEl modo reverso calcula todas las derivadas de una única salida (escalar, como la función de pérdida \\(\\mathcal{J}\\)) con respecto a todas las entradas y parámetros intermedios. Lo hace propagando las derivadas hacia atrás, desde la salida hacia la entrada, aplicando eficientemente la regla de la cadena sobre el grafo computacional definido por la evaluación de la función (el forward pass).\nEl proceso tiene dos fases:\n\nForward pass (Paso hacia adelante)\n\nPartimos de la entrada \\(x\\) y calculamos, capa a capa, las activaciones intermedias y la salida final (y la pérdida \\(\\mathcal{J}\\)). \\[\nz^{(1)} = W^{(1)} x + b^{(1)}, \\quad a^{(1)} = \\sigma(z^{(1)}), \\quad z^{(2)} = W^{(2)} a^{(1)} + b^{(2)}, \\dots, \\quad \\mathcal{J} = \\text{Loss}(\\text{output}, \\text{target})\n\\]\nSe almacenan en memoria todas las variables intermedias (\\(z^{(\\ell)}\\), \\(a^{(\\ell)}\\), \\(W^{(\\ell)}\\), \\(x\\)) que influyen en el resultado final, ya que serán necesarias para calcular las derivadas en el paso inverso.\n\nBackward pass (Paso hacia atrás)\n\nSe inicializa con el gradiente de la pérdida respecto a sí misma (\\(\\frac{\\partial \\mathcal{J}}{\\partial \\mathcal{J}} = 1\\)) o respecto a la salida final de la red antes de la función de pérdida.\nSe propaga el gradiente hacia atrás capa por capa (\\(\\ell = L, L-1, \\dots, 1\\)). Para cada capa \\(\\ell\\):\n\nSe parte del gradiente respecto a la activación de la capa, \\(\\frac{\\partial \\mathcal{J}}{\\partial a^{(\\ell)}}\\) (obtenido de la capa \\(\\ell+1\\), o inicial si \\(\\ell=L\\)).\nSe calcula el gradiente respecto a la entrada lineal de la capa \\(\\ell\\), denotado \\(\\delta^{(\\ell)}\\): \\[\n\\delta^{(\\ell)} = \\frac{\\partial \\mathcal{J}}{\\partial z^{(\\ell)}} = \\frac{\\partial \\mathcal{J}}{\\partial a^{(\\ell)}} \\odot \\sigma'(z^{(\\ell)})\n\\] donde \\(\\odot\\) es el producto elemento a elemento (Hadamard) y \\(\\sigma'\\) es la derivada de la función de activación \\(\\sigma\\) aplicada elemento a elemento a \\(z^{(\\ell)}\\). En el caso de ReLU, \\(\\sigma'(z^{(\\ell)}) = 1\\) para \\(z^{(\\ell)} &gt; 0\\) y \\(\\sigma'(z^{(\\ell)}) = 0\\) para \\(z^{(\\ell)} \\leq 0\\).\nSe calculan los gradientes respecto a los parámetros de la capa \\(\\ell\\) usando \\(\\delta^{(\\ell)}\\) y la activación de la capa anterior \\(a^{(\\ell-1)}\\) (guardada durante el forward pass): \\[\n\\frac{\\partial \\mathcal{J}}{\\partial W^{(\\ell)}} = \\delta^{(\\ell)} \\, (a^{(\\ell-1)})^T\n\\] \\[\n\\frac{\\partial \\mathcal{J}}{\\partial b^{(\\ell)}} = \\delta^{(\\ell)} \\quad \\text{(sumado sobre las muestras del batch si aplica)}\n\\]\nSe calcula el gradiente que se propagará a la capa anterior \\(\\ell-1\\): \\[\n\\frac{\\partial \\mathcal{J}}{\\partial a^{(\\ell-1)}} = (W^{(\\ell)})^T \\, \\delta^{(\\ell)}\n\\] Este \\(\\frac{\\partial \\mathcal{J}}{\\partial a^{(\\ell-1)}}\\) será el punto de partida para los cálculos de la capa \\(\\ell-1\\).\n\nEste proceso continúa hasta calcular los gradientes respecto a todos los parámetros \\(W^{(\\ell)}, b^{(\\ell)}\\) de la red (y respecto a la entrada \\(x\\) si fuera necesario).\n\n\nEficiencia: Backpropagation (modo reverso) es muy eficiente cuando tenemos muchas variables de entrada (parámetros) y una única salida escalar (la pérdida), ya que calcula todos los gradientes \\(\\frac{\\partial L}{\\partial \\theta_i}\\) con un coste computacional bajo (aproximadamente entre 2 y 3 veces el coste del forward pass). Por eso es el método estándar para entrenar redes neuronales.\n# Función de activación y su derivada\ndef relu(x):\n    return np.maximum(0, x)\ndef drelu(x):\n    return (x &gt; 0).astype(int)\n\n# Dimensiones de la red\ndim_entrada = 1\ndim_oculta = 2\ndim_salida = 1\n\n# Ejemplo de entrada y objetivo\nx = np.array([[0.5]])  # forma (1, 1)\ny_real = np.array([[1.0]])  # forma (1, 1)\n\n# Parámetros (inicialización aleatoria para demostración)\nW1 = np.random.randn(dim_oculta, dim_entrada)  # (2, 1)\nb1 = np.random.randn(dim_oculta, 1)            # (2, 1)\nW2 = np.random.randn(dim_oculta, dim_oculta)   # (2, 2)\nb2 = np.random.randn(dim_oculta, 1)            # (2, 1)\nW3 = np.random.randn(dim_salida, dim_oculta)   # (1, 2)\nb3 = np.random.randn(dim_salida, 1)            # (1, 1)\n\n# ----- Paso hacia adelante (forward) -----\nz1 = W1 @ x + b1         # (2, 1)\na1 = relu(z1)            # (2, 1)\nz2 = W2 @ a1 + b2        # (2, 1)\na2 = relu(z2)            # (2, 1)\nz3 = W3 @ a2 + b3        # (1, 1)\ny_pred = z3              # (1, 1)\n\n# Pérdida: Error cuadrático medio\nperdida = 0.5 * np.sum((y_pred - y_real) ** 2)\n\n# ----- Paso hacia atrás (backward) -----\n# dJ/dy_pred\ndJ_dy_pred = y_pred - y_real  # (1, 1)\n\n# Gradientes de la capa de salida\n# y_pred = z3\ndJ_dz3 = dJ_dy_pred           # (1, 1)\ndJ_dW3 = dJ_dz3 @ a2.T        # (1, 2)\ndJ_db3 = dJ_dz3               # (1, 1)\n\n# Segunda capa oculta\nda2_dz2 = drelu(z2)           # (2, 1)\ndJ_da2 = W3.T @ dJ_dz3        # (2, 1)\ndJ_dz2 = dJ_da2 * da2_dz2     # (2, 1)\ndJ_dW2 = dJ_dz2 @ a1.T        # (2, 2)\ndJ_db2 = dJ_dz2               # (2, 1)\n\n# Primera capa oculta\nda1_dz1 = drelu(z1)           # (2, 1)\ndJ_da1 = W2.T @ dJ_dz2        # (2, 1)\ndJ_dz1 = dJ_da1 * da1_dz1     # (2, 1)\ndJ_dW1 = dJ_dz1 @ x.T         # (2, 1)\ndJ_db1 = dJ_dz1               # (2, 1)\n\n# Ahora dJ_dW1, dJ_db1, dJ_dW2, dJ_db2, dJ_dW3, dJ_db3 son los gradientes para todos los parámetros\nprint(\"dJ/dW1:\", dJ_dW1)\nprint(\"dJ/db1:\", dJ_db1)\nprint(\"dJ/dW2:\", dJ_dW2)\nprint(\"dJ/db2:\", dJ_db2)\nprint(\"dJ/dW3:\", dJ_dW3)\nprint(\"dJ/db3:\", dJ_db3)\n(Nota: Librerías modernas como PyTorch, TensorFlow y JAX implementan AD de forma automática. El usuario define el paso hacia adelante (la estructura del modelo y el cálculo), y la librería se encarga de calcular los gradientes eficientemente usando backpropagation.)",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "12. Descenso de Gradiente y Auto-diferenciación"
    ]
  },
  {
    "objectID": "12_gradient_descent_and_autodiff.html#el-problema-de-la-varianza-grande-o-pequeña",
    "href": "12_gradient_descent_and_autodiff.html#el-problema-de-la-varianza-grande-o-pequeña",
    "title": "Descenso de Gradiente y Auto-diferenciación",
    "section": "El problema de la varianza grande o pequeña",
    "text": "El problema de la varianza grande o pequeña\nLa clave está en controlar la varianza de las salidas de cada capa (activaciones) y de los gradientes que fluyen hacia atrás. Aquí, \\(D_\\ell\\) representa el ‘fan-in’ de la capa \\(\\ell\\), es decir, el número de neuronas en la capa anterior que alimentan a una neurona de la capa \\(\\ell\\).\nLos pesos se inicializan de forma aleatoria, lo que significa que los pesos tendrán una varianza \\(\\sigma^2\\), que podemos escoger y puede ser distinta para cada capa. Resulta que el valor de esta varianza es importante para el entrenamiento de redes profundas ya que:\n\nVarianza demasiado grande (e.g., \\(\\sigma^2 = 5 / D_\\ell\\)): Las activaciones tienden a crecer exponencialmente capa tras capa (activaciones explosivas). Como la activación de cada capa se multiplica por la de la capa anterior, esto puede llevar a la saturación de las funciones de activación (como sigmoid o tanh, o incluso valores enormes con ReLU) y a gradientes gigantescos (gradientes explosivos), desestabilizando el entrenamiento.\nVarianza demasiado pequeña (e.g., \\(\\sigma^2 = 0.1 / D_\\ell\\)): Las activaciones tienden a atenuarse exponencialmente capa tras capa (activaciones desvanecientes). Como la activación de cada capa se multiplica por la de la capa anterior, esto provoca que las señales (y sus gradientes) que llegan a las primeras capas sean minúsculas (gradientes desvanecientes), impidiendo que esas capas aprendan (como el gradiente es pequeño, los pesos no se actualizan).\n\nLa @fig_varianza_activaciones ilustra cómo evoluciona la varianza de las activaciones a través de las capas para diferentes inicializaciones en una red con activación ReLU:\n\nPequeña (\\(\\sigma^2 = 0.1 / D_\\ell\\)): La varianza de las activaciones se atenúa rápidamente hacia cero.\nXavier/Glorot (\\(\\sigma^2 = 1 / D_\\ell\\)): Mantiene la varianza más estable, pero está diseñado teóricamente para activaciones lineales o simétricas (como tanh). Con ReLU, tiende a reducir la varianza. (La versión original de Xavier promedia fan-in y fan-out: \\(\\sigma^2 = 2 / (D_\\ell + D_{\\ell+1})\\)).\nHe (\\(\\sigma^2 = 2 / D_\\ell\\)): Diseñada específicamente para ReLU. Mantiene la varianza de las activaciones aproximadamente constante a lo largo de las capas. El factor 2 extra (comparado con Xavier) compensa el hecho de que ReLU anula ~la mitad de las entradas (las negativas), lo que de otro modo reduciría la varianza.\nGrande (\\(\\sigma^2 = 5 / D_\\ell\\)): La varianza explota brutalmente, saliéndose de escala y haciendo inviable el aprendizaje.\n\n\n\nCódigo\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\n# Parameters\nn_layers = 20\nfan_in = 100  # D_\\ell\nn_samples = 1000\n\n# Variance schemes to compare\ninit_schemes = {\n    r\"Pequeña ($0.1/D_\\ell$)\": 0.1,\n    r\"Xavier/Glorot ($1/D_\\ell$)\": 1.0,\n    r\"He ($2/D_\\ell$)\": 2.0,\n    r\"Grande ($5/D_\\ell$)\": 5.0,\n}\ncolors = [\"gray\", \"blue\", \"green\", \"red\"]\n\n# Store results\nall_variances = {}\n\n# Simulate for each scheme\nfor (label, scale), color in zip(init_schemes.items(), colors):\n    variances = []\n    # Input: standard normal\n    x = np.random.randn(fan_in, n_samples)\n    var = np.var(x)\n    variances.append(var)\n    for layer in range(n_layers):\n        # Initialize weights for this layer\n        W = np.random.randn(fan_in, fan_in) * np.sqrt(scale / fan_in)\n        # Linear + ReLU\n        x = W @ x\n        x = np.maximum(0, x)\n        var = np.var(x)\n        variances.append(var)\n    all_variances[label] = variances\n\n# Plot\nplt.figure(figsize=(8, 5))\nfor label, color in zip(init_schemes.keys(), colors):\n    plt.plot(all_variances[label], label=label, color=color, linewidth=2)\nplt.xlabel(\"Capa (layer)\")\nplt.ylabel(\"Varianza de activaciones\")\nplt.title(\"Evolución de la varianza de activaciones según inicialización (ReLU)\")\nplt.legend()\nplt.yscale('log')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nVarianza de las activaciones según inicialización\n\n\n\n\nEsto se puede entender porque la varianza de \\(z^{(\\ell)}\\) está dada por:\n\\[\n\\sigma^2_{z^{(\\ell)}} = \\frac{1}{2}D_\\ell \\sigma_{\\ell}^2\\sigma_{z^{(\\ell - 1)}}^2\\,,\n\\] {#eq_varianza_z}\ndonde \\(\\sigma_\\ell^2\\) es la varianza de los pesos de la capa \\(\\ell\\). Esta relación también nos da una pista de cómo resolver el problema.\n\n\n\n\n\n\nDerivación de la relación entre varianzas\n\n\n\n\n\nQueremos encontrar una relación entre la varianza de las preactivaciones de la capa \\(\\ell\\) y la varianza de las activaciones de la capa \\(\\ell - 1\\). Sea \\(\\sigma_{\\ell}^2\\) la varianza de los pesos de la capa \\(\\ell\\) y \\(\\sigma_{z^{(\\ell)}}^2\\) la varianza de las preactivaciones de la capa \\(\\ell\\).\nPara empezar, calculamos el valor esperado de la preactivación \\(z^{(\\ell)}\\) en la capa \\(\\ell\\):\n\\[\n\\langle z^{(\\ell)} \\rangle = \\langle W^{(\\ell)} \\cdot a^{(\\ell-1)} + b^{(\\ell)} \\rangle = \\langle W^{(\\ell)} \\rangle \\cdot \\langle a^{(\\ell-1)} \\rangle = 0\\,.\n\\]\nEntonces su varianza es:\n\\[\n\\text{Var}(z_{i}^{(\\ell)}) = \\langle (z_{i}^{(\\ell)})^2 \\rangle = \\left\\langle \\left(\\sum_{j=1}^{D_\\ell} W_{ij}^{(\\ell)} a_{j}^{(\\ell-1)}\\right)^2 \\right\\rangle = \\sum_{j = 1}^{D_\\ell} \\langle (W_{ij}^{(\\ell)})^2 \\rangle \\langle (a_{j}^{(\\ell-1)})^2 \\rangle = \\sigma_{\\ell}^2 \\sum_{j=1}^{D_\\ell}\\langle (a_{j}^{(\\ell-1)})^2 \\rangle\\,.\n\\]\nAhora bien, como \\(ReLU\\) corta a cero las entradas negativas, la varianza de las activaciones \\(a^{(\\ell)}\\) es la mitad de la varianza de \\(z^{(\\ell)}\\) (ya que la integral de \\(x^2\\) desde 0 hasta \\(\\infty\\) es la mitad de la integral desde \\(-\\infty\\) hasta \\(\\infty\\)). Entonces:\n\\[\n\\sigma_{z^{(\\ell)}}^2 = \\frac{1}{2}D_\\ell \\sigma_{\\ell}^2\\sigma_{z^{(\\ell - 1)}}^2\\,.\n\\]",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "12. Descenso de Gradiente y Auto-diferenciación"
    ]
  },
  {
    "objectID": "12_gradient_descent_and_autodiff.html#inicialización-para-el-forward-pass-sigma_ell2-2d_ell",
    "href": "12_gradient_descent_and_autodiff.html#inicialización-para-el-forward-pass-sigma_ell2-2d_ell",
    "title": "Descenso de Gradiente y Auto-diferenciación",
    "section": "Inicialización para el forward pass: \\(\\sigma_\\ell^2 = 2/D_\\ell\\)",
    "text": "Inicialización para el forward pass: \\(\\sigma_\\ell^2 = 2/D_\\ell\\)\nPara asegurar que la varianza de las activaciones \\(a^{(\\ell)}\\) se mantenga aproximadamente constante durante el forward pass (al pasar de la capa \\(\\ell\\) a \\(\\ell+1\\)) cuando se usa la activación ReLU, Kaiming He et al. propusieron inicializar los pesos \\(W^{(\\ell)}\\) muestreando de una distribución con media 0 y varianza:\n\\[\n\\sigma_\\ell^2 = \\text{Var}(W_{ij}^{(\\ell)}) = \\frac{2}{D_\\ell}\n\\]\ndonde \\(D_\\ell\\) es el fan-in de la capa \\(\\ell\\). Si reemplazamos esto en la @eq_varianza_z obtenemos \\(\\sigma_{z^{(\\ell)}}^2 = \\sigma_{z^{(\\ell - 1)}}^2\\), tal que la varianza es estable.",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "12. Descenso de Gradiente y Auto-diferenciación"
    ]
  },
  {
    "objectID": "12_gradient_descent_and_autodiff.html#inicialización-para-el-backward-pass-sigma2-2d_ell1",
    "href": "12_gradient_descent_and_autodiff.html#inicialización-para-el-backward-pass-sigma2-2d_ell1",
    "title": "Descenso de Gradiente y Auto-diferenciación",
    "section": "Inicialización para el backward pass: \\(\\sigma^2 = 2/D_{\\ell+1}\\)",
    "text": "Inicialización para el backward pass: \\(\\sigma^2 = 2/D_{\\ell+1}\\)\nDe forma análoga, si analizamos el backward pass, para que la varianza de los gradientes \\(\\frac{\\partial \\mathcal{J}}{\\partial z^{(\\ell)}}\\) se mantenga estable al retropropagar desde la capa \\(\\ell+1\\) a la capa \\(\\ell\\) (asumiendo activación ReLU), la condición requiere una varianza basada en el ‘fan-out’:\n\\[\n\\sigma^2 = \\text{Var}(W_{ij}^{(\\ell)}) = \\frac{2}{D_{\\ell+1}}\n\\]\nNotamos que la condición ideal para el forward pass (usa \\(D_h\\)) y para el backward pass (usa \\(D_{h+1}\\)) solo coinciden si \\(D_h = D_{h+1}\\). En la práctica, se puede usar la inicialización de He cuando la activación es ReLU, o un promedio \\(\\sigma_\\ell^2 = \\frac{4}{D_h + D_{h+1}}\\).\nNota: Cuando la activación es tanh o sigmoide, la derivación de la relación entre las varianzas sugiere usar \\(\\sigma_\\ell^2 = \\frac{1}{D_h}\\), que se llama inicialización de Xavier/Glorot.",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "12. Descenso de Gradiente y Auto-diferenciación"
    ]
  },
  {
    "objectID": "8_seleccion_de_modelos.html",
    "href": "8_seleccion_de_modelos.html",
    "title": "Selección de Modelos y Regularización",
    "section": "",
    "text": "Ahora discutiremos cómo evaluar un modelo. Vimos que puede haber varias formas de ajustar los datos. Por ejemplo, podemos usar todos los predictores \\(X_i\\) o sólo algunos de ellos que consideramos más relevantes. El problema es que en general mientars más parámetros tenga un modelo, mejor será el ajuste a los datos usados para entrenarlo: Si el modelo tiene más libertad, la usa para ajustarse mejor. Sin embargo cuando tiene demasiada libertad se ajustará también a las fluctuaciones aleatorias en los datos, tal que al tratar de hacer predicciones a partir de datos no vistos cometerá errores (gran varianza). Al error cometido sobre datos no vistos se lo llama error de generalización. Un modelo que se ajusta muy bien a los datos usados para entrenarlo, pero que tiene un gran error de generalización se dice que no generaliza bien.\nPor otro lado si tiene poca libertad no logrará capturar los patrones en los datos, lo que se llama poca expresividad del modelo, y cometerá un error grande al predecir (gran sesgo).\nEn esta clase veremos cómo hacer para evaluar modelos buscando el equilibrio entre la complejidad del modelo y el error de generalización. En otras palabras el equilibrio entre sesgo y varianza.\nCuando el número de parámetros es grande (dada una cantidad de datos), podemos intenar reducir su varianza. Esto se llama regularización que será muy importante cuando estudiemos redes neuronales. Introduciremos por ahora un par de métodos de regularización.\nCódigo\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Semilla para reproducibilidad\nnp.random.seed(0)\n\n# Parámetros\nn_muestras = 100\nn_total_variables = 20\nvariables_reales = [0, 1, 2, 3]  # Solo las dos primeras variables influyen en la respuesta\n\n# Generar una matriz de covarianza para producir predictores correlacionados.\nrho = 0.1  # alto grado de correlación entre predictores\ncov = np.full((n_total_variables, n_total_variables), rho)\nnp.fill_diagonal(cov, 1.2)\n\n# Generar predictores correlacionados a partir de una distribución normal multivariada.\nX = np.random.multivariate_normal(mean=np.zeros(n_total_variables), cov=cov, size=n_muestras)\n\n# Generar variable dependiente con solo dos predictores reales\ncoeficientes_reales = np.zeros(n_total_variables)\ncoeficientes_reales[variables_reales] = [3.0, -0.8, 0.5, 1.0]\ny = X @ coeficientes_reales + np.random.randn(n_muestras) * 0.5  # Agregar ruido\n\n# Calcular el error cuadrático medio (ECM) al incluir más predictores\nlista_ecm = []\nfor n_vars in range(1, n_total_variables + 1):\n    modelo = LinearRegression()\n    modelo.fit(X[:, :n_vars], y)\n    y_pred = modelo.predict(X[:, :n_vars])\n    ecm = mean_squared_error(y, y_pred)\n    lista_ecm.append(ecm)\n\n# Graficar los resultados\nplt.figure(figsize=(6, 5))\nplt.plot(range(1, n_total_variables + 1), lista_ecm, marker='o')\nplt.yscale(\"log\")\nplt.axvline(x=len(variables_reales), color='red', linestyle='--', label='Complejidad real del modelo')\nplt.xlabel('Número de predictores incluidos')\nplt.ylabel('Error cuadrático medio (ECM)')\nplt.title('ECM vs. Número de predictores en regresión lineal')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigura 1: Error cuadrático medio para la regresión lineal cuando la señal real depende solo de dos predictores.\nEn la Figura 1 graficamos el error cuadrático medio para una regresión lineal \\(\\sum_{i=1}^p \\beta_i X_i\\) a partir de datos ficticios. Los datos reales fueron producidos por sólo tres predictores. Sin embargo, vemos que al incluir más y más variables el error cuadrático medio sigue decreciendo.",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "8. Selección de Modelos"
    ]
  },
  {
    "objectID": "8_seleccion_de_modelos.html#criterios-de-información-para-penalizar-modelos-complejos",
    "href": "8_seleccion_de_modelos.html#criterios-de-información-para-penalizar-modelos-complejos",
    "title": "Selección de Modelos y Regularización",
    "section": "Criterios de información para penalizar modelos complejos",
    "text": "Criterios de información para penalizar modelos complejos\nExisten varios criterios apra evaluar modelos basados en la teoría de probabilidad.\n\nCriterio de información de Akaike\nSupongamos que los datos subyacentes tienen una distribución de probabilidad \\(f(y)\\) y nosotros la aproximamos con una verosimilitud \\(L(y|\\theta)\\). Una manera de cuantificar la diferencia entre las dos distribuciones es medir la entropía. Esto se hace mediante la divergencia de Kullback Liebler \\[\n\\begin{multline}\nD_{KL}(f || p) = \\int dy\\,f(y)\\log\\left(\\frac{f(y)}{L(y|\\theta)}\\right) \\\\ = \\int dy\\,f(y)\\log f(y) - \\int dy\\,f(y)\\log L(y|\\theta) = -\\langle L(y|\\theta)\\rangle + const.\\,.\n\\end{multline}\n\\] Se puede demostrar que \\(D_{KL}(f || p) &gt; 0\\) y es cero sólo cuando \\(f = p\\). Además tiene la interpretación de ser igual a la entropía de \\(f\\) más el costo asociado con usar \\(p(y)\\) al analizar los datos.\nAhora bien, supongamos que estimamos los parámetros maximizando la verosimilitud. Sea \\(\\log\\hat{L}\\) el máximo valor de la verosimilitud. Akaike demostró que este es un estimador sesgado de \\(\\langle \\ln L\\rangle\\). Tal que un estimador no sesgado de la divergencia de Kullback Liebler (a parte constantes) es \\[\nAIC = 2p -2\\log\\hat{L}\\,.\n\\] Un menor \\(AIC\\) representa un mejor ajuste en el sentido que la distribución de probabilidad es más cercana a la subyacente.\nEl valor de \\(AIC\\) no es importante, lo importante es la diferencia de valores entre modelos. En general una diferencia de \\(AIC\\) de \\(2\\) o menos indica que los modelos son equivalentes. Una diferencia entre 4 y 7 es una evidencia moderada de que el modelo con el \\(AIC\\) más alto no describe los datos. Una diferencia mayor a \\(10\\) suele indicar una evidencia más fuerte. En general \\(\\exp((AIC_{min} - AIC)/2)\\) es proporcional a la probabilidad de que el modelo minimice la pérdida de información.\n\n\nCódigo\nimport numpy as np\n\n# Lista para guardar los valores de AIC\nlista_aic = []\n\n# Número de muestras\nn = n_muestras\n\n# Calcular AIC para cada modelo con distinto número de predictores\nfor n_vars in range(1, n_total_variables + 1):\n    X_sub = X[:, :n_vars]\n    X_con_bias = np.hstack([np.ones((n_muestras, 1)), X_sub])  # Agregar término independiente\n\n    # Ajuste por mínimos cuadrados\n    beta = np.linalg.pinv(X_con_bias.T @ X_con_bias) @ X_con_bias.T @ y\n    y_pred = X_con_bias @ beta\n\n    # Log-verosimilitud bajo supuestos normales con varianza constante\n    residuo = y - y_pred\n    sigma2 = np.mean(residuo**2)\n    log_verosimilitud = -n / 2 * (np.log(2 * np.pi * sigma2) + 1)\n\n    # Número de parámetros (incluye término independiente)\n    k = n_vars + 1\n\n    # AIC\n    aic = 2 * k - 2 * log_verosimilitud\n    lista_aic.append(aic)\n\nimport matplotlib.pyplot as plt\n\n# Graficar AIC\nplt.figure(figsize=(6, 5))\nplt.plot(range(1, n_total_variables + 1), lista_aic, marker='o')\nplt.axvline(x=4, color='red', linestyle='--', label='Complejidad real del modelo')\nplt.xlabel('Número de predictores incluidos')\nplt.ylabel('Criterio de Información de Akaike (AIC)')\nplt.title('AIC vs. Número de predictores en regresión lineal')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigura 2: AIC para datos sintéticos generados a partir de tres predictores.\n\n\n\n\n\nSi aplicamos este criterio a nuestro modelo de juquete, vemos que la cantidad \\(AIC\\) tiene un mínimo justo en \\(3\\) predictores.\n\n\nCriterio de información de Bayes\nEn la visión bayesiana podemos asignarle una probabilidad a cada modelo dados los datos. En esa óptica podemos escoger el modelo más probable. Aplicando el teorema de Bayes tenemos \\[\n\\frac{P(M_1|D)}{P(M_2|D)} = \\frac{P(D|M_1)}{P(D|M_2)}\\frac{P(M_1)}{P(M_2)} \\equiv \\frac{E_1}{E_2} \\frac{P(M_1)}{P(M_2)}\\,.\n\\] La probabilidad previa \\(P(M)\\) está dada por información anterior al momento en que realizamos el experimento. En muchos casos no tenemos una preferencia por un modelo específico tal que \\(P(M_2)/P(M_1) = 1\\). La evidencia bayesiana \\(E \\equiv P(D|M)\\) es la probabilidad de los datos dado el modelo integrada sobre los parámetros del modelo \\[\nP(D|M) = \\int d^p\\beta\\,P(D|M, \\boldsymbol{\\beta}) = \\int d^p\\beta\\,L(\\boldsymbol{\\beta})\\,.\n\\] Existen códigos que pueden calcular la integral de la verosimilitud sobre todos los posibles valores de los parámetros.\nEsto suele ser algo costoso computacionalmente, tal que en muchos casos (sobre todo por fuera de la física) se usa una expresión aproximada válida para un número grande de mediciones i.i.d., escogiendo el modelo que minimiza el criterio de información de Bayes \\[\nBIC = -2\\ln\\hat{L} + p\\ln n\\,.\n\\] Comparado con el AIC, el BIC penaliza más fuertemente los modelos con más parámetros.\n\n\n\n\n\n\nDemostración\n\n\n\n\n\nPrimero asumimos que las mediciones son i.i.d., tal que \\(L = \\prod_i p(x_i)\\) y entonces \\(\\ln L = \\sum_i \\ln p(x_i)\\). Para \\(n\\) grande podemos aproximar \\(\\sum_i \\ln p(x_i) \\approx n \\langle\\ln p\\rangle \\equiv n f(\\boldsymbol{\\beta})\\).\nUsando eso, aproximamos la integral cerca del máximo. Esta aproximación es buena para \\(n\\) grande \\[\n\\begin{multline}\nP(D|M) \\approx \\int d^p\\beta\\,e^{\\ln L(\\boldsymbol{\\beta})} \\approx \\int d^p\\beta\\,e^{n\\ln f(\\boldsymbol{\\beta})} \\\\ \\approx \\int d^p\\beta\\,e^{n\\ln f(\\hat{\\boldsymbol{\\beta}}) + n\\frac{\\partial \\ln f(\\hat{\\boldsymbol{\\beta}})}{\\partial \\beta^i \\partial \\beta^j} (\\boldsymbol{\\beta}^i - \\hat{\\boldsymbol{\\beta}}^i)(\\boldsymbol{\\beta}^j - \\hat{\\boldsymbol{\\beta}}^j)} = e^{n\\ln f(\\hat{\\boldsymbol{\\beta}})}\\left(\\frac{|\\partial^2 \\ln f /\\partial\\beta^2|^{-1/2}}{n^{p/2}(2\\pi)^{p/2}}\\right)\\,.\n\\end{multline}\n\\] Tomando el logaritmo obtenemos \\[\n\\ln P(D|M) \\approx \\ln\\hat{L} - \\frac{k}{2}\\ln n + const.\n\\] de donde se obtiene el BIC.\n\n\n\n\n\nCódigo\nimport numpy as np\n\n# Lista para guardar los valores de AIC\nlista_aic = []\n\n# Número de muestras\nn = n_muestras\n\n# Calcular AIC para cada modelo con distinto número de predictores\nfor n_vars in range(1, n_total_variables + 1):\n    X_sub = X[:, :n_vars]\n    X_con_bias = np.hstack([np.ones((n_muestras, 1)), X_sub])  # Agregar término independiente\n\n    # Ajuste por mínimos cuadrados\n    beta = np.linalg.pinv(X_con_bias.T @ X_con_bias) @ X_con_bias.T @ y\n    y_pred = X_con_bias @ beta\n\n    # Log-verosimilitud bajo supuestos normales con varianza constante\n    residuo = y - y_pred\n    sigma2 = np.mean(residuo**2)\n    log_verosimilitud = -n / 2 * (np.log(2 * np.pi * sigma2) + 1)\n\n    # Número de parámetros (incluye término independiente)\n    k = n_vars + 1\n\n    # BIC\n    bic = k*np.log(n) - 2 * log_verosimilitud\n    lista_aic.append(bic)\n\nimport matplotlib.pyplot as plt\n\n# Graficar AIC\nplt.figure(figsize=(6, 5))\nplt.plot(range(1, n_total_variables + 1), lista_aic, marker='o')\nplt.axvline(x=4, color='red', linestyle='--', label='Complejidad real del modelo')\nplt.xlabel('Número de predictores incluidos')\nplt.ylabel('Criterio de Información de Bayes (BIC)')\nplt.title('BIC vs. Número de predictores en regresión lineal')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigura 3: BIC para datos sintéticos generados a partir de tres predictores.",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "8. Selección de Modelos"
    ]
  },
  {
    "objectID": "8_seleccion_de_modelos.html#evaluación-de-modelos-usando-subconjuntos-aleatorios",
    "href": "8_seleccion_de_modelos.html#evaluación-de-modelos-usando-subconjuntos-aleatorios",
    "title": "Selección de Modelos y Regularización",
    "section": "Evaluación de modelos usando subconjuntos aleatorios",
    "text": "Evaluación de modelos usando subconjuntos aleatorios\nEn algunos casos no es fácil escribir una verosimilitud y entonces no podemos usar los criterios de la sección anterior. Esto ocurre con mucha frecuencia cuando se trabaja con modelos de redes neuronales.\nCuando es así, la práctica común es estimar la precisión del modelo sobre datos no vistos usando un subconjunto de los datos mismos.\n\nEl conjunto de validación\nUna manera de lograrlo es separar una fracción de los datos para que sea un conjunto de validación. Los datos que no son parte de este forman el conjunto de entrenamiento. El modelo se ajusta o entrena con el conjunto de entrenamiento y el de validación se usa solo para evaluar su desempeño. Como el modelo no vio el conjunto de validación, el error cometido sobre él será una aproximación al error que cometerá cuando vea datos nuevos. Como sabemos los \\(Y\\) del conjunto de validación, podemos calcular dicho error.\nEs decir, procedemos en tres pasos:\n\nSeparar los datos en un conjunto de entrenamiento y un conjunto de validación. Con frecuencia se usa el 10% o 20% de los datos para el conjunto de validación.\nEntrenar el modelo en el conjunto de entrenamiento.\nEvaluar el modelo en el conjunto de validación.\n\nEn general, la estima del error de generalización obtenida de esta manera tendrá un sesgo porque usamos sólo un subconjunto de datos para entrenar el modelo. Un modelo entrenado en más datos funcionará mejor con un error menor generalización. En este sentido es una estima pesimista. Además tendrá una varianza ya que el subconjunto de validación es aleatorio.\n\n\nCódigo\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\ndef val_error(X, y, random_state):\n  # Separar en conjunto de entrenamiento (80%) y validación (20%)\n  X_entrenamiento, X_validacion, y_entrenamiento, y_validacion = train_test_split(X, y, test_size=0.2, random_state=random_state)\n\n  # Lista para almacenar errores de validación\n  lista_ecm_validacion = []\n\n  # Probar diferentes cantidades de predictores\n  for n_vars in range(1, n_total_variables + 1):\n      # Seleccionar subconjunto de variables\n      X_ent_sub = X_entrenamiento[:, :n_vars]\n      X_val_sub = X_validacion[:, :n_vars]\n      \n      # Agregar término independiente (bias)\n      X_ent_bias = np.hstack([np.ones((X_ent_sub.shape[0], 1)), X_ent_sub])\n      X_val_bias = np.hstack([np.ones((X_val_sub.shape[0], 1)), X_val_sub])\n\n      # Ajustar modelo por mínimos cuadrados\n      beta = np.linalg.pinv(X_ent_bias.T @ X_ent_bias) @ X_ent_bias.T @ y_entrenamiento\n      \n      # Predecir en conjunto de validación\n      y_val_predicho = X_val_bias @ beta\n      \n      # Calcular error cuadrático medio en validación\n      ecm_val = np.mean((y_validacion - y_val_predicho) ** 2)\n      lista_ecm_validacion.append(ecm_val)\n\n  return lista_ecm_validacion\n\nlista_ecm_validacion_1 = val_error(X, y, 42)\nlista_ecm_validacion_2 = val_error(X, y, 10)\n\n# Graficar error de validación frente al número de predictores\nplt.figure(figsize=(6, 5))\nplt.plot(range(1, n_total_variables + 1), lista_ecm_validacion_1, marker='o')\nplt.plot(range(1, n_total_variables + 1), lista_ecm_validacion_2, marker='o')\nplt.axvline(x=4, color='red', linestyle='--', label='Complejidad real del modelo')\nplt.xlabel('Número de predictores incluidos')\nplt.ylabel('Error cuadrático medio (ECM) en validación')\nplt.yscale(\"log\")\nplt.title('ECM de validación vs. número de predictores')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigura 4: Error con varios conjuntos de validación.\n\n\n\n\n\nEn la Figura 4 graficamos el error de validación estimado con usando dos particiones distintas de los datos entre validación y entrenamiento. Vemos que la estimación del error de generalización tiene varianza (varía dependiendo de cuál es el conjunto de validación).\n\n\nValidación cruzada\nSi queremos reducir el sesgo y la varianza, podemos tomar el promedio de muchas validaciones diferentes. La manera de hacerlo es:\n\nDividir el conjunto de datos en \\(k\\) partes.\nUsar una parte como conjunto de validación.\nUsar \\(k - 1\\) partes para entrenar el modelo. Validar con la parte restante.\nRepetir \\(k\\) veces hasta usar todas las partes como validación.\n\nEsto nos permite promediar sobre los \\(k\\) valores obtenidos para el error, esperando tener una varianza menor. A mayor \\(k\\), mayor será el conjunto de datos usado para entrenar y por lo tanto más cercano al modelo completo. Pero al aumentar \\(k\\) los valores obtenidos para la estima del error estarán más correlacionados ya que los diferentes conjuntos usados para entrenar se parecerán más, lo que aumenta la varianza. En la práctica se usa \\(k\\) entre \\(3\\) y \\(10\\).\n\n\nCódigo\n# Reimportar bibliotecas necesarias tras el reinicio\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\n\n# Lista de valores de K para validación cruzada\nvalores_k = [3, 5, 10]\n\n# Diccionario para almacenar los resultados\nresultados_cv = {}\n\n# Realizar validación cruzada para cada valor de K\nfor k in valores_k:\n    kf = KFold(n_splits=k, shuffle=True, random_state=0)\n    ecm_promedios = []\n\n    for n_vars in range(1, n_total_variables + 1):\n        errores_fold = []\n\n        for train_index, val_index in kf.split(X):\n            X_entrenamiento, X_validacion = X[train_index, :n_vars], X[val_index, :n_vars]\n            y_entrenamiento, y_validacion = y[train_index], y[val_index]\n\n            # Agregar término independiente (bias)\n            X_ent_bias = np.hstack([np.ones((X_entrenamiento.shape[0], 1)), X_entrenamiento])\n            X_val_bias = np.hstack([np.ones((X_validacion.shape[0], 1)), X_validacion])\n\n            # Ajustar modelo\n            beta = np.linalg.pinv(X_ent_bias.T @ X_ent_bias) @ X_ent_bias.T @ y_entrenamiento\n            y_val_pred = X_val_bias @ beta\n\n            # Calcular error cuadrático medio\n            ecm = np.mean((y_validacion - y_val_pred) ** 2)\n            errores_fold.append(ecm)\n\n        ecm_promedios.append(np.mean(errores_fold))\n    \n    resultados_cv[k] = ecm_promedios\n\n# Graficar los resultados\nplt.figure(figsize=(6, 5))\nfor k in valores_k:\n    plt.plot(range(1, n_total_variables + 1), resultados_cv[k], marker='o', label=f'{k} pliegues')\n\nplt.axvline(x=4, color='red', linestyle='--', label='Complejidad real del modelo')\nplt.xlabel('Número de predictores incluidos')\nplt.ylabel('ECM promedio de validación cruzada')\nplt.title('ECM de validación cruzada vs. número de predictores')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigura 5: Error con validación cruzada para varios valores de \\(k\\).\n\n\n\n\n\nEn la práctica se puede usar validación cruzada cuando el modelo es pequeño y barato de entrenar (tarda algunos minutos). Pero si el modelo es costoso de entrenar usualmente se usa un solo conjunto de validación.",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "8. Selección de Modelos"
    ]
  },
  {
    "objectID": "8_seleccion_de_modelos.html#búsqueda-hacia-adelante-y-hacia-atrás",
    "href": "8_seleccion_de_modelos.html#búsqueda-hacia-adelante-y-hacia-atrás",
    "title": "Selección de Modelos y Regularización",
    "section": "Búsqueda hacia adelante y hacia atrás",
    "text": "Búsqueda hacia adelante y hacia atrás\nLa búsqueda hacia adelante consiste en lo siguiente:\n\nA partir de un modelo con \\(\\ell\\) variables, llamado \\(\\mathcal{M}_\\ell\\), le agregamos una de las \\(q - \\ell\\) variables faltantes.\nRepetimos para cada una de las variables faltantes y escogemos el mejor de estos \\(q - \\ell\\) modelos en el sentido que sea el que obtiene el \\(R^2\\) o más alto. Lo llamamos \\(\\mathcal{M}_{\\ell + 1}\\).\nRepetimos 1 y 2 a partir de \\(\\mathcal{M}_{\\ell + 1}\\) para obtener \\(\\mathcal{M}_{\\ell + 2}\\).\nPara comparar los modelos \\(\\mathcal{M}_1, ..., \\mathcal{M}_q\\) usamos validación cruzada, AIC o BIC.\n\nLa búsqueda hacia atrás es similar pero empezamos por un modelo con todas las variables.\n\nA partir de un modelo con \\(\\ell\\) variables, llamado \\(\\mathcal{M}_\\ell\\), quitamos una de esas variables.\nRepetimos para cada una de las \\(\\ell\\) variables y escogemos el mejor de estos \\(\\ell\\) modelos en el sentido que sea el que obtiene el \\(R^2\\) o más alto. Lo llamamos \\(\\mathcal{M}_{\\ell - 1}\\).\nRepetimos 1 y 2 a partir de \\(\\mathcal{M}_{\\ell - 1}\\) para obtener \\(\\mathcal{M}_{\\ell - 2}\\).\nPara comparar los modelos \\(\\mathcal{M}_1, ..., \\mathcal{M}_q\\) usamos validación cruzada, AIC o BIC.",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "8. Selección de Modelos"
    ]
  },
  {
    "objectID": "8_seleccion_de_modelos.html#búsqueda-aleatoria",
    "href": "8_seleccion_de_modelos.html#búsqueda-aleatoria",
    "title": "Selección de Modelos y Regularización",
    "section": "Búsqueda aleatoria",
    "text": "Búsqueda aleatoria\nCuando el número de modelos es grande, y cuando no hay claridad sobre los hiperparámetros a usar, uno puede explorar modelos de forma aleatoria. La idea es que habrán muchos modelos parecidos entre ellos, tal que la búsqueda aleatoria puede encontrar varias zonas en el espacio de hiperparámetros y escoger la mejor.\nEstos métodos de búsqueda son aún muy costosos cuando el número de hiperparámetros es grande. Por eso se han desarrollado métodos más sofisticados como uno inspirado en métodos bayesianos para explorar el espacio de hiperparámetros de forma más eficiente.",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "8. Selección de Modelos"
    ]
  },
  {
    "objectID": "8_seleccion_de_modelos.html#conjunto-de-prueba",
    "href": "8_seleccion_de_modelos.html#conjunto-de-prueba",
    "title": "Selección de Modelos y Regularización",
    "section": "Conjunto de prueba",
    "text": "Conjunto de prueba\nCuando se busca un modelo entre muchos, el modelo se puede ajustar al conjunto de validación. Es como si uno hiciera un entrenamiento adicional a mano. Por lo tanto, el error de validación o validación cruzada puede ser mucho menor que el error de generalización.\nPara atacar este problema lo usual en aprendizaje automático es separar una fracción de los datos antes de empezar cualquier análisis, escogidos aleatoriamente para testear el modelo al final del análisis. Es decir, uno separa el \\(10-20\\%\\) de los datos en un conjunto de prueba y se usa sólo al final para evaluar el mejor modelo obtenido. De esta manera tendremos una idea de cómo se comportará ese modelo con datos no vistos nunca durante el entrenamiento ni la selección de modelos.",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "8. Selección de Modelos"
    ]
  },
  {
    "objectID": "8_seleccion_de_modelos.html#lazo-o-regularización-l1",
    "href": "8_seleccion_de_modelos.html#lazo-o-regularización-l1",
    "title": "Selección de Modelos y Regularización",
    "section": "Lazo o regularización L1",
    "text": "Lazo o regularización L1\nUna alternativa es usar la suma de valores absolutos \\[\n\\mathcal{L} = \\sum_{i=1}^n\\left[y_i - \\beta_0 - \\sum_{j=1}^q\\beta_j x_j\\right]^2 + \\alpha\\sum_{j=0}^q|\\beta_j|\\,.\n\\]\n\n\nCódigo\nfrom sklearn.linear_model import Lasso\n\n# Escalar los datos para que la regularización sea comparable entre variables\nescalador = StandardScaler()\nX_escalado = escalador.fit_transform(X)\n\n# Crear validación cruzada con 5 pliegues\nkf = KFold(n_splits=5, shuffle=True, random_state=0)\n\n# Lista para guardar errores promedio de validación cruzada\nlista_ecm_cv = []\n\n# Valores de alpha \nvalores_alpha_lineal = np.linspace(0, 0.5, 40)\n\n# Calcular ECM de validación cruzada para cada valor de alpha\nfor alpha in valores_alpha_lineal:\n    errores_fold = []\n\n    for train_index, val_index in kf.split(X_escalado):\n        X_ent, X_val = X_escalado[train_index], X_escalado[val_index]\n        y_ent, y_val = y[train_index], y[val_index]\n\n        modelo_ridge = Lasso(alpha=alpha, fit_intercept=True)\n        modelo_ridge.fit(X_ent, y_ent)\n        y_val_pred = modelo_ridge.predict(X_val)\n        ecm_val = mean_squared_error(y_val, y_val_pred)\n        errores_fold.append(ecm_val)\n\n    lista_ecm_cv.append(np.mean(errores_fold))\n\n# Graficar ECM de validación cruzada frente a alpha\nplt.figure(figsize=(6, 5))\nplt.plot(valores_alpha_lineal, lista_ecm_cv, marker='o')\nplt.xlabel('Valor de α (regularización Lazo)')\nplt.ylabel('ECM promedio de validación cruzada')\nplt.title('ECM de validación cruzada (5 pliegues) vs. α en regresión Lazo')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigura 8: Error de validación cruzada con \\(k = 5\\) pliegues para la regresión lineal con regularización L1.\n\n\n\n\n\n\n\nCódigo\n# Nuevos valores de alpha en escala logarítmica, extendidos hasta valores grandes\nvalores_alpha_log = np.logspace(-4, 1, 100)  # de 1e-4 a 1e3\n\n# Guardar coeficientes para cada valor de alpha\ncoeficientes_ridge_log = []\n\nfor alpha in valores_alpha_log:\n    modelo = Lasso(alpha=alpha)\n    modelo.fit(X_escalado, y)\n    coeficientes_ridge_log.append(modelo.coef_)\n\ncoeficientes_ridge_log = np.array(coeficientes_ridge_log)\n\n# Graficar los coeficientes en función de alpha (escala log en x)\nplt.figure(figsize=(6, 5))\nfor i in range(n_total_variables):\n    if i in variables_reales:\n        plt.plot(valores_alpha_log, coeficientes_ridge_log[:, i], linewidth=2.5, label=f'X{i}', alpha=0.9)\n    else:\n        plt.plot(valores_alpha_log, coeficientes_ridge_log[:, i], color='lightgray', linewidth=1)\n\nplt.xscale('log')\nplt.xlabel('Valor de α (regularización Lazo, escala logarítmica)')\nplt.ylabel('Coeficientes del modelo')\nplt.title('Evolución de los coeficientes de regresión Lazo según α (escala logarítmica)')\nplt.legend(title='Variables relevantes')\nplt.grid(True, which='both', linestyle='--', linewidth=0.5)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigura 9: Comportamiento de los coeficientes para la regresión lineal con regularización L1.\n\n\n\n\n\nEn la Figura 9 vemos que esta regularización hace tender los coeficientes poco importantes exactamente a cero. Por este motivo funciona mejor cuando la variable que se quiere predecir depende pocos regresores (como en este ejemplo).\nPara entender por qué pasa esto podemos ver la Figura 10 (sacada del libro). Las elipses representan diferentes niveles de la suma de errores al cuadrado. Siendo una ecuación cuadrática en los coeficientes nos podemos convencer que sus curvas de nivel deben ser cónicas que resultan ser elipses. La región llena es una curva de nivel de el término adicional agregado a la función de pérdida: Un círculo para L2 y un rombo para L1. La primera curva de nivel que intersecta el rombo tiende a hacerlo en uno de los ejes.\n\n\n\n\n\n\nFigura 10: Regresión L1 vs L2, sacada del libro “Introduction to Statistical Learning”.",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "8. Selección de Modelos"
    ]
  },
  {
    "objectID": "16_transformadores.html",
    "href": "16_transformadores.html",
    "title": "Transformadores y el mecanismo de atención",
    "section": "",
    "text": "Los transformadores son una arquitectura de red neuronal que ha demostrado ser muy efectiva en el procesamiento del lenguaje natural y en la clasificación de imágenes. Están detrás de los chatbots que han catapultado este campo a la primera página de los periódicos.",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "16. Transformadores y el mecanismo de atención"
    ]
  },
  {
    "objectID": "16_transformadores.html#calculando-los-pesos-de-atención",
    "href": "16_transformadores.html#calculando-los-pesos-de-atención",
    "title": "Transformadores y el mecanismo de atención",
    "section": "Calculando los pesos de atención",
    "text": "Calculando los pesos de atención\nPara calcular estos pesos de atención, primero definimos dos nuevas combinaciones de variables llamadas “queries” \\(\\boldsymbol{q}\\) y “keys” \\(\\boldsymbol{k}\\) (en español se podrían llamar “consultas” y “claves”, y tienen estos nombres por razones históricas) dadas por\n\\[\n\\boldsymbol{q}_n = \\boldsymbol{W}_q\\boldsymbol{x}_n + \\boldsymbol{\\beta}_q\\,,\n\\]\ny\n\\[\n\\boldsymbol{k}_m = \\boldsymbol{W}_k\\boldsymbol{x}_m + \\boldsymbol{\\beta}_k\\,,\n\\]\nrespectivamente. En principio, los vectores \\(\\boldsymbol{q}\\) y \\(\\boldsymbol{k}\\) tienen dimensión \\(D_q\\) que puede ser diferente de la dimensión \\(D\\) de los vectores \\(\\boldsymbol{x}\\). Entonces estas combinaciones introducen otros \\(2D_q (D + 1)\\) parámetros.\nLuego, en la llada “atención de producto punto” calculamos el producto punto entre \\(\\boldsymbol{q}_m\\) y \\(\\boldsymbol{k}_n\\) y lo ponemos en una función softmax\n\\[\na(\\boldsymbol{x}_m, \\boldsymbol{x}_n) = \\text{softmax}\\left(\\boldsymbol{k}_m^T\\boldsymbol{q}_n\\right) = \\frac{\\exp\\left(\\boldsymbol{k}_m^T\\boldsymbol{q}_n\\right)}{\\sum_{m=1}^N \\exp\\left(\\boldsymbol{k}_m^T\\boldsymbol{q}_n\\right)}\\,.\n\\]\nRecordemos que la función softmax es una función que regresa un valor entre \\(0\\) y \\(1\\) que suma \\(1\\) cuando se suman todos los valores. Entonces la atención nos da el peso de cada posición \\(m\\) para la posición \\(n\\), siendo \\(1\\) cuando la posición \\(m\\) es la más relevante para la posición \\(n\\) y \\(0\\) cuando no es relevante.\nNote que los coeficientes de atención \\(a(\\boldsymbol{x}_m, \\boldsymbol{x}_n)\\) forman una matriz \\(N \\times N\\).\nEl cálculo de los pesos de atención se ilustra en la figura 12.3 del libro.\nUsando notación matricial, podemos escribir los datos \\(N\\) datos \\(\\boldsymbol{x}\\) como una matriz \\(D \\times N\\), donde cada columna es un dato, tal que los valores, llaves y consultas son \\[\n\\boldsymbol{V} = \\boldsymbol{W}_v\\boldsymbol{X} + \\boldsymbol{\\beta}_v\\boldsymbol{1}_{N}^T\\,,\n\\] \\[\n\\boldsymbol{K} = \\boldsymbol{W}_k\\boldsymbol{X} + \\boldsymbol{\\beta}_k\\boldsymbol{1}_{N}^T\\,,\n\\] y \\[\n\\boldsymbol{Q} = \\boldsymbol{W}_q\\boldsymbol{X} + \\boldsymbol{\\beta}_q\\boldsymbol{1}_{N}^T\\,,\n\\]\ndonde \\(\\boldsymbol{1}_{N}\\) es un vector de \\(N\\) unos y los productos son productos matriciales. Ahora los valores son matrices \\(D \\times N\\), y las llaves y consultas son vectores \\(D_q \\times N\\). La atención se puede representar como\n\\[\n\\boldsymbol{a} = \\text{softmax}_\\text{rows}\\left(\\boldsymbol{K}^T\\boldsymbol{Q}\\right)^T\\boldsymbol{V}\\,.\n\\]\nAquí la función softmax se aplica por filas, es decir, para cada fila \\(n\\) de la matriz \\(\\boldsymbol{K}^T\\boldsymbol{Q}\\), se aplica la función softmax para obtener la fila \\(n\\) de la matriz \\(\\text{softmax}_\\text{rows}(\\boldsymbol{K}^T\\boldsymbol{Q})\\).\nEsta versión matricial se ilustra en la figura 12.4 del libro.",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "16. Transformadores y el mecanismo de atención"
    ]
  },
  {
    "objectID": "16_transformadores.html#escalado-de-la-atención",
    "href": "16_transformadores.html#escalado-de-la-atención",
    "title": "Transformadores y el mecanismo de atención",
    "section": "Escalado de la atención",
    "text": "Escalado de la atención\nEn la práctica, el producto punto entre las llaves y las consultas puede volverse muy grande, lo que puede hacer que la softmax tenga problemas numéricos. Por lo tanto, se escala el producto punto entre las llaves y las consultas por la raíz cuadrada de la dimensión de las llaves. Es decir,\n\\[\na(\\boldsymbol{x}_m, \\boldsymbol{x}_n) = \\text{softmax}\\left(\\frac{\\boldsymbol{k}_m^T\\boldsymbol{q}_n}{\\sqrt{D_q}}\\right)\\,.\n\\]",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "16. Transformadores y el mecanismo de atención"
    ]
  },
  {
    "objectID": "16_transformadores.html#codificación-posicional-positional-encoding",
    "href": "16_transformadores.html#codificación-posicional-positional-encoding",
    "title": "Transformadores y el mecanismo de atención",
    "section": "Codificación posicional (positional encoding)",
    "text": "Codificación posicional (positional encoding)\nComo se ha implementado arriba, la atención es equivariante bajo intercambio de las posiciones en la frase. Sin embargo, sabemos que las posiciones de las palabras pueden cambiar el significado de la frase. Por ejemplo la frase “el perro mordió al niño” es diferente a la frase “el niño mordió al perro”. Por lo tanto, para darle a la red información sobre la posición de las palabras, se le da una codificación posicional adicional. En su versión más sencilla esta codificación es una matriz \\(D \\times N\\) que se suma a la matriz \\(\\boldsymbol{X}\\).\nCuando se tienen pocos datos, esta matriz se puede fijar tal que cada columna sea una función distinta de la posición. Se usan con frecuencia matrices explotando las propiedades de las funciones trigonométricas para que la codificación posicional sea periódica, tal que se tiene información sobre la distancia relativa entre dos posiciones.\nPara los grandes modelos de lenguaje, la codificación posicional es un conjunto de parámetros adicionales que el modelo aprende.\nUna matriz posicional se muestra en la figura 12.5 del libro.",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "16. Transformadores y el mecanismo de atención"
    ]
  },
  {
    "objectID": "16_transformadores.html#atención-con-múltiples-cabezas",
    "href": "16_transformadores.html#atención-con-múltiples-cabezas",
    "title": "Transformadores y el mecanismo de atención",
    "section": "Atención con múltiples cabezas",
    "text": "Atención con múltiples cabezas\nUna manera de hacer más robusta la atención es usar múltiples cabezas de atención. Es decir, en lugar de usar una sola matriz de valores, llaves y consultas, se usan \\(h\\) matrices de valores, llaves y consultas para producir \\(h\\) resultados de atención. Esto hace que el modelo sea más robusto en el sentido que cada cabeza puede codificar diferentes aspectos de la información, lo que puede ser útil para diferentes tareas.\nEsto hace el modelo más robusto a la variación de los datos y puede mejorar el rendimiento en tareas donde la información es dispersa.\nEn la práctica, cuando se usan \\(h\\) cabezas, se escoge la dimensión \\(D_q\\) tal que \\(D_q = \\frac{D}{h}\\). Esto además reduce la complejidad del modelo, ya que al calcular el producto punto entre las llaves y las consultas \\(\\boldsymbol{k}_m^T\\boldsymbol{q}_n\\), se reduce el número de parámetros y multiplicaciones.\nUn ejemplo de atención con múltiples cabezas se muestra en la figura 12.6 del libro.",
    "crumbs": [
      "Home",
      "Redes Neuronales",
      "16. Transformadores y el mecanismo de atención"
    ]
  },
  {
    "objectID": "7_clasificacion.html",
    "href": "7_clasificacion.html",
    "title": "Clasificación",
    "section": "",
    "text": "La clase pasada estudiamos problemas de regresión, en los cuales tratamos de predecir una variable numérica \\(Y\\) a partir de un conjunto de predictores \\(X\\). Ahora nos enfocamos en el caso en el cual \\(Y\\) es una serie de categorías. En física esto puede surgir cuando queremos clasificar imágenes. Por ejemplo determinar el tipo de una galaxia en una imágen. Otro ejemplo es clasificar eventos en un detector de partículas en la categoría de señal o ruido.",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "7. Clasificación"
    ]
  },
  {
    "objectID": "7_clasificacion.html#discriminante-lineal",
    "href": "7_clasificacion.html#discriminante-lineal",
    "title": "Clasificación",
    "section": "Discriminante lineal",
    "text": "Discriminante lineal\nEn este caso aproximamos \\(f_j(X)\\) con una gaussiana donde la covarianza es la misma para todo \\(j\\) \\[\nf_k(x) = \\frac{1}{\\sqrt{2\\pi}|\\sigma|^{1/2}}\\exp\\left(-\\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{\\mu}_k)^T \\Sigma^{-1}(\\boldsymbol{x} - \\boldsymbol{\\mu}_k)\\right)\\,.\n\\] La media \\(\\mu_j\\) es la media de los \\(x\\) de cada categoría y la covarianza \\(\\Sigma\\) es la covarianza entre las variables \\(X\\) estimada a partir de los datos de todas las categorías.\nAsignamos la categoría más probable a cada dato a partir de su probabilidad. Como siempre, es más fácil trabajar con el logaritmo \\[\n\\ln p_k(x) = \\ln{\\pi_k} + \\boldsymbol{x}^T\\Sigma^{-1}\\boldsymbol{\\mu}_k - \\frac{1}{2}\\boldsymbol{\\mu}_k^T\\Sigma^{-1}\\boldsymbol{\\mu}_k + ...\n\\] donde los puntos contienen un montón de términos que son iguales para todas las categorías. Entonces basta calcular los términos que sí escribimos y escoger la categoría que los maximiza. Como esta combinación es lineal, se llama discriminante lineal.",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "7. Clasificación"
    ]
  },
  {
    "objectID": "7_clasificacion.html#discriminante-cuadrático",
    "href": "7_clasificacion.html#discriminante-cuadrático",
    "title": "Clasificación",
    "section": "Discriminante cuadrático",
    "text": "Discriminante cuadrático\nSi no asumimos que todas las categorías tienen la misma covarianza, tenemos que \\[\n\\ln p_k(x) = \\ln{\\pi_k} + \\frac{1}{2}\\boldsymbol{x}^T\\Sigma_k^{-1}\\boldsymbol{x} + \\boldsymbol{x}^T\\Sigma_k^{-1}\\boldsymbol{\\mu}_k - \\frac{1}{2}\\boldsymbol{\\mu}_k^T\\Sigma_k^{-1}\\boldsymbol{\\mu}_k + ...\n\\] Ahora la ecuación es cuadrática. Esto es más general que el caso anterior, pero requiere estimar el término cuadrático. Este término contiene la matriz \\(\\Sigma^{-1}\\) que contiene \\(q(q - 1)/2\\) elementos por cada clase. Esto quiere decir que necesitamos más datos en general para que funcione bien.",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "7. Clasificación"
    ]
  },
  {
    "objectID": "7_clasificacion.html#bayes-ingenuo",
    "href": "7_clasificacion.html#bayes-ingenuo",
    "title": "Clasificación",
    "section": "Bayes ingenuo",
    "text": "Bayes ingenuo\nEl último método que estudiaremos basado en el análisis discriminante es el método ingenuo de Bayes (naive Bayes). Consiste en asumir que todas las clases tienen una varianza diferente, pero que todas las variables \\(X_i\\) son independientes. Esto lo hace simultáneamente más general y más restringido que el discriminante lineal. Como las variables son independientes, escribimos \\[\nf_k(\\boldsymbol{x}) = f_{1k}(x_1)...f_{qk}(x_q)\\,.\n\\] Si \\(X_j\\) es cuantitativa podemos usar una gaussiana para \\(f_{jk}\\). Si es cualitativa, podemos usar la proporción de los datos en la clase \\(k\\) que caen en cada categoría.",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "7. Clasificación"
    ]
  },
  {
    "objectID": "7_clasificacion.html#regresión-logística-1",
    "href": "7_clasificacion.html#regresión-logística-1",
    "title": "Clasificación",
    "section": "Regresión logística",
    "text": "Regresión logística\n\nfrom sklearn.linear_model import LogisticRegression\n\nX = default.drop(['default'], axis=1)\nY = default['default']\n\nlogit = LogisticRegression(C=1e10, solver='liblinear')\nresults = logit.fit(X, Y)\n\nPara evaluar los modelos escribiremos una matriz de confusión a partir del siguiente código\n\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\n\ndef matriz_confusion_binaria(y_real, y_pred, levels=[0,1]):\n    \"\"\"\n    Calcula y muestra la matriz de confusión en formato tabla Markdown usando Quarto.\n    \n    Parámetros:\n    - etiquetas_reales: lista o array de etiquetas verdaderas (0 o 1)\n    - predicciones: lista o array de etiquetas predichas (0 o 1)\n    \"\"\"\n    VP = VN = FP = FN = 0\n\n    for real, pred in zip(y_real, y_pred):\n        if real == levels[1] and pred == levels[1]:\n            VP += 1\n        elif real == levels[0] and pred == levels[0]:\n            VN += 1\n        elif real == levels[0] and pred == levels[1]:\n            FP += 1\n        elif real == levels[1] and pred == levels[0]:\n            FN += 1\n\n    # Crear tabla en formato lista de listas\n    tabla = [\n        [\"\", \"Predicción: -\", \"Predicción: +\"],\n        [\"Real: -\", VN, FP],\n        [\"Real: +\", FN, VP]\n    ]\n\n    # Mostrar como tabla Markdown\n    return Markdown(tabulate(tabla, headers=\"firstrow\", tablefmt=\"github\"))\n\n\ny_pred = results.predict(X)\nmatriz_confusion_binaria(Y, y_pred, levels=[1,2])\n\n\n\nTabla 1: Matriz de confusión para la regresión logística.\n\n\n\n\n\n\n\nPredicción: -\nPredicción: +\n\n\n\n\nReal: -\n9659\n8\n\n\nReal: +\n318\n15\n\n\n\n\n\n\n\n\nComo vemos, tenemos \\(8\\) falsos positivos y \\(318\\) falsos negativos. Si esto es bueno o malo depende de los requerimientos del problema. Para la compañía puede ser muy costoso que \\(318\\) personas que se creía podían pagar el crédito luego no lo hagan. Para resolver esto, podemos cambiar el nivel en el cual un cliente se marca como un problema posible. Por ejemplo, lo podemos marcar como problemático si tiene una probabilidad de \\(20\\%\\) de no pagar.\n\ny_pred_proba = results.predict_proba(X)[:,1]\ny_pred = (y_pred_proba &gt; 0.2)\ny_verdad = (Y == 2)\nmatriz_confusion_binaria(y_verdad, y_pred)\n\n\n\nTabla 2: Matriz de confusión para la regresión logística.Con un nivel de 0.2.\n\n\n\n\n\n\n\nPredicción: -\nPredicción: +\n\n\n\n\nReal: -\n9414\n253\n\n\nReal: +\n193\n140\n\n\n\n\n\n\n\n\nAl cambiar el nivel, el número de falsos positivos y negativos cambiará. Mientras mayor el nivel, mayor el número de falsos positivos y menor el de falsos negativos. Esto se puede ver en la llamada curva ROC\n\nfrom sklearn.metrics import roc_curve, auc\n\ndef graficar_curva_roc(y_real, y_pred_proba):\n    \"\"\"\n    Genera y muestra la curva ROC a partir de las etiquetas reales y las probabilidades predichas.\n    \n    Parámetros:\n    - y_real: array de etiquetas verdaderas (0 o 1)\n    - y_pred_proba: array de probabilidades predichas para la clase 1\n    \"\"\"\n    # Calcular valores para la curva ROC\n    fpr, tpr, umbrales = roc_curve(y_real, y_pred_proba)\n    auc_roc = auc(fpr, tpr)\n\n    # Graficar\n    plt.figure(figsize=(6, 5))\n    plt.plot(fpr, tpr, label=f\"Curva ROC (AUC = {auc_roc:.2f})\", linewidth=2)\n    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label=\"Aleatorio\")\n    plt.xlabel(\"Tasa de Falsos Positivos (FPR)\")\n    plt.ylabel(\"Tasa de Verdaderos Positivos (TPR)\")\n    plt.title(\"Curva ROC\")\n    plt.legend(loc=\"lower right\")\n    plt.grid(True)\n    plt.show()\n\n\ngraficar_curva_roc(y_verdad, y_pred_proba)\n\n\n\n\n\n\n\nFigura 6: Curva ROC para la regresión logística. Se muestra también el área bajo la curva AUC. Mientras más cercana a 1 esta cantidad, mejor funciona el modelo.",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "7. Clasificación"
    ]
  },
  {
    "objectID": "7_clasificacion.html#discriminante-lineal-1",
    "href": "7_clasificacion.html#discriminante-lineal-1",
    "title": "Clasificación",
    "section": "Discriminante lineal",
    "text": "Discriminante lineal\nEste método en general tendrá resultados parecidos a los de la regresión logística ya que ambos son lineales. En general será un método muy potente cuando se cumple la suposición de que todas las clases tienen la misma covarianza.\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n\nlda = LDA()\nresults = lda.fit(X, Y)\n\n\ny_pred_proba = results.predict_proba(X)[:,1]\ny_pred = (y_pred_proba &gt; 0.5)\nmatriz_confusion_binaria(y_verdad, y_pred)\n\n\n\nTabla 3: Matriz de confusión para el análisis discriminante lineal.\n\n\n\n\n\n\n\nPredicción: -\nPredicción: +\n\n\n\n\nReal: -\n9645\n22\n\n\nReal: +\n254\n79\n\n\n\n\n\n\n\n\n\ngraficar_curva_roc(y_verdad, y_pred_proba)\n\n\n\n\n\n\n\nFigura 7: Curva ROC para el análisis discriminante lineal.",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "7. Clasificación"
    ]
  },
  {
    "objectID": "7_clasificacion.html#discriminante-cuadrático-1",
    "href": "7_clasificacion.html#discriminante-cuadrático-1",
    "title": "Clasificación",
    "section": "Discriminante cuadrático",
    "text": "Discriminante cuadrático\nEste método es más general que el discriminante lineal. Funciona mejor cuando las diferentes categorías tienen covarianzas diferentes. Sin embargo necesita más datos para reducir la varianza de las predicciones.\n\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n\nqda = QDA()\nresults = qda.fit(X, Y)\n\n\ny_pred_proba = results.predict_proba(X)[:,1]\ny_pred = (y_pred_proba &gt; 0.5)\nmatriz_confusion_binaria(y_verdad, y_pred)\n\n\n\nTabla 4: Matriz de confusión para el análisis discriminante cuadrático.\n\n\n\n\n\n\n\nPredicción: -\nPredicción: +\n\n\n\n\nReal: -\n9636\n31\n\n\nReal: +\n239\n94\n\n\n\n\n\n\n\n\n\ngraficar_curva_roc(y_verdad, y_pred_proba)\n\n\n\n\n\n\n\nFigura 8: Curva ROC para el análisis discriminante cuadrático.",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "7. Clasificación"
    ]
  },
  {
    "objectID": "7_clasificacion.html#bayes-ingenuo-1",
    "href": "7_clasificacion.html#bayes-ingenuo-1",
    "title": "Clasificación",
    "section": "Bayes ingenuo",
    "text": "Bayes ingenuo\nEste método funciona bien cuando se cumple la suposición de que no hay correlación entre los \\(X\\). Tiene un costo computacional y de datos similar al del discriminante lineal, pero puede ser mejor si se cumple esta suposición ya que admite una varianza diferente en cada clase.\n\nfrom sklearn.naive_bayes import GaussianNB\n\nnb = GaussianNB()\nresults = nb.fit(X, Y)\n\n\ny_pred_proba = results.predict_proba(X)[:,1]\ny_pred = (y_pred_proba &gt; 0.5)\nmatriz_confusion_binaria(y_verdad, y_pred)\n\n\n\nTabla 5: Matriz de confusión para el método de Bayes ingenuo.\n\n\n\n\n\n\n\nPredicción: -\nPredicción: +\n\n\n\n\nReal: -\n9620\n47\n\n\nReal: +\n246\n87\n\n\n\n\n\n\n\n\n\ngraficar_curva_roc(y_verdad, y_pred_proba)\n\n\n\n\n\n\n\nFigura 9: Curva ROC para el método de Bayes ingenuo.",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "7. Clasificación"
    ]
  },
  {
    "objectID": "7_clasificacion.html#k-vecinos-más-cercanos",
    "href": "7_clasificacion.html#k-vecinos-más-cercanos",
    "title": "Clasificación",
    "section": "\\(k\\) vecinos más cercanos",
    "text": "\\(k\\) vecinos más cercanos\nEste es un método no paramétrico. Esto implica que no dependerá de lo que supongamos sobre los datos y variables. Sin embargo los resultados dependen del hiperparámetro \\(k\\) y serán en general peores que los métodos paramétricos cuando las suposiciones que esos hacen se cumplen.\nUn detalle de estos modelos es que dependen de la distancia entre puntos. Esta distancia en general depende de las unidades en las que se mide. Por ejemplo si una columna de datos de propiedades está en centímetros cuadrados y otra en millones de pesos, el modelo creerá que una diferencia de diez mil centímetros cuadrados (un metro cuadrado) es igual de importante que una diferencia de diez mil millones de pesos. Por eso se acostumbra reescalar todos los valores a que tengan un valor en el rango \\([0,1]\\) antes de entrenar el modelo.\n\n\\(k = 1\\)\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nknn1 = KNeighborsClassifier(n_neighbors=1)\nresults = knn1.fit(X_scaled, Y)\n\n\ny_pred_proba = results.predict_proba(X)[:,1]\ny_pred = (y_pred_proba &gt; 0.5)\nmatriz_confusion_binaria(y_verdad, y_pred)\n\n/home/jnorena/machine_learning_course/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2684: UserWarning: X has feature names, but KNeighborsClassifier was fitted without feature names\n  warnings.warn(\n\n\n\n\nTabla 6: Matriz de confusión para el método de los \\(k\\) vecinos más cercanos con \\(k=1\\).\n\n\n\n\n\n\n\nPredicción: -\nPredicción: +\n\n\n\n\nReal: -\n9666\n1\n\n\nReal: +\n333\n0\n\n\n\n\n\n\n\n\n\ngraficar_curva_roc(y_verdad, y_pred_proba)\n\n\n\n\n\n\n\nFigura 10: Curva ROC para el método de los \\(k\\) vecinos más cercanos con \\(k=1\\).\n\n\n\n\n\n\n\n\\(k = 30\\)\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nknn1 = KNeighborsClassifier(n_neighbors=30)\nresults = knn1.fit(X_scaled, Y)\n\n\ny_pred_proba = results.predict_proba(X)[:,1]\ny_pred = (y_pred_proba &gt; 0.5)\nmatriz_confusion_binaria(y_verdad, y_pred)\n\n/home/jnorena/machine_learning_course/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2684: UserWarning: X has feature names, but KNeighborsClassifier was fitted without feature names\n  warnings.warn(\n\n\n\n\nTabla 7: Matriz de confusión para el método de los \\(k\\) vecinos más cercanos con \\(k=30\\).\n\n\n\n\n\n\n\nPredicción: -\nPredicción: +\n\n\n\n\nReal: -\n9666\n1\n\n\nReal: +\n333\n0\n\n\n\n\n\n\n\n\n\ngraficar_curva_roc(y_verdad, y_pred_proba)\n\n\n\n\n\n\n\nFigura 11: Curva ROC para el método de los \\(k\\) vecinos más cercanos con \\(k=30\\).\n\n\n\n\n\nLo que está ocurriendo aquí es que hay una variable categórica (student) que pone un poco de problemas con las distancias.",
    "crumbs": [
      "Home",
      "Aprendizaje Automático",
      "7. Clasificación"
    ]
  },
  {
    "objectID": "3_verosimilitud.html",
    "href": "3_verosimilitud.html",
    "title": "Más Sobre la Normal y la Función de Verosimilitud",
    "section": "",
    "text": "Antes de empezar a hablar de verosimilitud, hablemos un poco sobre uno de los resultados más importantes de la estadística: El teorema central del límite.",
    "crumbs": [
      "Home",
      "Probabilidad y Estadística",
      "3. Verosimilitud"
    ]
  },
  {
    "objectID": "3_verosimilitud.html#ejemplo",
    "href": "3_verosimilitud.html#ejemplo",
    "title": "Más Sobre la Normal y la Función de Verosimilitud",
    "section": "Ejemplo",
    "text": "Ejemplo\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef suma_aleatorios(num):\n  return sum([np.random.rand(1000) for i in range(num)])\n\nfig = plt.figure(figsize=(6,11), dpi= 100)\n\nax1 = fig.add_subplot(4,2,1)\nax2 = fig.add_subplot(4,2,2)\nax3 = fig.add_subplot(4,2,3)\nax4 = fig.add_subplot(4,2,4)\n\nax1.hist(suma_aleatorios(1), bins=30)\nax2.hist(suma_aleatorios(2), bins=30)\nax3.hist(suma_aleatorios(20), bins=30)\nax4.hist(suma_aleatorios(200), bins=30)\n        \nax1.set_title('$n = 1$')\nax2.set_title('$n = 2$')\nax3.set_title('$n = 20$')\nax4.set_title('$n = 200$')\n\nfig.tight_layout(pad=2.0)\n\n\n\n\n\n\n\n\nAhora estudiaremos algunas pdf’s derivadas de la Gaussiana que se usan mucho para sacar conclusiones estadídsticas.",
    "crumbs": [
      "Home",
      "Probabilidad y Estadística",
      "3. Verosimilitud"
    ]
  },
  {
    "objectID": "3_verosimilitud.html#la-distribución-chi2",
    "href": "3_verosimilitud.html#la-distribución-chi2",
    "title": "Más Sobre la Normal y la Función de Verosimilitud",
    "section": "La distribución \\(\\chi^2\\)",
    "text": "La distribución \\(\\chi^2\\)\nLa variable \\(\\chi^2\\) se define \\[\n\\chi^2 \\equiv \\sum_i \\frac{(y_i - g(x_i))^2}{\\sigma^2}\n\\] Si todos los errores son gaussianos, la probabilidad de obtener un cierto valor de \\(\\chi^2\\) es proporcional a \\(e^{-\\chi^2/2}\\) y además hay que integrar sobre todos los puntos que tienen un mismo valor de \\(\\chi\\) que forman una esfera, lo que nos da algo proporcional a \\(\\chi^{N - 1}e^{-\\chi^2/2}\\), donde \\(N\\) es el número de datos. Pero además necesitamos la probabilidad de \\(\\chi^2\\) tal que \\[\nP(\\chi^2) \\propto P(\\chi) \\frac{d\\chi}{d\\chi^2} = \\chi^{N-2}e^{-\\chi^2/2}\n\\] Sin embargo al ajustar \\(m\\) parámetros imponemos \\(m\\) condiciones sobre la región considerada, una por cada derivada parcial que fijamos a cero, tal que \\(N\\) debe ser en realidad el número de grados de libertad dado por el número de datos menos el número de parámetros.\nNos falta solo una normalización para que la integral sea uno. Los expertos reconocerán la función gama.\n\nfrom scipy.special import gamma\n\ndef chi2(x, n):\n    return 2**(-n/2)*x**((n-2)/2)*np.exp(-x/2)/gamma(n/2)\n\nx = np.linspace(0, 23, 100)\nfor n in [2, 3, 5, 10]:\n    plt.plot(x, chi2(x, n), label = '$n = $ %d'%(n))\nplt.xlabel(r'$\\chi^2$')\nplt.ylabel(r'$P(\\chi^2)$')\nplt.legend()",
    "crumbs": [
      "Home",
      "Probabilidad y Estadística",
      "3. Verosimilitud"
    ]
  },
  {
    "objectID": "3_verosimilitud.html#la-distribución-t-de-student",
    "href": "3_verosimilitud.html#la-distribución-t-de-student",
    "title": "Más Sobre la Normal y la Función de Verosimilitud",
    "section": "La distribución \\(t\\) de Student",
    "text": "La distribución \\(t\\) de Student\nStudent es el pseudónimo de la persona que inventó esta distribución.\nSegún el teorema central del límite, el promedio \\(\\bar{x}\\) tiene una distribución Gaussiana con media \\(\\mu\\) y varianza \\(\\sigma_n = \\sigma/\\sqrt{n}\\). Esto lo podemos usar para estimar el error cometido al realizar una medición. Por ejemplo, podemos usar \\(\\sigma\\) para estimar el error y sabemos que tenemos una probabilidad de aproximadamente \\(68\\%\\) de estar a \\(1\\sigma\\) de la media, \\(95\\%\\) de estar a \\(2\\sigma\\), y \\(99\\%\\) de estar a \\(3\\sigma\\).\nSin embargo no conocemos \\(\\sigma\\) en muchos casos. Entonces debemos usar algún estimador \\(\\hat{\\sigma}\\) calculado usando los mismos datos.\nEste reemplazo sigue produciendo una distribución Gaussiana cuando tenemos muchos datos. Pero si tenemos pocos (en la práctica aproximadamente menos de \\(30\\) datos) debemos tener cuidado.\nConsideremos la variable \\[\nu = \\frac{\\bar{x} - \\mu}{\\sigma}\\,,\n\\] esta sigue una distribución Gaussiana con media cero y varianza uno. Si estimamos la varianza, escribimos \\[\nt = \\frac{\\bar{x} - \\mu}{\\hat{\\sigma}/\\sqrt{n}}\\,.\n\\] Para \\(n\\) grande, \\(t\\) resulta que también sigue una Gaussiana. Pero para \\(n\\) pequeño sigue otra distribución llamada de Student. Usando lo que hemos visto podemos deducir la distribución, pero no lo haremos aquí. La derivación y forma están en el libro.\n\n# Código producido por ChatGPT 4o con el siguiente prompt:\n# \"Produce una serie de gráficos en python usando la distribución t de Student \n# y comparándola con la distribución normal. Comentarios y nombres de variables\n# y funciones en español.\"\n\n# Código revisado para garantizar que sea correcto\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n\n# Definir grados de libertad para la distribución t de Student\ngrados_libertad = [1, 2, 5, 10, 30]  # Diferentes grados de libertad\nx = np.linspace(-5, 5, 1000)  # Valores de X\n\n# Crear la figura y los ejes\nfigura, ejes = plt.subplots(nrows=len(grados_libertad), figsize=(6, 10), sharex=True)\n\n# Graficar para cada grado de libertad\nfor eje, gl in zip(ejes, grados_libertad):\n    # Distribución t de Student\n    t_pdf = stats.t.pdf(x, gl)\n    eje.plot(x, t_pdf, label=f\"Distribución t (gl={gl})\", linestyle='-', linewidth=2)\n    \n    # Distribución normal para comparación\n    normal_pdf = stats.norm.pdf(x)\n    eje.plot(x, normal_pdf, label=\"Distribución normal\", linestyle='--', linewidth=2)\n    \n    eje.legend()\n    eje.set_ylabel(\"Densidad\")\n    eje.set_title(f\"Distribución t vs Normal (gl={gl})\")\n\n# Establecer etiqueta común en el eje x\nejes[-1].set_xlabel(\"x\")\n\n# Ajustar el diseño\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Home",
      "Probabilidad y Estadística",
      "3. Verosimilitud"
    ]
  },
  {
    "objectID": "3_verosimilitud.html#cota-de-mínima-varianza",
    "href": "3_verosimilitud.html#cota-de-mínima-varianza",
    "title": "Más Sobre la Normal y la Función de Verosimilitud",
    "section": "Cota de mínima varianza",
    "text": "Cota de mínima varianza\nAntes de seguir, supongamos que el valor esperado del estimador es igual al verdadero valor. Esto ocurre si el estimador es no sesgado o para muestras grandes si es consistente. Es decir, suponemos \\[\n\\langle\\hat{\\theta}\\rangle = \\int d^n x\\,\\hat{\\theta}L = \\theta\\,.\n\\tag{1}\\] Demostraremos que hay una mínima varianza posible.\nPrimero tomamos la derivada de la ecuación (1) para obtener \\[\n\\int d^nx\\, \\hat{\\theta}\\frac{\\partial \\ln L}{\\partial\\theta} L = \\int d^nx\\,\\hat{\\theta}\\frac{\\partial L}{\\partial\\theta} = 1\\,.\n\\tag{2}\\] Luego tomamos la misma derivada de \\(\\int d^nx\\,L = 1\\) que nos da \\[\n\\int d^nx\\,\\frac{\\partial\\ln L}{\\partial\\theta}L = 0\\,,\n\\tag{3}\\] que también se puede escribir \\[\n\\left\\langle\\frac{\\partial\\ln L}{\\partial\\theta}\\right\\rangle = 0\\,.\n\\tag{4}\\] Multiplicando la ecuación (3) por \\(\\theta\\) y restando la ecuación (2) obtenemos \\[\n\\int d^nx\\,(\\hat{\\theta} - \\theta)\\frac{\\partial\\ln L}{\\partial\\theta}L = 1\\,.\n\\] Ahora invocamos la siguiente identidad \\[\n\\int d^nx\\,u \\int d^nx\\,v \\geq \\left(\\int d^nx uv\\right)^2\\,,\n\\] para obtener \\[\n\\left(\\int d^nx (\\hat{\\theta} - \\theta)^2 L\\right)\\left(\\int d^nx\\,(d\\ln L/d\\theta)^2 L\\right) \\geq 1\\,.\n\\] Esto lo reescribimos de la siguiente manera:\n\n\n\n\n\n\nCota de mínima varianza\n\n\n\nPara todo estimador cuyo valor esperado es igual al verdadero valor \\[\n\\left\\langle(\\hat{\\theta} - \\theta)^2\\right\\rangle \\geq \\frac{1}{\\left\\langle(d\\ln L/d\\theta)^2\\right\\rangle}\\,.\n\\tag{5}\\]",
    "crumbs": [
      "Home",
      "Probabilidad y Estadística",
      "3. Verosimilitud"
    ]
  },
  {
    "objectID": "3_verosimilitud.html#estimación-de-un-parámetro-con-máxima-verosimilitud",
    "href": "3_verosimilitud.html#estimación-de-un-parámetro-con-máxima-verosimilitud",
    "title": "Más Sobre la Normal y la Función de Verosimilitud",
    "section": "Estimación de un parámetro con máxima verosimilitud",
    "text": "Estimación de un parámetro con máxima verosimilitud\nUno de los estimadores más usados en estadística es el de máxima verosimilitud. Es decir \\[\n\\hat{\\theta} = \\underset{\\theta}{\\operatorname{argmax}} L(\\theta)\\,.\n\\] En otras palabras buscamos un punto tal que \\[\n\\left.\\frac{\\partial L}{\\partial\\theta}\\right|_{\\hat{\\theta}} = 0\\,,\\quad \\left.\\frac{\\partial^2 L}{\\partial\\theta^2}\\right|_{\\hat{\\theta}} &lt; 0\\,.\n\\] Normalmente es más cómodo trabajar con el logaritmo, y la primera condición se puede escribir \\[\n\\left.\\frac{\\partial \\ln L}{\\partial \\theta}\\right|_{\\hat{\\theta}} = \\frac{1}{L}\\left.\\frac{\\partial L}{\\partial \\theta}\\right|_{\\hat{\\theta}} = 0\\,.\n\\]\n\n\n\n\n\n\nEjemplo\n\n\n\nSuponga que usted hace un experimento en el cual quiere medir el tiempo de vida medio de un elemento radioactivo \\(\\tau\\). Usted observa los tiempos de decaimiento \\(\\{t_i\\}\\).\nSabemos que la probabilidad de decaer en un tiempo \\(t\\) es \\(\\frac{1}{\\tau}e^{-t_i/\\tau}\\), por lo tanto la verosimilitud es \\[\nL(\\tau) = \\frac{1}{\\tau^n}\\prod_i e^{-t_i/\\tau}\\,.\n\\] Como dijimos, es más fácil trabajar con el logaritmo porque convierte las multiplicaciones en sumas y cancela los exponenciales que aparecen en muchas distribuciones \\[\n\\ln L(\\tau) = -\\sum_i \\frac{t_i}{\\tau} - n\\ln\\tau\\,.\n\\] Podemos calcular su derivada \\[\n\\frac{\\partial \\ln L}{\\partial \\tau} = \\sum_i \\frac{t_i}{\\tau^2} - n\\frac{1}{\\tau}\\,.\n\\] Igualando a cero obtenemos \\[\n\\hat{\\tau} = \\frac{1}{N}\\sum_i t_i\\,.\n\\]\n\n\nEn general los estimadores de máxima verosimilitud son sesgados como veremos con un ejemplo.\n\n\n\n\n\n\nEjemplo\n\n\n\nConsideremos una distribución gaussiana de la cual deconocemos la media y la varianza. Es decir \\[\nf(x_i; \\mu, \\theta) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-(x_i - \\mu)^2/2\\sigma^2}\\,.\n\\] Busquemos el estimador de máxima verosimilitud para \\(\\mu\\) y \\(\\sigma\\) a partir de una muestra \\(\\{x_i\\}\\).\nLa verosimilitud es fácil de escribir \\[\n\\ln L = -n\\ln(\\sqrt{2\\pi}\\sigma) - \\sum_i\\frac{(x_i - \\mu)}{2\\sigma^2}\\,.\n\\] Tomando derivadas e igualando a cero se obtiene el sistema \\[\n\\sum_i(x_i - \\hat{\\mu}) = 0\\,,\\quad \\sum_i\\frac{(x_i - \\hat{\\mu})^2}{\\hat{\\sigma}^3} - \\frac{n}{\\hat{\\sigma}} = 0\\,.\n\\] La solución a la primera ecuación es fácil \\[\n\\hat{\\mu} = \\frac{1}{n}\\sum_i x_i = \\bar{x}\\,.\n\\] Este es el mismo estimador no sesgado estudiado anteriormente. Reemplazamos esto en la segunda ecuación, que también se hace fácil de resolver \\[\n\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_i(x_i - \\bar{x})^2\\,.\n\\] Este no es el estimador no sesgado. La máxima verosimilitud no nos da un estimador con esta propiedad.\n\n\nPara grandes muestras esto no es un problema ya que \\((n - 1) \\approx n\\).\nEntonces, ¿por qué usar estimadores de máxima verosimilitud? Es porque tienen dos propiedades útiles.",
    "crumbs": [
      "Home",
      "Probabilidad y Estadística",
      "3. Verosimilitud"
    ]
  },
  {
    "objectID": "3_verosimilitud.html#invarianza-de-estimadores-de-máxima-verosimilitud",
    "href": "3_verosimilitud.html#invarianza-de-estimadores-de-máxima-verosimilitud",
    "title": "Más Sobre la Normal y la Función de Verosimilitud",
    "section": "Invarianza de estimadores de máxima verosimilitud",
    "text": "Invarianza de estimadores de máxima verosimilitud\nSupongamos que tenemos un estimador para \\(\\theta\\), pero para el problema que queremos resolver nos interesa más una función \\(F(\\theta)\\). En general \\(\\hat{F}(\\theta) \\neq F(\\hat{\\theta})\\) tal que tenemos que calcular el estimador de la función.\nEsto sí se cumple cuando usamos el estimador de máxima verosimilitud, ya que el punto donde \\(L\\) es máximo es el mismo independientemente de si lo escribimos en función de \\(\\theta\\) o de \\(F(\\theta)\\).",
    "crumbs": [
      "Home",
      "Probabilidad y Estadística",
      "3. Verosimilitud"
    ]
  },
  {
    "objectID": "3_verosimilitud.html#cota-de-mínima-varianza-del-estimador-de-máxima-verosimilitud",
    "href": "3_verosimilitud.html#cota-de-mínima-varianza-del-estimador-de-máxima-verosimilitud",
    "title": "Más Sobre la Normal y la Función de Verosimilitud",
    "section": "Cota de mínima varianza del estimador de máxima verosimilitud",
    "text": "Cota de mínima varianza del estimador de máxima verosimilitud\nResulta que para una muestra grande el estimador de máxima verosimilitud satura la cota de mínima varianza. Es decir, que es un estimador con el error más pequeño posible. La demostración la haremos ahora aunque es un poco larga.\n\n\n\n\n\n\n¡Cuidado!\n\n\n\nNosotros habíamos llamado la varianza del estimador \\(\\left\\langle (\\hat{\\theta} - \\langle\\hat{\\theta}\\rangle)^2 \\right\\rangle\\). Esta corresponde a la varianza alrededor de su valor esperado.\nLa cota de mínima varianza en cambio se refiere a \\(\\left\\langle(\\hat{\\theta} - \\theta)^2\\right\\rangle\\), es decir la varianza alrededor del verdadero valor, que también recibe contribuciones del sesgo. Recuerde que habíamos demostrado:\n\\[\n\\left\\langle(\\hat{\\theta} - \\theta)^2\\right\\rangle = \\left\\langle (\\hat{\\theta} - \\langle\\hat{\\theta}\\rangle)^2 \\right\\rangle + \\left(\\left\\langle \\langle\\hat{\\theta}\\rangle - \\theta\\right\\rangle\\right)^2\\,.\n\\]\n\n\nSupongamos que tenemos una muestra grande, tal que el estimador de máxima verosimilitud \\(\\hat{\\theta}\\) tiende al veradero valor \\(\\theta_*\\). Nuestro estimador satisface \\[\n\\left.\\frac{\\partial \\ln L}{\\partial\\theta}\\right|_{\\theta = \\hat{\\theta}} = 0\\,.\n\\] Como nos esperamos estar cerca del verdadero valor para une muestra grande, podemos aproximar el lado izquierdo con una expansión de Taylor \\[\n\\left.\\frac{\\partial \\ln L}{\\partial \\theta}\\right|_{\\theta = \\theta_*} + (\\hat{\\theta} - \\theta_*)\\left.\\frac{\\partial^2 \\ln L}{\\partial\\theta^2}\\right|_{\\theta = \\theta_*} = 0\\,.\n\\]\nAhora bien, de la ecuación (4) sabemos que el valor esperado de \\(\\partial \\ln L/\\partial \\theta\\) es cero evaluado en \\(\\theta_*\\). Edemás, esta cantidad es la suma de \\(n\\) valores independientes para cada \\(x_i\\), y por el teorema central del límite sabemos que esas sumas van a estar descritas por una gaussiana, con varianza \\[\\begin{align}\n\\left\\langle\\left(\\frac{\\partial \\ln L}{\\partial a}\\right)^2\\right\\rangle &= \\int d^n x \\left(\\frac{\\partial \\ln L}{\\partial \\theta}\\right)^2 L = \\int d^nx \\frac{\\partial \\ln L}{\\partial \\theta} \\frac{\\partial L}{\\partial\\theta} \\\\\n&= -\\int d^nx \\frac{\\partial^2\\ln L}{\\partial\\theta^2} L = \\left\\langle-\\frac{\\partial^2 \\ln L}{\\partial\\theta^2}\\right\\rangle\\,.\n\\end{align}\\]\nComo \\((\\hat{\\theta} - \\theta)\\) es proporcional a \\(\\partial \\ln L/\\partial\\theta\\), su distribución también estará descrita por una gaussiana con varianza \\[\n\\left\\langle(\\hat{\\theta} - \\theta)^2\\right\\rangle = -\\left.\\left\\langle\\frac{\\partial^2\\ln L}{\\partial \\theta^2}\\right\\rangle\\right/\\left(\\left.\\frac{\\partial^2 \\ln L}{\\partial\\theta^2}\\right|_{\\theta=\\theta_*}\\right)^2\n\\] Además para \\(n\\) grande el valor esperado tiende al verdadero valor tal que el numerador cancela una potencia del denominador. Por lo tanto tenemos \\[\n\\left\\langle(\\hat{\\theta} - \\theta)^2\\right\\rangle = -\\frac{1}{\\left\\langle\\frac{\\partial^2 \\ln L}{\\partial\\theta^2}\\right\\rangle}\n\\] Pero esta es el mínimo valor posible según la ecuación (5).",
    "crumbs": [
      "Home",
      "Probabilidad y Estadística",
      "3. Verosimilitud"
    ]
  },
  {
    "objectID": "plan_de_clases.html",
    "href": "plan_de_clases.html",
    "title": "Plan de Clases",
    "section": "",
    "text": "Plan de Clases\n1- Probabilidad y estadística\nProbabilidad, variables aleatorias, valores esperados Distribuciones y estimación Estimación de parámetros, verosimilitud Mínimos cuadrados, estimación de intervalos Testeo de hipótesis\n2- Métodos Clásicos de Aprendizaje Automático\nIntroducción al aprendizaje automático y regresión lineal Clasificación lineal Validación y selección de modelos Modelos no lineales, árboles y bosques Otros modelos: SVM, aprendizaje no supervisado\n3- Redes neuronales\nEl perceptrón, redes completamente conectadas, funciones de pérdida Minimización y gradientes Ajuste y entrenamiento de modelos, regularización Redes convolucionales Redes residuales Transformadores\n4- Aplicaciones de redes neuronales a la física\nFlujos normalizantes Inferencia basada en simulaciones Métodos para solución de PDEs con redes neuronales Operadores neuronales"
  },
  {
    "objectID": "2_distribuciones_y_estimacion.html",
    "href": "2_distribuciones_y_estimacion.html",
    "title": "Distribuciones de Probabilidad y Estimación",
    "section": "",
    "text": "Hablamos de variables aleatorias como variables que pueden tomar un valor aleatorio dentro de un cierto conjunto. Cada valor prosible tiene una cierta probabilidad. Esto lo haremos más preciso en esta sección.\n\n\n\n\nConsideremos primero una variable discreta. Decimos que la variable \\(x\\) tiene una probabilidad \\(P[x_k]\\) de tomar el valor \\(x_k\\). Este valor está dado por una distribución de probabilidad. Es decir una función \\(f\\) tal que \\[\nP[x_k] = f(x_k)\\,.\n\\] Esta cumple\n\n\\(f\\) es univaluada y \\(f(x) \\geq 0\\).\nLa probabilidad total es \\(1\\), es decir \\(\\sum_i f(x_i) = 1\\).\n\nDefinimos también la distribución cumulativa de probabilidad \\[\nF(x) = \\sum_{x_k \\leq x} f(x_k)\\,.\n\\] Es decir, es la probabilidad de obtener un valor menor a \\(x\\).\nPara variables que toman valores continuos, la probabilidad de un valor dado tiende a cero (como hay infinitos valores, la probabilidad de cada uno debe ser minúscula para que su suma sea \\(1\\)). Por ese motivo no hablamos de una distribución de probabilidad sino de una función de densidad de probabilidad \\(f(x)\\) tal que la probabilidad de obtener un resultado en el intervalo \\([a,b]\\) es \\[\nP[a \\leq x \\leq b] = \\int_a^bdx\\,f(x)\\,.\n\\] Esta cumple\n\n\\(f\\) es univaluada y \\(f(x) \\geq 0\\).\nLa probabilidad total es \\(1\\), es decir \\(\\int_{-\\infty}^\\infty dx\\,f(x) = 1\\).\n\nAnálogamente definimos la distribución cumulativa de probabilidad \\[\nF(x) = \\int_{-\\infty}^x dx'\\,f(x')\\,.\n\\]\n\n\n\nDefinimos el valor esperado de una variable discreta como \\[\n\\langle x\\rangle \\equiv \\sum_x xf(x)\\,.\n\\] Para una variable continua \\[\n\\langle x\\rangle \\equiv \\int_{-\\infty}^\\infty dx\\,xf(x)\\,.\n\\]\nDe esta manera los momentos son \\[\n\\mu'_n \\equiv \\langle x^n \\rangle\\,,\n\\] y los momentos centrales \\[\n\\mu_n \\equiv \\left\\langle (x - \\langle x\\rangle)^n\\right\\rangle\\,.\n\\]\nDe la definición de valor esperado se puede demostrar para una constante \\(c\\) \\[\n\\langle cx\\rangle = c\\langle x\\rangle\\,,\\quad \\langle x_1 + x_2 + ... + x_n\\rangle = \\langle x_1\\rangle + \\langle x_2 \\rangle + ... + \\langle x_n\\rangle\\,.\n\\] Además si las variables \\(x_1, ..., x_n\\) son independientes \\[\n\\langle x_1 ... x_n\\rangle = \\langle x_1\\rangle ...\\langle x_n\\rangle\\,.\n\\]\n\n\n\nDefinimos la función generatriz \\[\nM_x(t) \\equiv \\langle e^{xt} \\rangle\\,.\n\\] Expandiendo en Taylor tenemos que \\[\nM_x(t) = \\left\\langle1 + xt + \\frac{1}{2}(xt)^2 + ...\\right\\rangle = \\sum_{n=0}^\\infty \\frac{1}{n!}\\mu'_n t^n\\,.\n\\] De esta expresión vemos que \\[\n\\mu'_n = \\left.\\frac{\\partial^n}{\\partial t^n}M_x(t)\\right|_{t=0}\\,.\n\\] Entonces decimos que podemos generar los momentos de la distribución.\nEn la mayoría de los casos, si dos distribuciones tienen la misma función generatriz, entonces son la misma distribución. Para algunas distribuciones la suma no converge, tal que se usa en cambio la función característica \\[\n\\phi_x(t) \\equiv \\langle e^{itx}\\rangle\\,.\n\\]\nTambién es útil definir la función generatriz conexa \\[\n\\ln M_x(t) \\equiv \\kappa_1 t + \\frac{1}{2}\\kappa_2 t^2 + ...\\,,\n\\] donde \\(\\kappa\\) son los cumulantes. Cuando dos distribuciones tienen los mismos cumulantes son iguales (si estos existen para ambas). El primer cumulante corresponde a la media \\(\\kappa_1 = \\mu\\) y el segundo a la varianza \\(\\kappa_2 = \\sigma^2\\).\n\n\n\n\n\n\nSi tenemos varias variables aleatorias, las definiciones son muy análogas. Por ejemplo, la distribución de probabilidad conjunta \\[\nP[a_1\\leq x_1 \\leq b_1, ..., a_n \\leq x_n \\leq b_n] \\equiv \\int_{a_n}^{b_n}...\\int_{a_1}^{b_1}\\prod_{i=1}^n dx_i\\, f(x_1, ..., x_n)\\,.\n\\]\n\n\n\nSi nos preguntamos cuál es el valor de una variable, independientemente del valor que tomen todas las demás, sólo tenemos que sumar sobre todos los valores de las demás variables. Esta se llama la distribución marginal \\[\nf^M(x_1) = \\int_{-\\infty}^\\infty...\\int_{-\\infty}^\\infty \\prod_{i=2}^n dx_i\\,f(x_1, x_2, ..., x_n)\\,.\n\\] Análogamente, si queremos la probabilidad conjunta para algunas variables, marginalizamos sobre todas las demás \\[\nf^M(x_1, ..., x_m) = \\int_{-\\infty}^\\infty...\\int_{-\\infty}^\\infty \\prod_{i=m+1}^n dx_i\\,f(x_1, x_2, ..., x_n)\\,.\n\\]\nUna pregunta distinta, es si fijamos el valor de las demás variables y nos preguntamos por la probabilidad de las restantes. Esta es la distribución condicional \\[\nf^C(x_1, ..., x_m| x_{m+1}, ..., x_n) \\equiv \\frac{f(x_1, ..., x_n)}{f^M(x_{m+1}, ..., x_n)}\\,.\n\\]\nEsto nos permite escribir por ejemplo la siguiente relación \\[\nf^C(x|y) = \\frac{f^C(y|x)f^M(x)}{f^M(y)}\\,,\n\\] o también \\[\nf(x, y) = f^C(x|y)f^M(y)\\,.\n\\] Decimos que dos variables son independientes si \\[\nf(x, y) = f^M(x)f^M(y)\\,.\n\\]\n\n\n\nSe definen de forma análoga a una variable. Por ejemplo, definimos la covarianza como el segundo momento central \\[\n\\text{cov}(x_i, x_j) \\equiv \\sigma_{ij} \\equiv \\int_{-\\infty}^\\infty...\\int_{-\\infty}^\\infty \\prod_{i=1}^n dx_i\\, (x_i - \\mu_i)(x_j - \\mu_j) f(x_1, ..., x_n)\\,.\n\\] La condición \\(\\sigma_{ij} = 0\\) es necesaria (pero no suficiente) para que dos variables sean independientes.\n\n\n\n\nSupongamos que la cantidad \\(y\\) es una función de la variable aleatoria \\(x\\). Podemos calcular su distribución de probabilidad haciendo un simple cambio de variable \\[\nf(y) = \\sum_{x}f(x)\\left|\\frac{dx}{dy}\\right|\\,.\n\\] Donde la suma es sobre todos los puntos tal que \\(y = y(x)\\).\nCuando hay varias variables, usamos el Jacobiano.",
    "crumbs": [
      "Home",
      "Probabilidad y Estadística",
      "2. Distribuciones y Estimación"
    ]
  },
  {
    "objectID": "2_distribuciones_y_estimacion.html#una-variable-aleatoria",
    "href": "2_distribuciones_y_estimacion.html#una-variable-aleatoria",
    "title": "Distribuciones de Probabilidad y Estimación",
    "section": "",
    "text": "Consideremos primero una variable discreta. Decimos que la variable \\(x\\) tiene una probabilidad \\(P[x_k]\\) de tomar el valor \\(x_k\\). Este valor está dado por una distribución de probabilidad. Es decir una función \\(f\\) tal que \\[\nP[x_k] = f(x_k)\\,.\n\\] Esta cumple\n\n\\(f\\) es univaluada y \\(f(x) \\geq 0\\).\nLa probabilidad total es \\(1\\), es decir \\(\\sum_i f(x_i) = 1\\).\n\nDefinimos también la distribución cumulativa de probabilidad \\[\nF(x) = \\sum_{x_k \\leq x} f(x_k)\\,.\n\\] Es decir, es la probabilidad de obtener un valor menor a \\(x\\).\nPara variables que toman valores continuos, la probabilidad de un valor dado tiende a cero (como hay infinitos valores, la probabilidad de cada uno debe ser minúscula para que su suma sea \\(1\\)). Por ese motivo no hablamos de una distribución de probabilidad sino de una función de densidad de probabilidad \\(f(x)\\) tal que la probabilidad de obtener un resultado en el intervalo \\([a,b]\\) es \\[\nP[a \\leq x \\leq b] = \\int_a^bdx\\,f(x)\\,.\n\\] Esta cumple\n\n\\(f\\) es univaluada y \\(f(x) \\geq 0\\).\nLa probabilidad total es \\(1\\), es decir \\(\\int_{-\\infty}^\\infty dx\\,f(x) = 1\\).\n\nAnálogamente definimos la distribución cumulativa de probabilidad \\[\nF(x) = \\int_{-\\infty}^x dx'\\,f(x')\\,.\n\\]\n\n\n\nDefinimos el valor esperado de una variable discreta como \\[\n\\langle x\\rangle \\equiv \\sum_x xf(x)\\,.\n\\] Para una variable continua \\[\n\\langle x\\rangle \\equiv \\int_{-\\infty}^\\infty dx\\,xf(x)\\,.\n\\]\nDe esta manera los momentos son \\[\n\\mu'_n \\equiv \\langle x^n \\rangle\\,,\n\\] y los momentos centrales \\[\n\\mu_n \\equiv \\left\\langle (x - \\langle x\\rangle)^n\\right\\rangle\\,.\n\\]\nDe la definición de valor esperado se puede demostrar para una constante \\(c\\) \\[\n\\langle cx\\rangle = c\\langle x\\rangle\\,,\\quad \\langle x_1 + x_2 + ... + x_n\\rangle = \\langle x_1\\rangle + \\langle x_2 \\rangle + ... + \\langle x_n\\rangle\\,.\n\\] Además si las variables \\(x_1, ..., x_n\\) son independientes \\[\n\\langle x_1 ... x_n\\rangle = \\langle x_1\\rangle ...\\langle x_n\\rangle\\,.\n\\]\n\n\n\nDefinimos la función generatriz \\[\nM_x(t) \\equiv \\langle e^{xt} \\rangle\\,.\n\\] Expandiendo en Taylor tenemos que \\[\nM_x(t) = \\left\\langle1 + xt + \\frac{1}{2}(xt)^2 + ...\\right\\rangle = \\sum_{n=0}^\\infty \\frac{1}{n!}\\mu'_n t^n\\,.\n\\] De esta expresión vemos que \\[\n\\mu'_n = \\left.\\frac{\\partial^n}{\\partial t^n}M_x(t)\\right|_{t=0}\\,.\n\\] Entonces decimos que podemos generar los momentos de la distribución.\nEn la mayoría de los casos, si dos distribuciones tienen la misma función generatriz, entonces son la misma distribución. Para algunas distribuciones la suma no converge, tal que se usa en cambio la función característica \\[\n\\phi_x(t) \\equiv \\langle e^{itx}\\rangle\\,.\n\\]\nTambién es útil definir la función generatriz conexa \\[\n\\ln M_x(t) \\equiv \\kappa_1 t + \\frac{1}{2}\\kappa_2 t^2 + ...\\,,\n\\] donde \\(\\kappa\\) son los cumulantes. Cuando dos distribuciones tienen los mismos cumulantes son iguales (si estos existen para ambas). El primer cumulante corresponde a la media \\(\\kappa_1 = \\mu\\) y el segundo a la varianza \\(\\kappa_2 = \\sigma^2\\).",
    "crumbs": [
      "Home",
      "Probabilidad y Estadística",
      "2. Distribuciones y Estimación"
    ]
  },
  {
    "objectID": "2_distribuciones_y_estimacion.html#varias-variables",
    "href": "2_distribuciones_y_estimacion.html#varias-variables",
    "title": "Distribuciones de Probabilidad y Estimación",
    "section": "",
    "text": "Si tenemos varias variables aleatorias, las definiciones son muy análogas. Por ejemplo, la distribución de probabilidad conjunta \\[\nP[a_1\\leq x_1 \\leq b_1, ..., a_n \\leq x_n \\leq b_n] \\equiv \\int_{a_n}^{b_n}...\\int_{a_1}^{b_1}\\prod_{i=1}^n dx_i\\, f(x_1, ..., x_n)\\,.\n\\]\n\n\n\nSi nos preguntamos cuál es el valor de una variable, independientemente del valor que tomen todas las demás, sólo tenemos que sumar sobre todos los valores de las demás variables. Esta se llama la distribución marginal \\[\nf^M(x_1) = \\int_{-\\infty}^\\infty...\\int_{-\\infty}^\\infty \\prod_{i=2}^n dx_i\\,f(x_1, x_2, ..., x_n)\\,.\n\\] Análogamente, si queremos la probabilidad conjunta para algunas variables, marginalizamos sobre todas las demás \\[\nf^M(x_1, ..., x_m) = \\int_{-\\infty}^\\infty...\\int_{-\\infty}^\\infty \\prod_{i=m+1}^n dx_i\\,f(x_1, x_2, ..., x_n)\\,.\n\\]\nUna pregunta distinta, es si fijamos el valor de las demás variables y nos preguntamos por la probabilidad de las restantes. Esta es la distribución condicional \\[\nf^C(x_1, ..., x_m| x_{m+1}, ..., x_n) \\equiv \\frac{f(x_1, ..., x_n)}{f^M(x_{m+1}, ..., x_n)}\\,.\n\\]\nEsto nos permite escribir por ejemplo la siguiente relación \\[\nf^C(x|y) = \\frac{f^C(y|x)f^M(x)}{f^M(y)}\\,,\n\\] o también \\[\nf(x, y) = f^C(x|y)f^M(y)\\,.\n\\] Decimos que dos variables son independientes si \\[\nf(x, y) = f^M(x)f^M(y)\\,.\n\\]\n\n\n\nSe definen de forma análoga a una variable. Por ejemplo, definimos la covarianza como el segundo momento central \\[\n\\text{cov}(x_i, x_j) \\equiv \\sigma_{ij} \\equiv \\int_{-\\infty}^\\infty...\\int_{-\\infty}^\\infty \\prod_{i=1}^n dx_i\\, (x_i - \\mu_i)(x_j - \\mu_j) f(x_1, ..., x_n)\\,.\n\\] La condición \\(\\sigma_{ij} = 0\\) es necesaria (pero no suficiente) para que dos variables sean independientes.",
    "crumbs": [
      "Home",
      "Probabilidad y Estadística",
      "2. Distribuciones y Estimación"
    ]
  },
  {
    "objectID": "2_distribuciones_y_estimacion.html#funciones-de-una-variable-aleatoria",
    "href": "2_distribuciones_y_estimacion.html#funciones-de-una-variable-aleatoria",
    "title": "Distribuciones de Probabilidad y Estimación",
    "section": "",
    "text": "Supongamos que la cantidad \\(y\\) es una función de la variable aleatoria \\(x\\). Podemos calcular su distribución de probabilidad haciendo un simple cambio de variable \\[\nf(y) = \\sum_{x}f(x)\\left|\\frac{dx}{dy}\\right|\\,.\n\\] Donde la suma es sobre todos los puntos tal que \\(y = y(x)\\).\nCuando hay varias variables, usamos el Jacobiano.",
    "crumbs": [
      "Home",
      "Probabilidad y Estadística",
      "2. Distribuciones y Estimación"
    ]
  },
  {
    "objectID": "2_distribuciones_y_estimacion.html#uniforme",
    "href": "2_distribuciones_y_estimacion.html#uniforme",
    "title": "Distribuciones de Probabilidad y Estimación",
    "section": "Uniforme",
    "text": "Uniforme\nSi la probabilidad es la misma en todas partes dentro de un intervalo \\[\nf(x) \\equiv u(x; a, b) = \\begin{cases}\n\\frac{1}{b - a}\\quad a\\leq x \\leq b\\,\n0\\quad \\text{de otra manera}\n\\end{cases}\\,.\n\\] Esta probabilidad es útil por ejemplo para generar números aleatorios. Cualquier variable se puede convertir en una variable con distribución uniforme mediante \\(u = F[x]\\).",
    "crumbs": [
      "Home",
      "Probabilidad y Estadística",
      "2. Distribuciones y Estimación"
    ]
  },
  {
    "objectID": "2_distribuciones_y_estimacion.html#normal-gaussiana",
    "href": "2_distribuciones_y_estimacion.html#normal-gaussiana",
    "title": "Distribuciones de Probabilidad y Estimación",
    "section": "Normal (Gaussiana)",
    "text": "Normal (Gaussiana)\nUbicua en física por motivos que discutiremos en unos minutos. La famosa distribución a campana \\[\nf(x) \\equiv n(x; \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left[-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right]\\,.\n\\] Con función generatriz \\[\nM_x(t) = \\langle e^{xt}\\rangle = e^{t\\mu + \\sigma^2 t^2/2}\\,.\n\\] Podemos tomar el logaritmo para obtener los cumulantes \\[\n\\ln M_x(t) = t\\mu + \\frac{1}{2}t^2\\sigma^2\\,.\n\\] De aquí se puede deducir que \\(\\kappa_1 = \\mu\\) y \\(\\kappa_2 = \\sigma^2\\) por lo que la varianza es \\(\\sigma^2\\). La función característica es análoga \\[\n\\phi(t) = e^{it\\mu - t^2\\sigma^2/2}\\,.\n\\]\nLa utilidad de la gaussiana es que por motivos que vermos representa los errores de muchas cantidades medidas. Entonces es útil saber las probabilidades de obtener un resultado a un cierto número de desviaciones estándar de la media\n\n\\(1\\sigma\\) es \\(68.3\\%\\).\n\\(2\\sigma\\) es \\(95.4\\%\\).\n\\(3\\sigma\\) es \\(99.7\\%\\).\n\nPara varias variables es similar \\[\nf(\\mathbf{x}) = \\frac{1}{(2\\pi)^{n/2}|V|^{1/2}}\\exp\\left[-\\frac{1}{2}(\\mathbf{x} - \\mathbf{\\mu})^T V^{-1} (\\mathbf{x} - \\mathbf{\\mu})\\right]\\,.\n\\] La covarianza es fácil de calcular y es \\(\\sigma_{ij} = V_{ij}\\).\nIntegrando, se puede demostrar que la distribución marginal también es una gaussiana.",
    "crumbs": [
      "Home",
      "Probabilidad y Estadística",
      "2. Distribuciones y Estimación"
    ]
  },
  {
    "objectID": "2_distribuciones_y_estimacion.html#binomial",
    "href": "2_distribuciones_y_estimacion.html#binomial",
    "title": "Distribuciones de Probabilidad y Estimación",
    "section": "Binomial",
    "text": "Binomial\nLa distribución binomial es para una variable discreta. Representa intentar muchas veces un experimento con probabilidad de éxito \\(p\\). Esta distribución es la probabilidad de obtener \\(r\\) éxitos en \\(n\\) intentos \\[\nf(r; p, n) = \\frac{n!}{r!(n - r)!}p^r (1 - p)^{n - r}\\,.\n\\] No usaremos mucho esta distribución. Esta y otras se pueden encontrar descritas en el libro.",
    "crumbs": [
      "Home",
      "Probabilidad y Estadística",
      "2. Distribuciones y Estimación"
    ]
  },
  {
    "objectID": "2_distribuciones_y_estimacion.html#poisson",
    "href": "2_distribuciones_y_estimacion.html#poisson",
    "title": "Distribuciones de Probabilidad y Estimación",
    "section": "Poisson",
    "text": "Poisson\nEs muy importante en física. Representa la probabilidad de obtener \\(r\\) éxitos cuando \\(n\\) tiende a ser un número grande y \\(p\\) tiende a ser pequeño tal que \\(pn \\equiv \\lambda\\) permanece constante \\[\n\\lim_{n\\rightarrow \\infty} \\frac{1}{r!}\\frac{n!}{(n-r)!}\\left(\\frac{\\lambda}{n}\\right)^r\\left(1 - \\frac{\\lambda}{n}\\right)^{n - r} = \\lim_{n\\rightarrow \\infty}\\frac{1}{r!}n^r\\frac{\\lambda^r}{n^r}\\left(1 - \\frac{\\lambda}{n}\\right)^n = \\frac{1}{r!}\\lambda^r e^{-\\lambda}\\,.\n\\] Entonces definimos la distribución de Poisson \\[\nf(r; \\lambda) = \\frac{1}{r!}\\lambda^r e^{-\\lambda}\\,.\n\\] La función generatriz es \\[\nM_r(t) = e^{-\\lambda}\\sum_{r = 0}^\\infty \\frac{1}{k!}(\\lambda e^t)^k = e^{-\\lambda}\\exp(\\lambda e^t)\\,.\n\\] Expandiendo, vemos que todos los cumulantes de la distribución de Poisson son iguales \\[\n\\ln M_r(t) = \\lambda e^t - \\lambda = \\sum_{n=1}^\\infty \\frac{1}{n!}t^n\\lambda\\,.\n\\] es decir \\(\\kappa_i = \\lambda\\). Es decir que su media es \\(\\lambda\\) y su varianza es \\(\\lambda\\).",
    "crumbs": [
      "Home",
      "Probabilidad y Estadística",
      "2. Distribuciones y Estimación"
    ]
  },
  {
    "objectID": "2_distribuciones_y_estimacion.html#muestras-aleatorias-y-estimadores",
    "href": "2_distribuciones_y_estimacion.html#muestras-aleatorias-y-estimadores",
    "title": "Distribuciones de Probabilidad y Estimación",
    "section": "Muestras aleatorias y estimadores",
    "text": "Muestras aleatorias y estimadores\nDefinimos variables independientes e idénticamente distribuídas (i.i.d.) como aquellas variables que provienen de una misma distribución de probabilidad y que además son independientes.\nPara nosotros una muestra aleatoria será un conjunto de variables i.i.d. Esto modela el repetir un experimento u observación muchas veces. Como son i.i.d. su distribución conjunta se escribe \\[\nf(x_1, ..., x_n) = f(x_1)...f(x_n)\\,,\n\\] para alguna distribución \\(f(x)\\).",
    "crumbs": [
      "Home",
      "Probabilidad y Estadística",
      "2. Distribuciones y Estimación"
    ]
  },
  {
    "objectID": "2_distribuciones_y_estimacion.html#propiedades-de-estimadores",
    "href": "2_distribuciones_y_estimacion.html#propiedades-de-estimadores",
    "title": "Distribuciones de Probabilidad y Estimación",
    "section": "Propiedades de estimadores",
    "text": "Propiedades de estimadores\nMuchas veces desconocemos la forma exacta de \\(f(x)\\) pero tenemos un modelo para producirla. Ese modelo puede depender de unos parámetros libres (masas de partículas, varianzas, algún parámetro experimental, …) \\(\\mathbf{\\theta} = (\\theta_1, ..., \\theta_m)\\). Queremos sacar el valor de esos parámetros a partir de los datos. Nuestra aproximación a ese valor usando los datos se llama un estimador \\(\\hat{\\theta}_n\\), que depende del número de datos tomados \\(n\\).\nPuede haber muchos estimadores diferentes, dependiendo del método usado. Discutamos cómo podemos evaluar si un estimador es bueno y en qué sentido lo puede ser.\nUna propiedad importante de un estimador es que sea consistente. Es decir, para todo \\(\\epsilon &gt; 0\\) \\[\n\\lim_{n\\rightarrow \\infty} P(|\\hat{\\theta}_n - \\theta| &gt; \\epsilon) = 0\\,.\n\\] En lenguaje más cristiano, un estimador consistente es aquel que tiende al verdadero valor del parámetro si tenemos infinitos datos.\nQue un estimador sea consistente no quiere decir que para una cantidad finita de datos nos de una buena aproximación al verdadero valor. Para caracterizar cuándo es buena la aproximación definimos otro par de propiedades.\nDefinimos el sesgo (muchas veces llamado por su nombre inglés bias) \\[\nb \\equiv \\langle \\hat{\\theta}_n\\rangle - \\theta\\,.\n\\] Un estimador no sesgado si \\(b = 0\\). En general nos van a interesar muchos estimadores sesgados, pero queremos tratar de mantener el sesgo bajo control.\nAdemás de un sesgo, un estimador tiene una varianza. Como es una cantidad que se obtiene operando sobre \\(n\\) variables aleatorias, el mismo estimador es una variable aleatoria. Por lo tanto tiene una distribución de probabilidad y una varianza. Si la varianza es muy grande, el estimador puede ser problemático porque al medir obtenemos sólo un valor, y este puede estar muy lejos del valor verdadero porque la distribución es muy esparcida.\nDecimos que un estimador es eficiente si la varianza cae rápidamente a medida que aumentamos \\(n\\).\nLo que nos interesa acotar en realidad es la diferencia entre el valor verdadero y el estimado. Este se relaciona con el sesgo y la varianza \\[\n\\left\\langle(\\hat{\\theta}_n - \\theta)^2\\right\\rangle = \\left\\langle(\\hat{\\theta}_n - \\langle\\hat{\\theta}_n\\rangle+ b)^2\\right\\rangle = \\left\\langle(\\hat{\\theta}_n - \\langle\\hat{\\theta}_n\\rangle)^2\\right\\rangle +2b\\left\\langle(\\hat{\\theta}_n - \\langle\\hat{\\theta}_n\\rangle)\\right\\rangle + b^2 = \\sigma^2_{\\hat{\\theta}} + b^2\\,.\n\\] Entonces la varianza del error cometido es la suma de la varianza del estimador y el cuadrado del sesgo. Es decir que idealmente queremos reducir ambos para tener errores pequeños.\nVeremos que en muchos métodos al aumentar el sesgo disminuye la varianza y vice versa, tal que debemos buscar un equilibrio entre ambos.",
    "crumbs": [
      "Home",
      "Probabilidad y Estadística",
      "2. Distribuciones y Estimación"
    ]
  },
  {
    "objectID": "2_distribuciones_y_estimacion.html#estimadores-de-la-media-y-la-varianza",
    "href": "2_distribuciones_y_estimacion.html#estimadores-de-la-media-y-la-varianza",
    "title": "Distribuciones de Probabilidad y Estimación",
    "section": "Estimadores de la media y la varianza",
    "text": "Estimadores de la media y la varianza\nPara dar un ejemplo, supongamos que queremos encontrar la media y la varianza de una distribución a partir de una muestra. Existen estimadores no sesgados para ambos.\nPara la media, el estimador es el promedio sobre la muestra \\[\n\\hat{\\mu} = \\frac{1}{n}\\sum_{i = 1}^n x_i \\equiv \\bar{x}\\,.\n\\] Veamos que es no sesgado recordando que todos vienen de la misma distribución con media \\(\\mu\\) \\[\n\\langle \\hat{\\mu}\\rangle = \\frac{1}{n}\\sum_{i=1}^n \\langle x_i\\rangle = \\frac{1}{n}\\sum_{i=1}^n \\mu = \\mu\\,.\n\\]\nPara la varianza es un poco más sutil. Queremos escribir una expresión que dependa sólo de los datos. Para obtener un estimador no sesgado necesitamos escribir \\[\n\\hat{\\sigma}^2 = \\frac{1}{n - 1}\\sum_{i=1}^n(x_i - \\bar{x})^2\\,.\n\\] Ese denominador no es lo que uno esperaría ingenuamente. Si yo no conociera este tema, habría dicho que el estimador debería ser el promedio sobre las desviaciones al cuadrado, pero el denominador no corresponde a esa intuición. Para ver de dónde viene demostremos que no es sesgado \\[\n\\begin{align}\n\\langle \\hat{\\sigma}^2\\rangle &= \\frac{1}{n - 1}\\sum_{i = 1}^n \\left\\langle(x_i - \\bar{x})^2\\right\\rangle\\\\\n&= \\frac{1}{n - 1}\\sum_{i = 1}^n \\left\\langle\\left(x_i - \\frac{1}{n}\\sum_{j=1}^n x_j\\right)^2\\right\\rangle\\\\\n&= \\frac{n}{n - 1} \\left\\langle\\left(x_1 - \\frac{1}{n}\\sum_{j=1}^n x_j\\right)^2\\right\\rangle\\\\\n&= \\frac{n}{n - 1} \\left(\\langle x_1^2\\rangle - \\frac{2}{n}\\sum_{j=1}^n \\langle x_1 x_j\\rangle + \\frac{1}{n^2}\\sum_{j,k=1}^n \\langle x_j x_k\\rangle \\right)\\\\\n&= \\frac{n}{n - 1} \\left(\\langle x_1^2\\rangle - \\frac{1}{n}\\sum_{j=1}^n \\langle x_1 x_j\\rangle\\right)\\\\\n&= \\frac{n}{n - 1} \\left(\\left(1 - \\frac{1}{n}\\right)\\langle x_1^2\\rangle - \\frac{1}{n}\\sum_{j=2}^n \\langle x_1 x_j\\rangle\\right)\\\\\n&= \\frac{n}{n - 1} \\left(\\left(1 - \\frac{1}{n}\\right)\\langle x_1^2\\rangle - \\frac{1}{n}\\sum_{j=2}^n \\langle x_1\\rangle\\langle x_j\\rangle\\right)\\\\\n&= \\frac{n}{n - 1} \\left(\\frac{n - 1}{n}\\langle x_1^2\\rangle - \\frac{n - 1}{n}\\mu^2\\right) = \\langle x^2\\rangle - \\mu^2 = \\sigma^2\\,.\n\\end{align}\n\\] Para ir de la segunda a la tercera igualdad hemos usado el hecho que todos los términos de la suma deben dar el mismo resultado ya que las variables son idénticamente distribuídas. Para ir de la cuarta a la quinta hemos usado ese mismo truco. Para ir de la sexta a la séptima hemos usado el hecho que las variables son independientes tal que \\(\\langle x_1 x_j\\rangle = \\langle x_1\\rangle\\langle x_j\\rangle\\) para \\(j\\neq 1\\).",
    "crumbs": [
      "Home",
      "Probabilidad y Estadística",
      "2. Distribuciones y Estimación"
    ]
  }
]