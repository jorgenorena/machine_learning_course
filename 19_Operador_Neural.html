<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="es" xml:lang="es"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Operador Neuronal – Aprendizaje Automático para Físicos</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-e13f217ceb0135cb77e1494052e28e8a.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Sin resultados",
    "search-matching-documents-text": "documentos encontrados",
    "search-copy-link-title": "Copiar el enlace en la búsqueda",
    "search-hide-matches-text": "Ocultar resultados adicionales",
    "search-more-match-text": "resultado adicional en este documento",
    "search-more-matches-text": "resultados adicionales en este documento",
    "search-clear-button-title": "Borrar",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancelar",
    "search-submit-button-title": "Enviar",
    "search-label": "Buscar"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Aprendizaje Automático para Físicos</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Buscar"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Navegación de palanca" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-clases" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Clases</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-clases">    
        <li class="dropdown-header">Probabilidad y Estadística</li>
        <li>
    <a class="dropdown-item" href="./1_probabilidad.html">
 <span class="dropdown-text">1. Probabilidad</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./2_distribuciones_y_estimacion.html">
 <span class="dropdown-text">2. Distribuciones y Estimación</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./3_verosimilitud.html">
 <span class="dropdown-text">3. Verosimilitud</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./4_minimos_cuadrados_intervalos.html">
 <span class="dropdown-text">4. Mínimos Cuadrados e Intervalos</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./5_testeo_de_hipotesis_entropia.html">
 <span class="dropdown-text">5. Testeo de Hipótesis y Entropía</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">Aprendizaje Automático</li>
        <li>
    <a class="dropdown-item" href="./6_aprendizaje_automatico_regresion_lineal.html">
 <span class="dropdown-text">6. Aprendizaje Automático y Regresión Lineal</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./7_clasificacion.html">
 <span class="dropdown-text">7. Clasificación</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./8_seleccion_de_modelos.html">
 <span class="dropdown-text">8. Selección de Modelos</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./9_arboles_bosques_boosting.html">
 <span class="dropdown-text">9. Árboles, Bosques y Boosting</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./10_svm_y_no_supervisado.html">
 <span class="dropdown-text">10. SVM y Aprendizaje No Supervisado</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">Redes Neuronales</li>
        <li>
    <a class="dropdown-item" href="./11_perceptron_y_redes_conexas.html">
 <span class="dropdown-text">11. Perceptrón y Redes Conexas y Pérdidas</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./12_gradient_descent_and_autodiff.html">
 <span class="dropdown-text">12. Descenso de Gradiente y Auto-diferenciación</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./13_evaluacion_y_regularizacion_de_nn.html">
 <span class="dropdown-text">13. Evaluación y Regularización de redes neuronales</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./14_redes_convolucionales.html">
 <span class="dropdown-text">14. Redes Convolucionales</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./15_redes_residuales.html">
 <span class="dropdown-text">15. Redes Residuales</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./16_transformadores.html">
 <span class="dropdown-text">16. Transformadores y el mecanismo de atención</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">Algunas aplicaciones</li>
        <li>
    <a class="dropdown-item" href="./17_flujos_normalizantes.html">
 <span class="dropdown-text">17. Flujos Normalizantes</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./18_PINNs.html">
 <span class="dropdown-text">18. Redes Neuronales Informadas por la Física (PINNs)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./19_Operador_Neural.html">
 <span class="dropdown-text">19. Operadores Neuronales</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="./about.html"> 
<span class="menu-text">Acerca de</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Alternar barra lateral" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./17_flujos_normalizantes.html">Algunas aplicaciones</a></li><li class="breadcrumb-item"><a href="./19_Operador_Neural.html">19. Operadores Neuronales</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Alternar barra lateral" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Probabilidad y Estadística</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Alternar sección">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1_probabilidad.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1. Probabilidad</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2_distribuciones_y_estimacion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2. Distribuciones y Estimación</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3_verosimilitud.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3. Verosimilitud</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./4_minimos_cuadrados_intervalos.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4. Mínimos Cuadrados e Intervalos</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./5_testeo_de_hipotesis_entropia.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5. Testeo de Hipótesis y Entropía</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Aprendizaje Automático</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Alternar sección">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./6_aprendizaje_automatico_regresion_lineal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6. Aprendizaje Automático y Regresión Lineal</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./7_clasificacion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7. Clasificación</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./8_seleccion_de_modelos.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8. Selección de Modelos</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./9_arboles_bosques_boosting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">9. Árboles, Bosques y Boosting</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10_svm_y_no_supervisado.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">10. SVM y Aprendizaje No Supervisado</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Redes Neuronales</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Alternar sección">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11_perceptron_y_redes_conexas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">11. Perceptrón, Redes Conexas y Pérdidas</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12_gradient_descent_and_autodiff.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">12. Descenso de Gradiente y Auto-diferenciación</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13_evaluacion_y_regularizacion_de_nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">13. Evaluación y Regularización de redes neuronales</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14_redes_convolucionales.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">14. Redes Convolucionales</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15_redes_residuales.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">15. Redes Residuales</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16_transformadores.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">16. Transformadores y el mecanismo de atención</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Algunas aplicaciones</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Alternar sección">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17_flujos_normalizantes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">17. Flujos Normalizantes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18_PINNs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">18. Redes Neuronales Informadas por la Física (PINNs)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19_Operador_Neural.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">19. Operadores Neuronales</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">En esta página</h2>
   
  <ul>
  <li><a href="#introducción" id="toc-introducción" class="nav-link active" data-scroll-target="#introducción">Introducción</a></li>
  <li><a href="#ejemplo-la-ecuación-de-onda-1d" id="toc-ejemplo-la-ecuación-de-onda-1d" class="nav-link" data-scroll-target="#ejemplo-la-ecuación-de-onda-1d">Ejemplo: La Ecuación de Onda 1D</a></li>
  <li><a href="#aprendiendo-el-operador-de-solución-de-la-ecuación-de-onda" id="toc-aprendiendo-el-operador-de-solución-de-la-ecuación-de-onda" class="nav-link" data-scroll-target="#aprendiendo-el-operador-de-solución-de-la-ecuación-de-onda">Aprendiendo el Operador de Solución de la Ecuación de Onda</a>
  <ul class="collapse">
  <li><a href="#generación-de-datos" id="toc-generación-de-datos" class="nav-link" data-scroll-target="#generación-de-datos">1. Generación de Datos</a></li>
  <li><a href="#definición-del-modelo-de-operador-neuronal" id="toc-definición-del-modelo-de-operador-neuronal" class="nav-link" data-scroll-target="#definición-del-modelo-de-operador-neuronal">2. Definición del Modelo de Operador Neuronal</a></li>
  <li><a href="#entrenamiento-del-modelo" id="toc-entrenamiento-del-modelo" class="nav-link" data-scroll-target="#entrenamiento-del-modelo">3. Entrenamiento del Modelo</a></li>
  <li><a href="#prueba-del-operador-aprendido" id="toc-prueba-del-operador-aprendido" class="nav-link" data-scroll-target="#prueba-del-operador-aprendido">4. Prueba del Operador Aprendido</a></li>
  </ul></li>
  <li><a href="#comparación-con-métodos-numéricos-tradicionales" id="toc-comparación-con-métodos-numéricos-tradicionales" class="nav-link" data-scroll-target="#comparación-con-métodos-numéricos-tradicionales">Comparación con Métodos Numéricos Tradicionales</a></li>
  <li><a href="#operadores-neuronales-vs.-redes-neuronales-informadas-por-la-física-pinns" id="toc-operadores-neuronales-vs.-redes-neuronales-informadas-por-la-física-pinns" class="nav-link" data-scroll-target="#operadores-neuronales-vs.-redes-neuronales-informadas-por-la-física-pinns">Operadores Neuronales vs.&nbsp;Redes Neuronales Informadas por la Física (PINNs)</a></li>
  <li><a href="#fuentes" id="toc-fuentes" class="nav-link" data-scroll-target="#fuentes">Fuentes:</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./17_flujos_normalizantes.html">Algunas aplicaciones</a></li><li class="breadcrumb-item"><a href="./19_Operador_Neural.html">19. Operadores Neuronales</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Operador Neuronal</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introducción" class="level1">
<h1>Introducción</h1>
<p>Hasta ahora hemos estudiado redes neuronales que toman un vector de entrada <span class="math inline">\(\boldsymbol{x}\)</span> y producen un vector de salida <span class="math inline">\(\boldsymbol{y}\)</span>, es decir, <span class="math inline">\(f: \mathbb{R}^n \to \mathbb{R}^m\)</span>. En contraste, los operadores neuronales son una clase de modelos de aprendizaje profundo diseñados para aprender <strong>mapeos entre espacios de funciones</strong>, en lugar de solo vectores de dimensión finita. Es decir, los operadores neuronales aprenden un <strong>operador</strong> <span class="math inline">\(\mathcal{G}\)</span> que toma una <em>función de entrada</em> y produce una <em>función de salida</em>. Formalmente, uno puede pensar en <span class="math inline">\(\mathcal{G}: X \to Y\)</span>, donde <span class="math inline">\(X\)</span> y <span class="math inline">\(Y\)</span> son espacios de dimensión infinita (por ejemplo, espacios de Banach) de funciones.</p>
<p>Arquitectónicamente, se asemejan a las redes que hemos visto, pero con capas definidas por transformadas integrales en vez de transformaciones lineales elemento por elemento. Una capa de operador neuronal puede escribirse como:</p>
<p><span class="math display">\[
v_{j+1}(x) = \sigma\!\Big( W\, v_j(x) \;+\; \int_D K_\Theta(x,y)\, v_j(y)\,dy \Big),
\]</span></p>
<p>donde <span class="math inline">\(v_j(x)\)</span> es una función intermedia (comenzando con <span class="math inline">\(v_0\)</span> como la función de entrada), <span class="math inline">\(W\)</span> es una transformación lineal aprendida, <span class="math inline">\(K_\Theta(x,y)\)</span> es un kernel aprendido para un operador integral, y <span class="math inline">\(\sigma\)</span> es una activación no lineal (aplicada elemento por elemento). Al componer varias de estas capas, los operadores neuronales pueden representar operadores no lineales muy generales. De hecho, existen teoremas de aproximación universal que aseguran que los operadores neuronales pueden aproximar cualquier operador continuo (en conjuntos compactos) con precisión arbitraria. Esto extiende el clásico teorema de aproximación universal de redes neuronales de dimensión finita al caso de mapeos de operadores de dimensión infinita.</p>
<p>Una característica clave de los operadores neuronales es que, con la formulación adecuada, <em>el mismo modelo aprendido puede aplicarse a diferentes discretizaciones</em> o puntos de evaluación de las funciones de entrada/salida. A diferencia de una red neuronal estándar que está atada a un tamaño de entrada fijo, los operadores neuronales pueden recibir funciones muestreadas en cualquier malla (o incluso como coeficientes en una base) y producir salidas en cualquier conjunto de puntos deseado. En cierto sentido, así como ocurría en las PINNs, son ya operadores interpolados.</p>
<p>Se han desarrollado varias arquitecturas especializadas de operadores neuronales como DeepONet, FNO, etc.</p>
<p>La motivación principal de los operadores neuronales es crear sustitutos rápidos y generalizables para problemas de física computacional. Muchos sistemas físicos se modelan mediante ecuaciones diferenciales parciales u otras ecuaciones donde un conjunto de funciones de entrada (por ejemplo, condiciones iniciales, de frontera o coeficientes que varían espacialmente) se mapea a una función de salida (la solución). Los métodos estándar deben recomputar la solución <em>desde cero para cada nueva entrada</em> (por ejemplo para cada nueva condición inicial) y pueden ser extremadamente costosos para sistemas grandes. Los operadores neuronales ofrecen un enfoque de aprender una vez, reutilizar muchas veces: <em>Entrenamos</em> un modelo con muchas parejas de entrada-salida (usando datos de un solucionador de alta fidelidad o experimentos) y luego <em>predecimos nuevas soluciones en una sola pasada hacia adelante</em>.</p>
<p>Los operadores neuronales se han aplicado en áreas como modelado de flujo turbulento, modelado climático/meteorológico, mecánica estructural y flujo en medios porosos. En estos dominios, obtener soluciones en tiempo real o muchas consultas es crítico (para control, optimización, cuantificación de incertidumbre, etc.), y los operadores neuronales sirven como sustitutos eficientes.</p>
<p>En general, a los métodos usados para aproximar el resultado de una simulación se los llama <em>emuladores</em>. Los emuladores se usan extensivamente en cosmología para explorar espacioes de parámetros de alta dimensión (tirando muchos puntos) sin necesidad de correr una simulación nueva cada vez.</p>
</section>
<section id="ejemplo-la-ecuación-de-onda-1d" class="level1">
<h1>Ejemplo: La Ecuación de Onda 1D</h1>
<p>Para concretar estos conceptos, consideremos un ejemplo físico sencillo: la ecuación de onda unidimensional. La ecuación de onda (en un dominio 1D, digamos <span class="math inline">\(x\in [0,1]\)</span> con condiciones de frontera apropiadas) es</p>
<p><span class="math inline">\(u_{tt}(t,x) = c^2\,u_{xx}(t,x),\)</span></p>
<p>con condiciones iniciales <span class="math inline">\(u(0,x) = f(x)\)</span> y <span class="math inline">\(u_t(0,x) = g(x)\)</span>. Aquí, <span class="math inline">\(f(x)\)</span> es el desplazamiento inicial de la cuerda (o campo de onda) y <span class="math inline">\(g(x)\)</span> es la velocidad inicial. Podemos pensar en el <em>operador de solución</em> <span class="math inline">\(\mathcal{G}_T\)</span> que mapea las condiciones iniciales al estado de la onda en el tiempo <span class="math inline">\(T\)</span></p>
<p><span class="math inline">\(\mathcal{G}_T:\; (f, g) \mapsto u(T,\cdot),\)</span></p>
<p>el cual es a su vez una función de <span class="math inline">\(x\)</span>. En este ejemplo, para simplificar, tomaremos <span class="math inline">\(g(x)=0\)</span> (velocidad inicial nula), de modo que la onda parte desde el reposo y <span class="math inline">\(f(x)\)</span> determina completamente el movimiento. La ecuación de onda es lineal y se puede escribir su solución explícitamente usando la <em>fórmula de d’Alembert</em>. En particular, en un dominio 1D infinito o periódico, la solución en el tiempo <span class="math inline">\(t\)</span> está dada por el promedio de una onda que se mueve a la derecha y otra a la izquierda originadas en <span class="math inline">\(f\)</span></p>
<p><span class="math display">\[
u(t,x) = \frac{1}{2}\Big[\,f(x - c\,t) + f(x + c\,t)\Big] +
\frac{1}{2c}\int_{x-ct}^{x+ct} g(s)\,ds \,.
\]</span></p>
<p>Para <span class="math inline">\(g=0\)</span>, la fórmula se simplifica a <span class="math inline">\(u(t,x) = \frac{1}{2}\left[f(x-ct) + f(x+ct)\right]\)</span>. Esto describe dos ondas de media amplitud viajando en sentidos opuestos, cuya superposición da la solución. Por ejemplo, si <span class="math inline">\(c=1\)</span> y <span class="math inline">\(t=0.25\)</span> (con las unidades apropiadas) en un dominio periódico de longitud <span class="math inline">\(1\)</span>, entonces <span class="math inline">\(u(0.25,x) = \tfrac{1}{2}\left[f(x-0.25)+f(x+0.25)\right]\)</span>. Geométricamente, esto es la forma inicial <span class="math inline">\(f(x)\)</span> dividida en dos mitades que se han desplazado una distancia 0.25 a izquierda y derecha y se superponen.</p>
<p>Podemos ver a <span class="math inline">\(\mathcal{G}_{0.25}\)</span> (tiempo <span class="math inline">\(T=0.25\)</span>) como un operador que toma la función inicial <span class="math inline">\(f(x)\)</span> y produce una nueva función <span class="math inline">\(u(0.25,x)\)</span>. Este operador es lineal (de hecho es un operador de convolución/desplazamiento en este caso). Pero imaginemos que no conocemos la fórmula. Podríamos recolectar datos eligiendo muchas funciones iniciales diferentes <span class="math inline">\(f(x)\)</span>, resolviendo la ecuación de onda para cada una y registrando la solución en <span class="math inline">\(t=0.25\)</span>. El objetivo de un operador neuronal sería aprender el mapeo <span class="math inline">\(f \mapsto u(0.25,\cdot)\)</span> a partir de muchos ejemplos (formas distintas de <span class="math inline">\(f\)</span>), y luego generalizar a nuevas condiciones iniciales rápidamente sin volver a resolver la ecuación explícitamente.</p>
<p>Esta solución obtenida por el operador aprendido será una interpolación entre las soluciones vistas, pero puede ser una buena aproximación si la función <span class="math inline">\(f\)</span> se asemeja a alguna en el conunto de entrenamiento.</p>
</section>
<section id="aprendiendo-el-operador-de-solución-de-la-ecuación-de-onda" class="level1">
<h1>Aprendiendo el Operador de Solución de la Ecuación de Onda</h1>
<p>Para demostrar cómo implementar un operador neuronal, construiremos un ejemplo simple en PyTorch. Generaremos datos sintéticos resolviendo la ecuación de onda 1D para desplazamientos iniciales aleatorios, y luego entrenaremos una red neuronal para aprender el mapeo de la condición inicial a la solución. Para simplificar, discretizamos el dominio espacial en una cuadrícula de <span class="math inline">\(N\)</span> puntos y representamos las funciones como vectores de longitud <span class="math inline">\(N\)</span> (esto convierte el aprendizaje del operador en un problema de aprendizaje de función de dimensión finita, pero que puede aproximar el operador verdadero si <span class="math inline">\(N\)</span> es grande o si las funciones viven en un subespacio adecuado).</p>
<section id="generación-de-datos" class="level2">
<h2 class="anchored" data-anchor-id="generación-de-datos">1. Generación de Datos</h2>
<p>Primero, establecemos los parámetros del problema y creamos un conjunto de datos. Elegimos una cuadrícula espacial de tamaño <span class="math inline">\(N=100\)</span> (puntos en <span class="math inline">\([0,1]\)</span>), consideramos <span class="math inline">\(c=1\)</span> y un tiempo final <span class="math inline">\(T=0.25\)</span>. Generamos funciones de desplazamiento inicial aleatorio <span class="math inline">\(f(x)\)</span> superponiendo algunos modos de Fourier aleatorios para obtener funciones suaves y periódicas. Luego calculamos el correspondiente <span class="math inline">\(u(T,x)\)</span> usando la solución analítica. Esto nos da pares <span class="math inline">\((f, u_T)\)</span> para entrenamiento.</p>
<div id="d1fbcb56" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Parametros</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">100</span>                 <span class="co"># numero de puntos espaciales</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>dominio <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, N, endpoint<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="fl">0.25</span>                <span class="co"># tiempo final</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>desplazamiento <span class="op">=</span> <span class="bu">int</span>(c <span class="op">*</span> T <span class="op">*</span> N)  <span class="co"># desplazamiento en puntos de la cuadrícula</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Funcion para generar un desplazamiento inicial aleatorio f(x)</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> funcion_inicial_aleatoria(x, num_modos<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    f <span class="op">=</span> np.zeros_like(x)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># superponer modos de Fourier aleatorios</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, num_modos<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        A <span class="op">=</span> np.random.uniform(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)        <span class="co"># amplitud aleatoria</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        phi <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="dv">2</span><span class="op">*</span>np.pi) <span class="co"># fase aleatoria</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        f <span class="op">+=</span> A <span class="op">*</span> np.cos(<span class="dv">2</span><span class="op">*</span>np.pi <span class="op">*</span> k <span class="op">*</span> x <span class="op">+</span> phi)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> f</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Crear conjuntos de entrenamiento y prueba</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>n_entrenamiento <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>n_prueba  <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>entrenamiento_x <span class="op">=</span> np.zeros((n_entrenamiento, N))</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>entrenamiento_y <span class="op">=</span> np.zeros((n_entrenamiento, N))</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_entrenamiento):</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    f <span class="op">=</span> funcion_inicial_aleatoria(dominio)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Solucion en T: u(T,x) = 0.5[f(x - cT) + f(x + cT)]</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    u_T <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> (np.roll(f, desplazamiento) <span class="op">+</span> np.roll(f, <span class="op">-</span>desplazamiento))</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    entrenamiento_x[i, :] <span class="op">=</span> f</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    entrenamiento_y[i, :] <span class="op">=</span> u_T</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="co"># De forma similar para el conjunto de prueba</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>prueba_x <span class="op">=</span> np.zeros((n_prueba, N))</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>prueba_y <span class="op">=</span> np.zeros((n_prueba, N))</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(n_prueba):</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    f <span class="op">=</span> funcion_inicial_aleatoria(dominio)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    u_T <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> (np.roll(f, desplazamiento) <span class="op">+</span> np.roll(f, <span class="op">-</span>desplazamiento))</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>    prueba_x[j, :] <span class="op">=</span> f</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    prueba_y[j, :] <span class="op">=</span> u_T</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Convertir los datos a tensores de PyTorch</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>entrenamiento_x_t <span class="op">=</span> torch.tensor(entrenamiento_x, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>entrenamiento_y_t <span class="op">=</span> torch.tensor(entrenamiento_y, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>prueba_x_t  <span class="op">=</span> torch.tensor(prueba_x, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>prueba_y_t  <span class="op">=</span> torch.tensor(prueba_y, dtype<span class="op">=</span>torch.float32)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>En el código anterior, <code>np.roll(f, desplazamiento)</code> desplaza el arreglo <code>f</code> una cantidad de puntos (con envoltura periódica), evaluando efectivamente <span class="math inline">\(f(x - T)\)</span> o <span class="math inline">\(f(x + T)\)</span> en la cuadrícula discreta. Al promediar obtenemos la solución en el tiempo <span class="math inline">\(T\)</span>. Finalmente, convertimos los arrays de NumPy a tensores de PyTorch para entrenamiento.</p>
</section>
<section id="definición-del-modelo-de-operador-neuronal" class="level2">
<h2 class="anchored" data-anchor-id="definición-del-modelo-de-operador-neuronal">2. Definición del Modelo de Operador Neuronal</h2>
<p>Usaremos una <em>red neuronal totalmente conectada</em>. Esta red toma un vector de longitud <span class="math inline">\(N\)</span> (la condición inicial discretizada <span class="math inline">\(f\)</span>) y produce un vector de longitud <span class="math inline">\(N\)</span> (la solución predicha <span class="math inline">\(u_T\)</span>). Aunque es esencialmente una red estándar (sin imponer estructura especial de operador), aún puede aprender el mapeo de <span class="math inline">\(f\)</span> a <span class="math inline">\(u_T\)</span> dada suficiente capacidad y datos. Modelos más sofisticados podrían usar convoluciones o compartir pesos para aprovechar simetrías, pero aquí usamos este enfoque básico por claridad.</p>
<div id="094229df" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> OperadorNeuronal1D(nn.Module):</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, N):</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.red <span class="op">=</span> nn.Sequential(</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>            nn.Linear(N, <span class="dv">128</span>), nn.ReLU(),</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">128</span>, <span class="dv">128</span>), nn.ReLU(),</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">128</span>, N)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.red(x)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Instanciar el modelo</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>modelo <span class="op">=</span> OperadorNeuronal1D(N)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Esto define un perceptrón de 3 capas (dos ocultas de tamaño 128 con activaciones ReLU). No hay capas convolucionales ni de Fourier; la entrada se trata como vector plano.</p>
</section>
<section id="entrenamiento-del-modelo" class="level2">
<h2 class="anchored" data-anchor-id="entrenamiento-del-modelo">3. Entrenamiento del Modelo</h2>
<p>Entrenamos el operador neuronal usando el error cuadrático medio (MSE) entre la salida de la red y la solución verdadera <span class="math inline">\(u_T(x)\)</span>, mediante descenso de gradiente (optimizador Adam). En cada época hacemos una pasada completa sobre los datos (por simplicidad, no usamos minibatches aquí, pero podría hacerse).</p>
<div id="dc8196d1" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Funcion de perdida y optimizador</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>criterio <span class="op">=</span> nn.MSELoss()</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>optimizador <span class="op">=</span> optim.Adam(modelo.parameters(), lr<span class="op">=</span><span class="fl">1e-3</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Ciclo de entrenamiento</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>num_epocas <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoca <span class="kw">in</span> <span class="bu">range</span>(num_epocas):</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Paso hacia adelante con todos los datos de entrenamiento</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    prediccion <span class="op">=</span> modelo(entrenamiento_x_t)            <span class="co"># forma: (n_entrenamiento, N)</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    perdida <span class="op">=</span> criterio(prediccion, entrenamiento_y_t)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Retropropagacion y actualizacion de pesos</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    optimizador.zero_grad()</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    perdida.backward()</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    optimizador.step()</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Imprimir progreso ocasionalmente</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> epoca <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Epoca </span><span class="sc">{</span>epoca<span class="sc">}</span><span class="ss">: perdida de entrenamiento = </span><span class="sc">{</span>perdida<span class="sc">.</span>item()<span class="sc">:.6f}</span><span class="ss">"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoca 0: perdida de entrenamiento = 0.335672
Epoca 100: perdida de entrenamiento = 0.001228
Epoca 200: perdida de entrenamiento = 0.000407
Epoca 300: perdida de entrenamiento = 0.000203
Epoca 400: perdida de entrenamiento = 0.000127</code></pre>
</div>
</div>
<p>Corremos, por ejemplo, 500 épocas. Cada 100 épocas imprimimos la pérdida para monitorear el avance.</p>
</section>
<section id="prueba-del-operador-aprendido" class="level2">
<h2 class="anchored" data-anchor-id="prueba-del-operador-aprendido">4. Prueba del Operador Aprendido</h2>
<p>Tras entrenar, evaluamos el modelo sobre condiciones iniciales de prueba no vistas. Calculamos el error relativo para ver cuán bien se aprendió el operador y comparamos visualmente para un ejemplo.</p>
<div id="afa0ea77" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluar en datos de prueba</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>modelo.<span class="bu">eval</span>()  <span class="co"># modo evaluacion</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># elegir un indice del conjunto de prueba</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    k <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    f0 <span class="op">=</span> prueba_x_t[k]         <span class="co"># condicion inicial</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    solucion_verdadera <span class="op">=</span> prueba_y_t[k]    <span class="co"># solucion exacta en T</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    prediccion_u <span class="op">=</span> modelo(f0.unsqueeze(<span class="dv">0</span>)).squeeze(<span class="dv">0</span>)  <span class="co"># agregar dimension batch</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calcular error relativo L2</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    error <span class="op">=</span> torch.norm(prediccion_u <span class="op">-</span> solucion_verdadera) <span class="op">/</span> torch.norm(solucion_verdadera)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Error relativo L2 en muestra de prueba </span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>error<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Error relativo L2 en muestra de prueba 0: 0.0513</code></pre>
</div>
</div>
<p>En este ejemplo 1D, el problema era fácil – el operador era un promedio de desplazamientos lineales, y la red lo aprende rápidamente. En escenarios más complejos (EDPs no lineales, más dimensiones, etc.), se requerirían arquitecturas más avanzadas y conjuntos de datos más grandes, pero el flujo de trabajo sería análogo: simular muchas instancias del sistema, entrenar una red para mapear entradas a salidas, y luego usar el modelo entrenado para predicciones rápidas.</p>
</section>
</section>
<section id="comparación-con-métodos-numéricos-tradicionales" class="level1">
<h1>Comparación con Métodos Numéricos Tradicionales</h1>
<p>¿Cómo se comparan los operadores neuronales con los solucionadores numéricos clásicos como diferencias finitas o elementos finitos? Existen diferencias importantes:</p>
<ul>
<li><p><strong>Resolución explícita vs.&nbsp;modelo sustituto aprendido:</strong> Los métodos tradicionales requieren resolver la EDP desde cero para cada nueva entrada (condición inicial/de frontera o parámetro). Esto suele involucrar algoritmos iterativos, resolución en malla, etc., lo cual puede ser computacionalmente costoso. Los operadores neuronales tienen un alto costo inicial (la fase de entrenamiento), pero después de eso, pueden interpolar para nuevas entradas. Por eso se pueden usar en situaciones en las cuales necesitamos rsolver muchas veces como en cadenas monte carlo.</p></li>
<li><p><strong>Precisión y convergencia:</strong> Los solucionadores tradicionales tienen formas sistemáticas de mejorar la precisión (por ejemplo, refinar la malla o aumentar el orden polinomial en métodos de elementos finitos) y suelen contar con cotas teóricas de error. Los operadores neuronales, al ser impulsados por datos, no garantizan convergencia en el mismo sentido. Su precisión depende de haber visto entradas similares durante el entrenamiento y de la capacidad de la red. Un operador neuronal bien entrenado puede ser muy preciso dentro de la distribución de datos vista en entrenamiento (a menudo igualando el error de discretización numérica del solucionador que generó los datos). Sin embargo, si se le pide a un operador neuronal que extrapole muy lejos de lo que vio en entrenamiento (por ejemplo, un tipo de función de entrada completamente diferente), podría dar resultados pobres o no físicos.</p></li>
<li><p><strong>Generalidad:</strong> Un solo operador neuronal puede <em>manejar un rango de parámetros o entradas</em> sin modificación, aprendiendo esencialmente el mapeo general de solución de una EDP paramétrica. Los solucionadores clásicos también pueden resolver EDPs paramétricas, pero lo hacen volviendo a resolver cada vez. Se puede ver a un operador neuronal como una especie de <em>solucionador generalizado</em> que ha visto muchas instancias y puede interpolar entre ellas.</p></li>
<li><p><strong>Interpretabilidad y garantías:</strong> Las soluciones numéricas tradicionales obedecen las leyes físicas conocidas por construcción (por ejemplo, las leyes de conservación de energía o principios de máximo se cumplirán si el solucionador está bien diseñado). Los operadores neuronales, salvo que se les impongan restricciones explícitamente, podrían violar estos principios físicos porque aprenden ajustando datos. Por ejemplo, un operador neuronal para flujo de fluidos podría no conservar exactamente la masa salvo que esa propiedad se refuerce o emerja de los datos. En cuanto a estabilidad, los esquemas numéricos se analizan para garantizar estabilidad bajo refinamiento, mientras que la estabilidad de una red neuronal (por ejemplo, cómo se amplifican los errores en entradas no vistas) es más difícil de evaluar. Dicho esto, los operadores neuronales <em>pueden</em> incorporar cierta estructura física (por ejemplo, una arquitectura convolucional respeta la invariancia por traslación, y una capa de Fourier impone una estructura periódica global).</p></li>
<li><p><strong>Datos y costo de entrenamiento:</strong> Una desventaja importante del enfoque de operador neuronal es la necesidad de un <em>gran conjunto de datos de soluciones</em>. Generar estos datos a menudo requiere ejecutar muchas veces un solucionador tradicional (o hacer experimentos) para producir soluciones de alta fidelidad para entrenar. Esto puede ser extremadamente costoso en EDPs 3D complejas: esencialmente se traslada la carga computacional al inicio. Los solucionadores clásicos no requieren precomputación: resuelven directamente las ecuaciones para el caso de interés. En escenarios donde los datos son escasos o costosos de obtener, un operador puramente impulsado por datos puede ser impráctico.</p></li>
</ul>
<p>En resumen, los solucionadores tradicionales son fiables, generales y precisos para cada resolución pero pueden ser lentos si se repiten muchas veces; los operadores neuronales sacrifican parte de la precisión y generalidad garantizadas a cambio de velocidad y la capacidad de aprender de datos a través de muchas instancias. En la práctica, se pueden usar operadores neuronales como sustitutos para acelerar tareas como optimización o cuantificación de incertidumbre, mientras que los solucionadores clásicos sirven para verificación o para escenarios fuera del régimen entrenado.</p>
</section>
<section id="operadores-neuronales-vs.-redes-neuronales-informadas-por-la-física-pinns" class="level1">
<h1>Operadores Neuronales vs.&nbsp;Redes Neuronales Informadas por la Física (PINNs)</h1>
<p>Las Redes Neuronales Informadas por la Física (PINNs) son otro enfoque neuronal popular para problemas de EDPs, pero su caso de uso es fundamentalmente diferente al de los operadores neuronales. Es útil comparar y contrastar ambos métodos:</p>
<ul>
<li><p>Las PINNs entrenan una red neuronal <em>para que satisfaga una EDP dada</em> (y condiciones de frontera/iniciales) <em>para un problema específico</em>. La función de pérdida de la PINN incluye el residuo de la EDP: la red aprende una solución <span class="math inline">\(u_\theta(x,t)\)</span> que hace que <span class="math inline">\(u_{tt} - c^2 u_{xx} \approx 0\)</span> (por ejemplo) y además satisface los datos iniciales y de frontera. Los operadores neuronales, en cambio, no codifican la EDP directamente en la pérdida; tratan el problema como una caja negra y aprenden a partir de <em>datos de soluciones precomputadas</em>. En resumen, las PINNs se entrenan de forma <em>no supervisada</em> (guiadas por la física) para un solo problema a la vez, mientras que los operadores neuronales se entrenan de forma <em>supervisada</em> (guiados por datos) sobre muchos problemas a la vez.</p></li>
<li><p>Una PINN se usa típicamente como <em>solucionador alternativo</em> para un escenario concreto Por ejemplo, se entrena una PINN para resolver las ecuaciones de Navier–Stokes en una configuración de flujo específica, en vez de usar software clásico. Un operador neuronal se usa como <em>modelo sustituto</em>: se entrena sobre una clase de problemas para que pueda producir soluciones instantáneamente para nuevos casos de esa clase. Por ejemplo, se puede entrenar un operador neuronal en Navier–Stokes para distintas condiciones de contorno o forzamientos, y luego usarlo para predecir el flujo para una nueva condición rápidamente.</p></li>
<li><p>Las PINNs son atractivas porque <em>no requieren datos de solución</em>, la ley física es la señal de entrenamiento. Esto es útil si no se tienen ejemplos de la solución o si se confía más en la ecuación que en datos ruidosos. Los operadores neuronales <em>sí requieren datos</em>. Si se dispone de datos de alta fidelidad (de simulaciones o experimentos), los operadores neuronales pueden aprovecharlos para lograr precisión. Existen enfoques híbridos (aprendizaje de operadores informado por la física) que usan tanto datos como residuos de la EDP para entrenar, pero son áreas activas de investigación.</p></li>
<li><p>Es importante notar que la distinción entre PINNs y operadores neuronales no es absoluta. Existen métodos para entrenar operadores neuronales con restricciones físicas (para reducir la cantidad de datos requeridos o forzar leyes físicas), y, a la inversa, se puede ver una PINN con entradas paramétricas como un aprendiz de operador rudimentario. La biblioteca SciML de Julia, por ejemplo, ofrece tanto solucionadores PINN (NeuralPDE.jl) como herramientas de aprendizaje de operadores, reconociendo ambos como métodos complementarios. En general, se puede preferir una PINN cuando hay pocos datos pero una EDP bien definida (especialmente si es difícil para los métodos clásicos, como una EDP fraccionaria o de alta dimensión), mientras que los operadores neuronales destacan cuando se puede costear el entrenamiento previo sobre una familia de problemas para habilitar <em>inferencias rápidas</em> después.</p></li>
</ul>
</section>
<section id="fuentes" class="level1">
<h1>Fuentes:</h1>
<ol type="1">
<li><p>Kovachki, N. et al.&nbsp;<em>Neural Operator: Learning Maps Between Function Spaces</em>. J. Mach. Learn. Res. 24(1): 4061-4157, 2023.</p></li>
<li><p>Wikipedia: “Neural operators.” <em>Wikipedia, The Free Encyclopedia</em>, 2024.</p></li>
<li><p>Lu, L., Jin, P., Karniadakis, G. <em>DeepONet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators</em>. <strong>arXiv:1910.03193</strong> (2019).</p></li>
<li><p>Li, Z. et al.&nbsp;<em>Fourier Neural Operator for Parametric Partial Differential Equations</em>. NeurIPS 2020. (See also <strong>arXiv:2010.08895</strong>).</p></li>
<li><p>SciML Julia Library Documentation – Sections on PINNs and Neural Operators.</p></li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiado");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiado");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>